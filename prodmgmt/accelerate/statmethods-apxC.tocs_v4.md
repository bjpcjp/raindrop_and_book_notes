![statmethods-apxC](statmethods-apxC.best.png)

- **Survey Preparation**
  - Once constructs and hypotheses are set each year, survey instruments are designed using previously validated items when possible.
  - Custom measures are developed following procedures adapted from Dillman (1978).
  - Relevant examples include organizational performance (Widener 2007) and noncommercial performance (Cavalluzzo and Ittner 2004).
  - For further reading, see [Dillman (1978)](https://www.jstor.org/stable/2096344).

- **Data Collection**
  - Data was collected using snowball sampling, a nonprobabilistic technique.
  - Strategies addressed the technique’s limitations, as detailed in Chapter 15.
  - Snowball sampling was selected due to the nature of the research design.
  - For more information, refer to Chapter 15.

- **Tests for Bias**
  - Chi-square tests check differences in categorical variables (e.g., gender).
  - T-tests detect differences in scale variables (e.g., Likert scores) between early and late responders.
  - Common method bias was assessed via Harman’s single-factor test and the marker variable test.
  - Bias was not observed between early and late responders; common-method bias was not a problem.
  - See [Podsakoff and Dalton 1987] and [Lindell and Whitney 2001] for methodologies.

- **Testing for Relationships**
  - The analysis is conducted in two stages: measurement validation followed by hypothesis testing.
  - Only constructs passing validity and reliability criteria enter the second stage.
  - Gefen and Straub (2005) provide guidelines for this two-stage approach.

- **Tests of the Measurement Model**
  - Principal Components Analysis (PCA) with varimax rotation confirms convergent validity separately for independent and dependent variables.
  - Exploratory Factor Analysis (EFA) is preferred over Confirmatory Factor Analysis (CFA) except in specific cases (e.g., transformational leadership).
  - Items must load above 0.60 on their constructs and avoid cross-loading.
  - Average Variance Extracted (AVE) thresholds: >0.50 for convergent validity; square root of AVE must exceed cross-correlations for discriminant validity.
  - Correlation below 0.85 between constructs supports divergent validity; Pearson correlations are used.
  - Reliability is assessed by Cronbach’s alpha and composite reliability (CR), both requiring cutoffs of 0.70.
  - All constructs passed these psychometric tests.
  - See [Straub et al. 2004], [Brown 2006], and [Chin et al. 2003] for details.

- **Tests for Relationships (Correlation and Prediction) and Classification**
  - Pearson correlation measures linear relationships between constructs, with values ranging from -1 to 1.
  - Regression tests predictive relationships using:
    - Partial Least Squares (PLS) regression (2015–2017), selected for predictive optimization, no assumption of normality, and suitability for exploratory research.
    - Linear regression applied in 2014.
  - Chin (2010) supports PLS choice for exploratory and predictive modeling.
  
- **Tests for Classification**
  - Cluster analysis classifies software delivery performance into high, medium, and low performers using five methods; Ward’s method was selected for best performance.
  - Hierarchical cluster analysis was preferred due to strong explanatory power, lack of predetermined cluster number, and manageable dataset size.
  - Analysis of Variance (ANOVA) with Tukey’s post hoc tests interpreted cluster differences, using significance threshold p < 0.10.
  - Duncan’s multiple range test confirmed the Tukey’s results.
  - High performers consistently showed best outcomes; low performers the worst, with medium in between.
  - Ulrich and McKelvey (1990) informed cluster evaluation criteria; Hair et al. (2006) supported ANOVA methods.
