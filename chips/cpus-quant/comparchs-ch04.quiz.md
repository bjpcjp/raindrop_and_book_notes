1. **Q:** Compare and contrast the architectural features and programming models of vector architectures, SIMD multimedia extensions, and GPUs, highlighting their handling of data-level parallelism, memory operations, and conditional execution.  
   **A:** Vector architectures use large vector registers operated on by a single instruction; typically they have explicit vector length and mask registers that aid compilation and support strided and gather-scatter memory access modes. They amortize memory latency via deep pipelining and chaining of operations, and use compiler-managed mask registers for conditional execution. SIMD multimedia extensions fix vector length, lack sophisticated addressing modes (no stride or gather-scatter), and have limited masking capabilities, making them harder to program and less flexible, but simpler to implement and integrated with conventional processors. GPUs organize computation into grids of thread blocks, with multithreaded SIMD processors executing many threads of SIMD instructions in hardware with implicit masking and dynamic hardware-managed masks for conditional execution. GPUs treat all loads and stores as gather-scatter, mitigating latency by multithreading and hardware address coalescing rather than relying on specialized vector load/store instructions. Programming models reflect these designs: vector programs rely on vectorizing compilers and explicit vector instructions, SIMD multimedia is often programmed via libraries or assembly, while GPUs use CUDA or OpenCL to expose massive fine-grained parallelism as threads with hardware managing scheduling and masking.  
   **External example:** NVIDIA's CUDA programming model and GPU architecture documentation illustrate GPU multithreaded SIMD execution and memory coalescing: https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html

2. **Q:** Explain the concept of "convoys" and "chimes" in vector architectures, including how they relate to execution time and how multilevel pipelining and multiple lanes improve vector performance.  
   **A:** A convoy is a set of vector instructions that can execute together without structural hazards. The number of convoys determines the minimum serialization in vector instruction execution. A chime is the unit of time (in clock cycles) needed to execute a convoy, approximately equal to the product of the number of convoys and vector length divided by initiation rate. Multiple lanes increase throughput by processing multiple vector elements simultaneously, reducing the effective vector length per lane and thus shortening the chime. For example, a four-lane vector unit can complete four elements per cycle, reducing execution time to vector_length/4 cycles. Chaining within convoys allows overlapping dependent instructions by forwarding first computed elements, preventing additional serialization within a convoy.  
   **External example:** The Cray-1 vector chaining and pipelining concepts are explained in academic literature: https://ieeexplore.ieee.org/document/4585161

3. **Q:** Describe how vector-length registers and strip mining enable vector architectures to efficiently handle vector operations with lengths that are not equal to the maximum vector register size.  
   **A:** Vector-length registers (VLR) specify the active number of elements a vector instruction operates on, which can be less than or equal to the architectural maximum vector length (MVL). Strip mining partitions operations on long vectors into segments smaller or equal to MVL, processing full MVL-sized blocks iteratively, with a smaller final remainder block if needed. This approach allows loops with unknown or dynamic vector lengths to be executed efficiently by setting VLR accordingly and avoids instruction set changes or recompilations for different vector sizes.  
   **External example:** Strip mining and vector-length programming is described in "Computer Architecture: A Quantitative Approach" by Hennessy and Patterson: https://dl.acm.org/doi/book/10.5555/58033

4. **Q:** How do GPUs handle conditional branching within SIMD threads compared to vector architectures, and what are the architectural mechanisms used in Nvidia GPUs to manage divergence and convergence of threads?  
   **A:** GPUs use hardware-managed per-lane masks and a branch synchronization stack to handle control flow divergence in SIMD threads. Predication is supported via 1-bit predicate registers enabling per-lane instruction execution masking. A branch synchronization stack tracks active masks per SIMD thread, allowing divergent paths for different lanes. When lanes reconverge, the stack is popped to restore mask states, managing nested conditionals and loops. Vector architectures rely on software-managed mask registers controlled by the compiler and chaining to handle conditional execution, lacking hardware stacks. GPUs execute all instructions of both THEN and ELSE paths with lanes masked accordingly, which can reduce efficiency, especially with nested divergence.  
   **External example:** NVIDIA’s whitepaper on divergent branches explains this mechanism: https://developer.nvidia.com/blog/efficient-cuda-c-control-flow/

5. **Q:** Discuss the memory hierarchy and bandwidth strategies of vector processors compared to GPUs, including how GPU address coalescing compensates for gather-scatter memory access patterns.  
   **A:** Vector processors utilize large vector registers, deep pipelining of memory accesses, and explicit stride and gather-scatter instructions to amortize high DRAM latency and achieve high bandwidth. Memory is interleaved into many banks to support multiple concurrent accesses, minimizing conflicts. GPUs treat all memory accesses as gathers or scatters with per-lane addresses; hardware address coalescing detects when parallel threads access sequential memory blocks and bundles these accesses into efficient burst requests, optimizing bandwidth. GPUs compensate for high-latency DRAM by massive multithreading to hide latency rather than relying solely on instruction pipelining. Unlike vector processors which may rely heavily on memory bandwidth, GPUs use smaller caches alongside multithreading.  
   **External example:** NVIDIA’s CUDA C Best Practices guide discusses memory coalescing: https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#memory-coalescing

6. **Q:** What are the main reasons SIMD multimedia extensions lack features such as variable vector length, vector masking, and advanced addressing modes present in vector or GPU architectures, and what are the implications for compiler and programmer responsibilities?  
   **A:** SIMD multimedia extensions were designed for ease of implementation with minimal silicon cost and context-switch overhead, reusing existing register files and fixed instruction formats with fixed vector length. This design simplicity came at the cost of expressiveness: lacking variable vector length forces the addition of numerous specialized instructions; absence of advanced addressing modes (strides, gather/scatter) limits automatic vectorization of irregular data; lack of mask registers requires explicit programmer intervention for conditional operations. Consequently, SIMD multimedia heavily relies on programmer-written libraries or assembly for optimization, making compiler automatic vectorization difficult and often incomplete.  
   **External example:** Intel’s developer guide explains the limitations and uses of SSE and AVX: https://software.intel.com/content/www/us/en/develop/articles/intel-simd-intrinsics-guide.html

7. **Q:** In what way do GPUs’ architectural multithreading and SIMD lanes affect their execution efficiency on divergent code paths, and how does this compare to vector processor execution under similar conditions?  
   **A:** GPUs execute SIMD threads with single program counters but manage divergence through hardware masks, executing all branches sequentially with inactive lanes masked off; thus, divergent branches cause serialization and underutilization (e.g., 50% efficiency for IF-THEN-ELSE with equal paths). Multithreading hides latency but does not eliminate this inefficiency. Vector processors use compiler-driven mask registers similarly but tend to be less multithreaded. When few mask bits are active, vector processors have low throughput analogous to GPU divergence inefficiency. However, vector chaining provides some element-level pipelining advantages, while GPU multithreading can better hide latency in practice.  
   **External example:** Analysis of GPU branch divergence inefficiency is found in “Understanding the Efficiency of GPU Architectures” (Owens et al., IEEE Micro 2008): https://doi.org/10.1109/MM.2008.50

8. **Q:** Explain the concepts of VMIPS vector instructions ADDVV.D, ADDVS.D, and the use of scalar and vector operands, including implications for instruction bandwidth and pipeline stalls utilization.  
   **A:** ADDVV.D adds corresponding elements of two vector registers producing another vector register; ADDVS.D adds a scalar register’s value to each element of a vector register creating a vector result. These allow loop vectorization with fewer instructions—e.g., DAXPY loop vectorized into just a few vector instructions instead of hundreds of scalar instructions—dramatically reducing instruction bandwidth. Vector pipelining amortizes pipeline stalls, as element dependencies only stall once per vector instruction, whereas scalar pipelines stall every element, yielding efficient utilization and throughput.  
   **External example:** Smith’s lecture notes on vector processors describe vector-scalar and vector-vector operations and pipeline behavior: https://people.cs.clemson.edu/~dhouse/courses/405/pres/vectorOC.pdf

9. **Q:** How does the Roofline model visually relate hardware floating-point peak performance, memory bandwidth, and kernel arithmetic intensity, and what insight does it provide into the performance differences between NEC SX-9 vector processors and Intel Core i7 multicore processors?  
   **A:** The Roofline model plots attainable GFLOP/s versus arithmetic intensity (FLOPs per byte). The attainable performance is bound by the minimum of peak compute rate (horizontal line) and memory bandwidth times arithmetic intensity (diagonal line). Vector processors (e.g., NEC SX-9) have higher peak compute and much higher memory bandwidth, shifting their ridge point left, meaning more kernels are compute-bound and achieve peak performance. Core i7 has lower bandwidth and compute, ridge point right, limiting peak performance to smaller, high arithmetic intensity kernels. This explains why NEC SX-9 outperforms Core i7 up to 10× on memory-bound kernels.  
   **External example:** The original Roofline paper details this model and comparisons: https://ieeexplore.ieee.org/document/5201466

10. **Q:** Describe the structural and functional role of a multithreaded SIMD processor in GPUs, including how it differs from traditional vector processors and the impact on hardware utilization and thread scheduling.  
    **A:** A multithreaded SIMD processor executes multiple SIMD threads, each containing SIMD instructions executed across SIMD lanes representing vector elements. It contains hardware thread schedulers with scoreboards for latency hiding and can issue instructions from different threads out-of-order to maximize resource usage. Unlike traditional vector processors with few lanes and deep pipelines, GPUs have many lanes with shallow pipelining and heavy multithreading to overlap stalls and maximize throughput despite memory latency, improving utilization. Thread block schedulers assign thread blocks (akin to strip-mined vector loop bodies) to these processors. This results in an architecture optimized for massive fine-grained parallelism with latency hiding via hardware multithreading rather than deep pipelining.  
    **External example:** NVIDIA’s Fermi architecture whitepaper explains multi-threaded SIMD cores: https://www.nvidia.com/content/PDF/fermi_white_papers/NVIDIA_Fermi_Compute_Architecture_Whitepaper.pdf
