1. **Q:** Explain how the introduction of microprocessors and RISC architectures contributed to the unprecedented 17-year performance growth described in the text. What key architectural features enabled this growth and how did market changes facilitate new architecture development?
   **A:** The microprocessor emergence in the late 1970s, leveraging advances in integrated circuit technology, accelerated performance growth to roughly 35% per year. The creation of RISC architectures in the early 1980s, with simpler instructions, exploited instruction-level parallelism through pipelining and multiple instruction issue, alongside the use of caches. Market changes, such as the reduction of dependence on assembly language and the introduction of standardized, vendor-independent operating systems (e.g., UNIX and Linux), lowered risk and costs for new architectures. This fostered successful development of RISC-based machines, which raised the performance bar, causing others like Intel to internally translate complex x86 instructions into more streamlined RISC-like instructions, enabling the 17 years of sustained growth averaging over 50% annually.
   **External example:** The success of ARM processors in mobile devices demonstrates the impact of RISC design and market forces lowering entry barriers: https://ieeexplore.ieee.org/document/7485565

2. **Q:** Differentiate among the five mainstream classes of computing environments introduced. How do their system price, microprocessor price, and critical system design issues vary, and what implications do these have for computer architects?
   **A:** The five classes are Personal Mobile Devices (PMDs), Desktops, Servers, Clusters/Warehouse-scale Computers (WSCs), and Embedded computers. PMDs are low-cost ($100–$1000) with microprocessors priced $10–$100, focusing on cost, energy efficiency, media performance, and responsiveness due to battery and packaging constraints. Desktops range $300–$2500, emphasizing price-performance, energy, and graphics. Servers cost $5000–$10 million+, focusing on throughput, availability, scalability, and price-performance. WSCs cost $100 million to $200 million, with critical design aiming at price-performance, throughput, and energy proportionality. Embedded systems vary widely ($10–$100,000) but prioritize price, energy, and application-specific performance. Architects must tailor performance, energy efficiency, scalability, and cost optimizations according to these differentiated constraints and use-cases.
   **External example:** Typical server design considerations emphasizing scalability and availability are discussed in https://ieeexplore.ieee.org/document/6511733

3. **Q:** Define Instruction Set Architecture (ISA) and describe its seven foundational dimensions as illustrated by MIPS, ARM, and 80x86 examples from the text.
   **A:** ISA is the programmer-visible part of the computer defining the set of instructions, operand types, addressing modes, and control instructions forming the interface between hardware and software. The seven dimensions are: (1) Class of ISA—general-purpose register architectures with load-store (ARM/MIPS) or register-memory (x86) styles; (2) Memory addressing—byte addressing with alignment requirements for ARM/MIPS, not for x86; (3) Addressing modes—register, immediate, displacement, and others like indexed or scaled indexing; (4) Types and sizes of operands—varying from 8-bit to 64-bit integers and floating point formats; (5) Operations—data transfer, arithmetic/logic, control flow, floating point; (6) Control flow instructions—conditional branches, jumps, procedure calls with PC-relative addressing; (7) Instruction encoding—fixed length (ARM/MIPS at 32 bits) vs variable length (x86), affecting complexity and program size. 
   **External example:** A detailed study on RISC versus CISC instruction set designs aligning with these dimensions: https://dl.acm.org/doi/10.1145/358572.358593

4. **Q:** What key trends in semiconductor technologies underlie system design challenges and improvements? Discuss the impact of these trends on processor, DRAM, Flash, disk, and network performance growth as explained in the text.
   **A:** The text highlights five critical semiconductor trends: transistor density increase (~40–55% per year per Moore’s Law), DRAM capacity growth (~25–40% per year, doubling every 2–3 years but slowing), Flash memory capacity growth (~50–60% per year), magnetic disk density growth (slowing from past 100% per year to about 40%), and network performance improvement (enabling Ethernet speeds from 10 Mbits/sec in 1978 to 100 Gbits/sec in 2010). These trends drive design needs like increased processor transistor counts, larger main memory, cheaper large-capacity storage, and higher bandwidth networks. Slowing DRAM growth pressures system memory hierarchies, while power constraints and wire delays limit clock rate increases, pushing architecture toward parallelism and energy efficiency.
   **External example:** An NSF report on microprocessor and memory technology scaling reflecting similar trends: https://www.nsf.gov/pubs/2005/nsf04201/nsf04201.pdf

5. **Q:** Discuss power and energy challenges faced by modern integrated circuits, distinguishing between dynamic and static power consumption. How have processor designs adapted according to the principles and examples in the text?
   **A:** Dynamic power, related to transistor switching, dominates traditionally and is proportional to capacitive load, voltage squared, and switching frequency. Voltage reductions have significantly decreased dynamic power. Static power arises from leakage current even when transistors are off; it grows with transistor count and smaller feature sizes, currently around 25-50% of power in designs with large SRAM caches. To manage power, designs employ clock gating for inactive modules, dynamic voltage and frequency scaling (DVFS), power modes exploiting typical case workloads, and power gating to cut leakage. Processors also implement "race-to-halt" strategies—running faster to let other system parts enter sleep modes. These approaches address thermal and power delivery limits set by current silicon processes.
   **External example:** Intel’s documentation on power management features including DVFS and power gating: https://www.intel.com/content/www/us/en/develop/documentation/64-ia-32-architectures-optimization-manual/top/energy-efficiency-and-thermal-management-technology.html

6. **Q:** How does the text define dependability for computer systems and what quantitative metrics are used? Illustrate with an example how redundancy improves system Mean Time To Failure (MTTF).
   **A:** Dependability is quantified by reliability (MTTF and failure rates) and availability (MTTF/(MTTF + MTTR)). Reliability measures continuous service accomplishment, while availability quantifies the fraction of time a system meets its service specifications. Using exponential failure assumptions and independent failures, system MTTF is the inverse sum of component failure rates. Redundancy improves reliability significantly — for example, duplicating power supplies with independent failures and a repair time of 24 hours increases the power supply MTTF from 200,000 hours to approximately 830 million hours (a 4150x improvement). This dramatically enhances subsystem reliability even if other components remain unchanged.
   **External example:** A reliability engineering overview with redundancy examples by NIST: https://csrc.nist.gov/CSRC/media/Projects/Dependable-Embedded-Systems/documents/reliability.pdf

7. **Q:** What is Amdahl’s Law, and how does it limit performance improvements? Provide an example based on the document text demonstrating the law's application to processor enhancements.
   **A:** Amdahl’s Law states that the overall speedup from a performance enhancement is limited by the fraction of execution time affected by that enhancement. The speedup is: Speedupoverall = 1 / [(1 - Fractionenhanced) + (Fractionenhanced / Speedupenhanced)]. For example, if computation is 40% of execution time and a new processor is 10x faster at computation but I/O dominates remaining time, the overall speedup is only about 1.56x despite the 10x computation improvement. This illustrates diminishing returns; speedup is bounded by the portion of time the enhancement impacts.
   **External example:** Classic description and example of Amdahl's Law in computer architecture context: https://ieeexplore.ieee.org/document/1094026

8. **Q:** Explain the processor performance equation and its components. How can improvements in CPI, instruction count, or clock cycle time collectively influence CPU time, according to the text?
   **A:** CPU time = Instruction count (IC) × Cycles per instruction (CPI) × Clock cycle time. Instruction count depends on the ISA and compiler, CPI depends on processor microarchitecture and pipeline efficiency, and clock cycle time depends on technology and design. Improvements in any component linearly improve CPU time for a fixed workload. However, these parameters often interrelate and cannot be optimized in isolation. For example, reducing CPI by optimizing pipeline stages, lowering IC through better compiler optimization or ISA design, and decreasing clock cycle through faster technology or simpler circuits can collectively lower CPU time.
   **External example:** The ARM architecture explanation of CPI and its effect on performance: https://ieeexplore.ieee.org/document/745159

9. **Q:** How does the principle of locality shape memory hierarchy design and performance improvement strategies? Give an example mentioned or implied in the text that demonstrates temporal and spatial locality.
   **A:** The principle of locality states that programs access data and instructions that were recently used (temporal locality) and those located near to previous accesses in memory (spatial locality). This enables prediction of future accesses based on past behavior and underpins the effectiveness of caching. For example, processor caches store recently accessed data so repeated or sequential accesses hit the cache rather than slower memory. The text mentions the rule-of-thumb that programs spend 90% of execution time in 10% of the code, showing temporal locality, while sequential accesses reflect spatial locality.
   **External example:** Intel’s description of cache design exploiting locality for performance gains: https://software.intel.com/content/www/us/en/develop/articles/performance-analysis-guide-for-intel-cplusplus-compiler.html
