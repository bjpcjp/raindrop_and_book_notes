1. **Q:** How does cache hierarchy design, including levels of cache and inclusion policies, impact processor performance and memory access latency as discussed in the document?
   **A:** Cache hierarchy design reduces memory access latency and miss rates by employing multilevel caches with varying sizes, associativities, and inclusion policies; for example, inclusion ensures data present in upper-level caches also exist in lower levels, reducing cache coherence complexity, while multilevel exclusion allows more efficient cache utilization. Techniques like multibanked and nonblocking caches optimize bandwidth and reduce stalls, which significantly improve performance, as seen in designs like AMD Opteron's multilevel cache system and Intel Core i7's three-level cache hierarchy.
   **External example:** Intel Core processor's multi-level cache hierarchy improves processor speed and efficiency by minimizing latency and increasing hit rates. https://www.intel.com/content/www/us/en/architecture-and-technology/64-bit-architecture.html

2. **Q:** Describe the roles and challenges of branch prediction techniques, including tournament predictors and branch-target buffers, in dynamic instruction scheduling and pipeline performance.
   **A:** Branch prediction techniques such as tournament predictors combine static and dynamic strategies to improve prediction accuracy, while branch-target buffers store target addresses of branches to expedite instruction fetch. These mechanisms reduce control hazards and pipeline stalls in dynamically scheduled processors (e.g., Intel Core i7), maintaining high instruction throughput. However, misprediction causes stalls and flushes, impacting CPI and execution time, necessitating complex hardware to minimize latency and maximize speculative execution benefits.
   **External example:** Modern CPUs like Intel Core i7 implement sophisticated branch prediction units, including tournament predictors and branch-target buffers, to sustain pipeline efficiency. https://ieeexplore.ieee.org/document/5334178

3. **Q:** How does memory coherence, particularly using directory-based protocols and snooping, influence multiprocessor design and performance? Include the role of hardware primitives and scalability considerations.
   **A:** Memory coherence ensures consistency of shared data across caches in multiprocessors via protocols like snooping for small-scale systems and directory-based coherence for large-scale and distributed systems. Directory protocols maintain sharer lists to reduce broadcast traffic, improving scalability. Hardware primitives facilitate atomic operations and locks to synchronize accesses. However, coherence protocol complexity impacts performance and power, and implementations must balance scalability, fault tolerance, and latency, exemplified in systems like AMD Opteron and large-scale DSM multiprocessors.
   **External example:** Directory-based cache coherence protocols improve scalability in multicore processors such as Intel's multiprocessors by reducing coherence traffic. https://www.usenix.org/legacy/event/als03/tech/full_papers/leung/leung_html/node3.html

4. **Q:** What are the key distinctions between GPUs and vector processors in the context of SIMD and multithreading, focusing on memory architecture, instruction execution, and parallelism?
   **A:** GPUs emphasize massive fine-grained multithreading with many lightweight threads to hide memory latency, employing wide SIMD lanes and banked memory structures optimized for throughput rather than latency, supporting divergent branching with hardware mechanisms. Vector processors use longer vectors with dedicated vector registers and memory units, exploiting data-level parallelism with more rigid control flow. GPUs' threading model, as in CUDA, contrasts with vector processors’ length-varying vectors, enabling different programming and performance trade-offs.
   **External example:** CUDA programming exploits GPU fine-grained multithreading and SIMD lanes for high throughput, differing from traditional vector processors like Cray. https://developer.nvidia.com/cuda-zone

5. **Q:** Explain how workload characteristics such as loop-level parallelism, data dependence, and memory access patterns affect compiler optimizations and hardware support for exploiting instruction-level parallelism (ILP).
   **A:** Loop-level parallelism depends on analyzing and resolving data dependences, particularly loop-carried dependencies. Compilers perform dependence analysis, loop unrolling, and scheduling to expose ILP, assisted by hardware techniques like dynamic scheduling, register renaming, and speculation. Memory access patterns influence cache utilization and prefetching efficiency, crucial for sustaining hardware ILP exploitation. Restrictions from dependencies and memory latencies limit parallelism, motivating compiler-hardware joint strategies for performance, especially in vectorizable loops and complex nested structures.
   **External example:** LLVM employs dependence analysis and loop transformations to maximize ILP, leveraging CPU dynamic scheduling. https://llvm.org/docs/PerformanceTips.html

6. **Q:** Discuss the impact of cloud computing and warehouse-scale computer (WSC) architecture on system design, cost, power efficiency, and reliability, as delineated in the document.
   **A:** Cloud computing and WSCs emphasize commodity clusters with scalable, cost-effective designs, optimizing power through energy proportionality, infrastructure innovations (like cooling), and workload management. High availability targets and fault-tolerant network/storage architectures (e.g., erasure coding, replication) maintain reliability. WSCs leverage containerization, orchestration, and specialized memory/storage hierarchies to reduce costs and increase efficiency compared to traditional datacenters, illustrated by Google and Amazon architectures.
   **External example:** Google's warehouse-scale computing leverages container-based designs and power-efficient infrastructure for cost-effective cloud services. https://research.google/pubs/pub35290/

7. **Q:** Clarify the role and implementation of synchronization primitives, such as barriers and atomic operations, in large-scale multiprocessor systems, including their impact on scalability and correctness.
   **A:** Synchronization primitives like barriers coordinate parallel thread execution phases, and atomic operations ensure memory consistency and mutual exclusion. Hardware primitives, such as fetch-and-increment and atomic exchange, support low-latency synchronization in large-scale multiprocessors, mitigating rendezvous overhead. However, synchronization scalability is challenged by contention and latency; efficient algorithms like exponential back-off and distributed barriers enhance performance. Correctness demands that synchronization adhere to memory consistency models, facilitating fault-tolerant parallel execution.
   **External example:** Barrier synchronization is vital in HPC systems such as Cray supercomputers to coordinate processor states efficiently. https://dl.acm.org/doi/10.1145/1282156.1282174

8. **Q:** How do instruction set architecture (ISA) design choices, including addressing modes, instruction encoding, and operation types, influence compiler design, code size, and system performance?
   **A:** ISA design defines addressing modes (immediate, register, memory), instruction encoding (fixed-/variable-length), and operation semantics that shape compiler optimizations, register allocation, and code density. Rich addressing modes can reduce code size but complicate decoding and pipelining, while fixed-length instructions favor simpler hardware and predictability. Operation type variety (e.g., multimedia extensions) allows better exploitation of domain-specific performance. ISA features can limit or enhance compiler scheduling and ILP, as seen in RISC vs. CISC debates and VLIW architectures.
   **External example:** ARM ISA’s fixed-length instruction encoding aids compiler optimizations and low-power execution in embedded systems. https://developer.arm.com/documentation/dui0489/latest

9. **Q:** What are the methods used to measure and analyze performance at different system hierarchy levels, including benchmarks, queuing models, and roofline models, and how do they inform architecture design?
   **A:** Performance evaluation employs benchmarks targeting CPU, memory, storage, and network subsystems, alongside analytical tools like queuing models for I/O and network latency, and roofline models that relate arithmetic intensity to performance and memory bandwidth limits. These analyses identify bottlenecks, guide cache and memory hierarchy design, and inform trade-offs in processor pipeline depth and multithreading degrees. Realistic workloads and multiprogramming scenarios contextualize measurements, leading to architecture optimizations aligning with application demands.
   **External example:** The Roofline performance model helps architects visualize computational and memory limits, informing balanced system design. https://people.eecs.berkeley.edu/~kubitron/courses/cs267-spr10/lectures/04-roofline.pdf
