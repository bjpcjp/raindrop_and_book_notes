1. **Q:** What are the three major types of instruction dependences that affect instruction-level parallelism, and how does each type influence pipeline hazards and execution order?
   **A:** The three types of dependences are: (1) Data dependences (true dependences) where one instruction produces a result used by another, requiring execution order preservation to avoid RAW hazards; (2) Name dependences, which include antidependences (WAR hazards) and output dependences (WAW hazards), arising from reuse of the same registers or memory locations without actual data flow but requiring renaming or preservation of order; and (3) Control dependences, where instruction execution depends on branch outcomes, requiring that instructions not be moved before or after controlling branches in ways that violate program order or correctness. Data dependences establish the necessity of preserving order to maintain data flow, name dependences require renaming for out-of-order execution to avoid hazards, and control dependences maintain correct program behavior including exception handling.
   **External example:** ARM’s architecture uses register renaming to resolve WAW and WAR hazards to enable out-of-order execution. (See ARM Architecture Reference Manual, https://developer.arm.com/documentation/ihi0042/latest)

2. **Q:** Explain how loop unrolling combined with basic compiler scheduling increases instruction-level parallelism and discuss the associated limitations and trade-offs.
   **A:** Loop unrolling replicates the loop body multiple times, increasing the number of instructions within a single basic block, thereby exposing more independent instructions for parallel execution. This larger instruction window enables better scheduling to separate dependent instructions by necessary latencies and to distribute overhead such as branches over multiple computations. Scheduling these unrolled loops reduces pipeline stalls and improves IPC. However, limitations include increased code size, increased register pressure requiring more registers for live values leading to possible spilling, and compiler complexity. Excessive unrolling can cause diminishing returns due to these factors. The balance requires analyzing loop iteration independence, effective register allocation, and memory aliasing.
   **External example:** GCC compiler uses loop unrolling and scheduling optimizations to improve ILP for different architectures. (https://gcc.gnu.org/projects/tree-ssa/vectorizer.html)

3. **Q:** Describe the principles of dynamic scheduling using Tomasulo’s algorithm, focusing on how it detects and overcomes data hazards, and how register renaming is implemented in hardware.
   **A:** Tomasulo’s algorithm dynamically schedules instruction execution allowing out-of-order execution while preserving program correctness. It uses reservation stations for each functional unit to hold instructions and their operands, either as values or as tags referencing producing units. Instructions issue in order if reservation stations and buffers are available; they execute when operands are ready, thus avoiding RAW hazards by waiting until data arrives. Register renaming is performed by allocating tags (reservation station identifiers) as virtual register names to eliminate WAR and WAW hazards, effectively allowing multiple pending writes or reads to the same architectural register without conflicts. Results are broadcast on a common data bus (CDB) to all waiting units, enabling operand forwarding and overlapping. This distributed hazard detection and operand buffering allows higher ILP exploitation.
   **External example:** IBM’s System/360 Model 91 used Tomasulo’s algorithm for floating-point units to improve performance significantly. (IEEE Ann. History of Computing: https://ieeexplore.ieee.org/document/5392149)

4. **Q:** How does hardware-based speculation extend dynamic scheduling, and what mechanisms ensure precise exceptions and correct register/memory state in the presence of speculative execution?
   **A:** Hardware-based speculation extends dynamic scheduling by allowing instructions to execute before branch outcomes are resolved, using branch prediction to speculatively execute down predicted paths. To maintain correctness, speculative execution separates the execution and completion of instructions from their commit phase. Completed instructions write results to a reorder buffer (ROB), which holds results and destination information until instructions reach the head of the ROB and commit in program order. Only at commit do register and memory updates occur, ensuring that speculative effects can be discarded on misprediction by flushing the ROB. This enforces precise exceptions, as exceptions are reported only on instructions that commit, maintaining architectural state consistent with in-order execution despite speculation and out-of-order completion.
   **External example:** Intel’s Pentium Pro processor uses reorder buffers and speculative execution techniques to achieve precise interrupts and high ILP. (Intel IA-32 Architecture Software Developer’s Manual, vol. 3: https://www.intel.com/content/www/us/en/architecture-and-technology/64-ia-32-architectures-software-developer-vol-3a-part-1-manual.html)

5. **Q:** Compare and contrast the different approaches to multiple instruction issue processors, including statically scheduled superscalars, VLIWs, and dynamically scheduled superscalars, highlighting their scheduling methods, hazard detection, and execution order.
   **A:**  
   - Statically scheduled superscalars rely primarily on the compiler to schedule instructions, detect hazards mostly in software, and use in-order execution; hazard detection hardware is minimal, and instructions are issued in program order.  
   - VLIWs are fixed-issue width architectures where the compiler explicitly schedules multiple instructions per very long instruction word, eliminating hardware hazard detection, relying solely on static scheduling, and also use in-order execution. VLIWs require sophisticated compiler support and suffer from code size and binary compatibility challenges.  
   - Dynamically scheduled superscalars perform hardware-based hazard detection, support out-of-order execution and completion, and include features like speculation and register renaming for increased ILP and tolerance of dynamic events like cache misses. Scheduling is done dynamically in hardware.  
   Superscalars are more flexible and achieve compatibility with legacy code easier than VLIWs, while VLIWs achieve simplicity in hardware at the cost of compiler complexity and binary portability.  
   **External example:** The Intel Core i7 exemplifies speculative dynamically scheduled superscalar design, while the TI C6x is a classic VLIW processor. (https://www.intel.com/content/www/us/en/architecture-and-technology/core-i7-processor.html, https://www.ti.com/lit/an/spru701a/spru701a.pdf)

6. **Q:** Explain how control and data dependences together determine the ordering and correct program execution in the presence of branches, using the example involving conditional instructions and branches.
   **A:** Control dependences determine whether an instruction is executed based on branch outcomes, preventing instructions dependent on a branch's outcome from moving before the branch and vice versa. Data dependences enforce the ordering of instructions producing and consuming data values. Together, they maintain correct data flow and exception behavior. For example, in code where an instruction OR uses R1 which can come from two computations depending on a branch, preserving control dependence prevents moving the instruction into an incorrect control flow path, which would alter the data used and violate semantics. Speculation techniques can relax strict control dependence enforcement by allowing instructions to execute before branch resolution while maintaining correct final program state.
   **External example:** The Intel Architecture Software Developer’s Manual explains the importance of control and data dependences to preserve program order for correctness. (https://software.intel.com/content/www/us/en/develop/articles/intel-sdm.html)

7. **Q:** Describe the limitations of instruction-level parallelism and how they influenced the evolution from ILP approaches to multicore architectures.
   **A:** The limitations on ILP arise from data and control dependences which restrict how much parallelism can be extracted from a single thread. Dense data hazards, frequent branches that constrain control flow, and resource constraints limit CPI improvements from extending pipeline depth or width. Aggressive ILP techniques increase hardware complexity and power consumption with diminishing returns. These limitations have been identified as fundamental, leading to a shift in computer architecture towards multicore processors, which exploit thread-level parallelism to scale performance by running multiple threads/cores concurrently rather than extracting more ILP from a single thread.
   **External example:** The “Power Wall” and ILP limitations drove multicore adoption, discussed in the ISCA conference papers on CMPs. (http://www.cs.cmu.edu/~prabal/papers/isca07.pdf)

8. **Q:** How does register renaming in both static and dynamic scheduling help eliminate name dependences, and what is the difference between static and dynamic renaming approaches?
   **A:** Register renaming eliminates name dependences (WAR and WAW hazards) by mapping architectural registers to a larger set of physical or virtual registers so that different uses or definitions of a register do not conflict on the same storage. Static renaming is performed by the compiler before execution, transforming the code to use different registers where possible; dynamic renaming is done in hardware during instruction issue, assigning physical registers or tags (such as reservation station numbers or ROB entries) to operands dynamically. Dynamic renaming allows more flexibility, handles runtime information (like memory aliasing), and supports out-of-order execution and speculation, while static renaming has limited scope and is constrained by compile-time information.
   **External example:** The ARM Cortex-A9 uses dynamic register renaming in hardware to avoid hazards. (ARM Cortex-A9 Technical Reference Manual, https://developer.arm.com/documentation/ddi0406/c)

9. **Q:** In the context of advanced branch prediction, what is the principle behind correlating predictors and tournament predictors, and why do they outperform simple 2-bit predictors?
   **A:** Correlating predictors improve branch prediction by using the history of multiple recent branches (global history) to predict the outcome of a branch, capturing correlations between branches missed by single-branch 2-bit predictors. By indexing prediction counters with both branch address bits and a pattern of recent branch outcomes, they can recognize complex branch behavior patterns. Tournament predictors combine local (per-branch) and global (correlating) prediction schemes and use a selector mechanism to choose the best predictor dynamically per branch based on recent accuracies, achieving better accuracy than either alone. This dynamic adaptive selection allows tournament predictors to perform well across varying code types, outperforming simple 2-bit schemes.
   **External example:** Intel Core i7 employs tournament branch predictors combining multiple prediction schemes for high accuracy. (https://ieeexplore.ieee.org/document/4777783)

10. **Q:** How are loads and stores handled in dynamically scheduled and speculatively executing processors to avoid hazards related to memory dependencies?
    **A:** Loads and stores are managed by calculating effective addresses in program order to maintain memory access order. Loads can begin execution only if no earlier stores in the store buffer have the same address (prevents RAW hazards). Stores wait at the head of the ROB to commit in program order, ensuring WAW/WAR hazards are eliminated as stores update memory only when no earlier conflicting memory operations are pending. The hardware compares load addresses against store buffer addresses to detect conflicts, delaying load execution or speculatively forwarding store data to loads when possible, preserving correctness while enabling out-of-order and speculative memory access.
    **External example:** Intel’s Memory Ordering and Store Buffer implementation in IA-32 architecture allows out-of-order loads and stores with hazard detection. (Intel® 64 and IA-32 Architectures Software Developer’s Manual, Vol. 3: System Programming Guide, https://software.intel.com/content/www/us/en/develop/articles/intel-sdm.html)
