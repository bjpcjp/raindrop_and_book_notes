1. **Q:** Explain the key reasons that led to the shift from uniprocessor performance improvements to multiprocessors exploiting thread-level parallelism (TLP), according to the document.
   **A:** The shift occurred due to diminishing returns in exploiting instruction-level parallelism (ILP), which became inefficient as power and silicon costs grew faster than performance from 2000 to 2005, the growing importance of cloud computing and data-intensive applications needing high-end servers, a decreased focus on desktop performance outside graphics, and an improved understanding of how to efficiently use multiprocessors, especially in natural parallel server environments, where parallelism can be exploited via large datasets or independent requests. Replication also offered design leverage over unique design investments.
   **External example:** Intel's movement to multicore designs reflected similar industry trends focusing on TLP over ILP as captured in Intel's official statement: https://newsroom.intel.com/news-releases/intel-multicore-future/

2. **Q:** Distinguish between symmetric shared-memory multiprocessors (SMP) and distributed shared-memory multiprocessors (DSM) in terms of memory organization, typical scale, and how memory access latency differs.
   **A:** SMPs feature centralized shared memory accessible with uniform latency from usually up to eight processors (small-scale), making them uniform memory access (UMA) systems with shared caches (like multicore L3). DSMs distribute memory physically among processors to increase memory bandwidth and reduce latency to local memory, scaling beyond eight processors; they are nonuniform memory access (NUMA) systems where access latency varies depending on whether memory is local or remote to the requesting processor.
   **External example:** The NUMA architecture in https://www.intel.com/content/www/us/en/develop/documentation/um-and-numa-paper/top/understanding-non-uniform-memory-access-numa.html illustrates NUMA as a distributed memory system.

3. **Q:** Describe the cache coherence problem that arises from caching shared data in multiprocessors and outline the three fundamental properties any coherent memory system must satisfy.
   **A:** When multiple processors cache shared data, their caches may hold different values for the same memory location, leading to incoherence. A coherent memory system must: (1) return the most recently written value to a processor for a read following its own write without intermediate writes by others; (2) return the correct value for a read following a write by another processor once sufficient time has passed and no other intermediate writes occur; and (3) ensure write serialization where all processors observe writes to the same location in the same order, preventing out-of-order visibility of writes.
   **External example:** The MESI protocol described in https://ieeexplore.ieee.org/document/6372609 discusses cache coherence ensuring these properties in multiprocessors.

4. **Q:** Compare and contrast snooping and directory-based cache coherence protocols, including their scalability challenges and typical application in SMP and DSM systems.
   **A:** Snooping protocols rely on broadcasting all cache miss addresses on a shared medium (bus/network) so every cache controller can monitor and maintain coherence; they are suitable for small-scale SMPs but suffer scalability limits as broadcasts to many caches overload bandwidth. Directory-based protocols maintain a directory with per-block sharing information (which nodes hold copies and block state), enabling targeted invalidations and scaling better to large DSM systems; each memory node holds directory info for its memory region, reducing broadcast overhead but adding directory storage and complexity.
   **External example:** A detailed survey on directory coherence protocols is in https://ieeexplore.ieee.org/document/7785061 which discusses scalability benefits over snooping.

5. **Q:** Explain how invalidate-based snooping cache coherence protocols enforce coherence, the key cache block states involved, and the role of bus arbitration in write serialization.
   **A:** In invalidate-based snooping protocols, before a processor writes to a shared cache block, it first invalidates other copies by broadcasting an invalidate message on the bus. The key cache block states are Invalid, Shared (block may be cached by multiple processors), and Modified (block is exclusively owned and dirty). Bus arbitration serializes concurrent write attempts to the same block—only one processor wins bus access to send invalidations and perform the write, thus enforcing write serialization and ensuring only one valid writable copy at a time.
   **External example:** The MESI protocol description in Intel's optimization manual https://software.intel.com/content/www/us/en/develop/articles/intel-64-architecture-optimization-reference-manual.html explains this mechanism.

6. **Q:** Discuss the challenges that arise from implementing snooping protocols on interconnection networks other than a bus and how atomicity and race conditions are managed.
   **A:** Without a bus to serialize events, snooping on interconnection networks faces challenges ensuring atomicity of write or upgrade misses since each coherence action involves several steps and communication delays. To avoid deadlock and ensure correct write serialization (only one winner in a race to write a block), mechanisms like ordering messages, explicit acknowledgments, and the ability to restart losers' miss handling are used. These ensure that coherence operations happen as if atomic, despite network asynchrony.
   **External example:** Research on coherence in networks-on-chip at https://ieeexplore.ieee.org/document/4583697 discusses these challenges and solutions.

7. **Q:** What are true sharing and false sharing misses in the context of cache coherence, how do they differently impact performance, and how does block size affect their rates?
   **A:** True sharing misses occur when multiple processors genuinely share a data item, causing invalidations and coherence misses inherently tied to interprocessor communication. False sharing misses happen when processors invalidate and miss cache blocks due to write operations on different words within the same block, inflating miss counts without corresponding data sharing. Increasing block size typically reduces true sharing misses by capturing more locality but can increase false sharing by expanding blocks, causing more invalidations even when unrelated words are modified.
   **External example:** The impact of false sharing and mitigation techniques are discussed in https://www.usenix.org/legacy/event/usenix01/full_papers/dennis/dennis.pdf.

8. **Q:** Summarize the insights gained from the performance analysis of the OLTP commercial workload regarding L3 cache size, processor count, and block size effects on miss rates and memory traffic.
   **A:** Increasing L3 cache size reduces instruction and capacity/conflict misses but leaves compulsory, true sharing, and false sharing misses mostly unaffected, leading to limited improvement beyond 2 MB due to true sharing misses dominating. Increasing processor count raises true sharing misses, increasing memory access cycles. Increasing block size steadily reduces miss rates (true sharing and compulsory) but increases false sharing misses slightly, with an optimal trade-off around 128 bytes block size for this workload.
   **External example:** Similar observations on workload memory behavior are made in IBM's documentation on OLTP transactions and cache behavior: https://www.ibm.com/docs/en/zos/2.3.0?topic=files-workload-characterization-online-transaction-processing-oltp.

9. **Q:** Describe how synchronization primitives such as spin locks can be efficiently implemented in a multiprocessor system using atomic operations and cache coherence, highlighting the mechanism to reduce bus traffic during spin-waiting.
   **A:** Spin locks use atomic operations like atomic exchange or load-linked/store-conditional to atomically test and set a lock variable (0=unlocked, 1=locked). To reduce bus traffic from constant writes during spin-waiting, a processor spins by repeatedly reading the lock until it observes it is free (value 0), then attempts an atomic swap to acquire the lock. Cache coherence ensures invalidations propagate lock release, and spinning on cached reads avoids flooding the bus with writes, improving performance under contention.
   **External example:** The effectiveness of spin locks using atomic primitives and coherence is analyzed at https://researcher.watson.ibm.com/researcher/files/us-traynor/cas-spinning-locks.pdf.

10. **Q:** Explain the distinction between cache coherence and memory consistency models, using the example in the text, and outline why sequential consistency is considered a straightforward but limiting model.
    **A:** Cache coherence ensures a consistent view of each individual memory location (e.g., the most recent write is visible), whereas memory consistency models define the ordering constraints on reads and writes across multiple memory locations and processors. For example, without strict ordering, two processors could both read stale values due to write propagation delays. Sequential consistency requires the result to be as if all memory references execute in program order on each processor but interleaved arbitrarily—preventing out-of-order visibility but at the cost of performance and complexity in some architectures.
    **External example:** Lamport's sequential consistency model original paper https://dl.acm.org/doi/10.1145/257841.257854 discusses these distinctions and their implications.
