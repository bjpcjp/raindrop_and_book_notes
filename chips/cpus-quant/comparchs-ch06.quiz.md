1. **Q:** What distinguishes warehouse-scale computers (WSCs) from traditional servers and datacenters in terms of architecture, operational goals, and workload types, and how do these differences impact design priorities such as cost-performance, dependability, energy efficiency, and network I/O?
   **A:** WSCs differ fundamentally from traditional servers and datacenters as they act as a single giant machine composed of tens of thousands of servers, focusing heavily on cost-performance and energy efficiency due to scale. Unlike servers that emphasize peak performance under fixed cost and datacenters that consolidate varied services with heterogeneous hardware, WSCs operate with homogeneous hardware tailored for huge scales, balancing long-term amortized infrastructural costs with operational expenses. They exploit ample parallelism, both request-level (independent user requests) and data-level (massive batch processing), unlike servers limited by application parallelism. WSCs rely on software-managed redundancy instead of expensive hardware replication for dependability, targeting at least 99.99% availability. Network I/O is critical to keep data consistent across multiple WSCs and support public interfaces, necessitating specialized hierarchical networks. Batch and interactive workloads coexist in WSCs, requiring flexible design considerations. These aspects demand designs that manage oversubscription in networks, facilitate failure handling, optimize power usage across the facility, and prioritize scalability.
   **External example:** Google’s “The Datacenter as a Computer” book extensively discusses WSC architectural principles focusing on scalability and efficiency distinct from traditional servers: http://research.google.com/pubs/pub35290.html

2. **Q:** How does the hierarchical memory and network architecture within a WSC, including local server memory, rack-level and array-level memory and storage, influence latency, bandwidth, and capacity trade-offs, and what are the implications for data locality and software design?
   **A:** A WSC’s memory and network architecture is hierarchical, where local DRAM has low latency (~0.1 µs) and high bandwidth (~20 GB/s), while rack-level and array-level memory and disk access involve increased latency (up to hundreds of µs to ms) and reduced bandwidth (down to 10 MB/s at array level). Disk access latency is orders of magnitude higher than DRAM access but bandwidth differences decrease at larger network distances due to switch bottlenecks. This hierarchy results in an average memory access latency over 120 times higher when nonlocal accesses are considered, influencing software to focus heavily on data locality within servers or racks. Additionally, network oversubscription between racks compels software to minimize cross-rack communication to avoid performance penalties. Consequently, applications use sharding and partitioning to process data on-site, and replication strategies are optimized to reduce expensive network transfers.
   **External example:** “The Datacenter as a Computer” discusses hierarchical memory and network implications on WSC programming models: http://research.google.com/pubs/pub35290.html

3. **Q:** Describe the power distribution and cooling infrastructure of a WSC, including the typical voltage transformations, efficiencies, and their cumulative impact on overall power utilization effectiveness (PUE), and explain how containerized datacenter designs, as implemented by Google, improve energy efficiency.
   **A:** Power distribution in a WSC involves multiple voltage steps—from 115 kV utility lines down to 208 V usable by servers—with cumulative efficiency around 89%. The chain includes substations (99.7% efficient), UPS systems (94%), power distribution units (98%), and final wiring (99%). Cooling infrastructure, often dominant in power overhead, uses systems like CRAC units, cooling towers, and chillers. Typical PUE across datacenters averages 1.69, with cooling accounting for over half of non-IT power. Google’s containerized datacenter approach modularizes WSCs into ISO shipping containers with integrated cooling and power systems, isolating hot/cold airflows while operating at warmer intake air temperatures (81°F), reducing chiller use and cooling energy significantly. The containers’ tight airflow control and distributed UPS via server-level batteries improve PUE to as low as 1.12, near state-of-art datacenter efficiency.
   **External example:** Google’s published PUE data and container datacenter whitepapers demonstrate modular cooling and power efficiency improvements: https://www.google.com/about/datacenters/efficiency/internal/ (Google datacenter efficiency overview)

4. **Q:** What programming models and workload characteristics are typical in WSCs, and how do frameworks like MapReduce exploit request-level and data-level parallelism to meet the scale and availability requirements unique to WSC environments?
   **A:** WSC workloads include highly parallel, large-scale batch processing and interactive internet services. Popular frameworks such as MapReduce support data-level parallelism by splitting data into independent chunks processed in parallel (map step), followed by aggregation (reduce step). Request-level parallelism arises naturally from millions of independent user requests, each served independently, reducing synchronization needs and improving scalability. These models accommodate failures by re-executing slow or failed tasks, allowing recovery from hardware/software faults without user-visible downtime. Systems rely on underlying scalable storage services (e.g., GFS, Bigtable) that manage replication, consistency, and availability, often relaxing strict ACID compliance in favor of eventual consistency. This approach provides elasticity and fault tolerance crucial for maintaining at least 99.99% availability at massive scale.
   **External example:** The original MapReduce paper from Google illustrates its use for large-scale processing in WSCs: https://research.google/pubs/pub62/

5. **Q:** How do operational costs over the lifetime of a WSC compare to capital expenditure, and how do amortization periods for servers, networking equipment, and facilities influence the cost structure and economic rationale of cloud computing services such as Amazon Web Services (AWS)?
   **A:** WSC operational expenditures (OPEX) including power, cooling, and personnel, amount to approximately 15% of overall costs, while capital expenditure (CAPEX) accounts for about 85%. Servers typically have shorter amortization periods (~3 years), networking gear around 4 years, and facilities 10 years. This leads to servers accounting for a greater proportion of total amortized costs over time. Monthly amortized OPEX per server is roughly $0.11 per hour, which supports cloud providers offering low hourly compute rates. Economies of scale in WSCs allow cloud providers like AWS to offer computing as a utility with pay-as-you-go pricing, with flexible instance types and rental terms (on-demand, reserved, spot). This economic model benefits from exploiting server and facility amortization differences, high utilization, and geographic distribution of facilities, contributing to profitability even at low prices.
   **External example:** AWS cost models and Amazon EC2 pricing detail cost structures and amortization reflected in cloud services: https://aws.amazon.com/ec2/pricing/on-demand/

6. **Q:** What are the main fallacies and pitfalls identified by WSC architects regarding power management, hardware reliability, and cost trade-offs in WSC design, and what are the recommended practices to address these challenges?
   **A:** Key fallacies include assuming cloud providers lose money at low prices; misjudging CAPEX, where server hardware costs exceed facility costs over time due to amortization; believing inactive low power modes save energy—active modes are more responsive and efficient given typical utilization; using overly wimpy processors reduces latency and throughput without software costs; eliminating ECC to reduce cost ignores high DRAM error rates and the resultant overhead for reboot and data corruption; powering down servers during low load is less beneficial given fixed infrastructural costs; replacing all disks with flash ignores cost per capacity disadvantages. Recommended practices include employing ECC memory, favoring energy-proportional hardware, consolidating workloads to maintain utilization, leveraging software-managed redundancy and parallelism, and carefully balancing hardware selection to optimize latency and throughput.
   **External example:** Schroeder et al.’s paper on DRAM errors in WSCs reinforces the need for ECC: https://www.usenix.org/legacy/event/fast09/tech/full_papers/schroeder.pdf

7. **Q:** How does the Google WSC design integrate modular physical infrastructure, custom hardware, and operational strategies to achieve a PUE of approximately 1.23, and what are the key architectural features enabling this efficiency?
   **A:** Google’s WSC design uses modified ISO shipping containers housing 20 racks of servers stacked 20 high, with hot and cold aisles separated via ceiling-mounted racks and underfloor cooling air distributions. Servers have custom power supplies operating at 12V to allow distributed UPS batteries per server, removing large battery rooms and improving power distribution efficiency. Cooling maintains higher supply air temperatures (~81°F) than traditional datacenters, enabled by airflow isolation to prevent hot spots. External cooling towers reduce or eliminate chiller use when ambient temperatures allow, leveraging geographic climate. Extensive real-time monitoring of power usage at circuit level allows fine-tuning. Network architecture employs hierarchical rack and array switches with configurable oversubscription balancing cost and performance. These features together enable PUE improvements reducing cooling overhead and increasing power supply efficiency.
   **External example:** Google’s “Data Center Efficiency” overview describes containerized designs and PUE optimization: https://www.google.com/about/datacenters/efficiency/internal/
