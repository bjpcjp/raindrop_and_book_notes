<html><head><meta charset="utf-8"><title>info-theory</title>
<style>.cards { display:block; }
.card {
  border: 1px solid #e2e2e2;
  border-radius: 12px;
  padding: 12px 14px;
  margin: 10px 0;
  box-shadow: 0 1px 2px rgba(0,0,0,0.04);
}
.card-title {
  margin: 0 0 6px 0;
  font-weight: 600;
  font-size: 1.05rem;
  line-height: 1.3;
}
.card-title a { text-decoration: none; }
.card-image { margin: 6px 0 8px 0; }
.card-image img { display:block; max-width:100%; height:auto; border-radius: 8px; }
.card-excerpt {
  margin: 0;
  font-size: .9rem;
  color: #444;
}
.nav {
  margin: 0 0 12px 0;
  font-size: .9rem;
}
.nav a { text-decoration: none; }
</style></head><body>
<div class="nav">⟵ <a href="index.html">Up</a> &nbsp;|&nbsp; <a href="index.html">Index</a></div>
<h1>info-theory</h1>
<div class="cards">
  <div class="card">
    <div class="card-title"><a href="https://eli.thegreenplace.net/2025/cross-entropy-and-kl-divergence/">Cross-entropy and KL divergence - Eli Bendersky's website</a></div>
    <div class="card-image"><a href="https://eli.thegreenplace.net/2025/cross-entropy-and-kl-divergence/"><img src="https://eli.thegreenplace.net/images/2025/distrib-1-0s.png" alt=""></a></div>
    <p class="card-excerpt"></p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://towardsdatascience.com/understanding-kl-divergence-entropy-and-related-concepts-75e766a2fd9e?source=rss----7f60cf5620c9---4">Understanding KL Divergence Entropy and Related Concepts</a></div>
    <div class="card-image"><a href="https://towardsdatascience.com/understanding-kl-divergence-entropy-and-related-concepts-75e766a2fd9e?source=rss----7f60cf5620c9---4"><img src="https://miro.medium.com/v2/resize:fit:1200/1*MDhu1KyK_OCx71Uuh8I59A.png" alt=""></a></div>
    <p class="card-excerpt">Important concepts in information theory, machine learning, and statistics</p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://towardsdatascience.com/how-to-understand-and-use-jensen-shannon-divergence-b10e11b03fd6?source=rss----7f60cf5620c9---4">How to Understand and Use the Jensen-Shannon Divergence</a></div>
    <div class="card-image"><a href="https://towardsdatascience.com/how-to-understand-and-use-jensen-shannon-divergence-b10e11b03fd6?source=rss----7f60cf5620c9---4"><img src="https://miro.medium.com/v2/resize:fit:1020/1*iXqnpz9Rlxl2X83fAYxyLg.jpeg" alt=""></a></div>
    <p class="card-excerpt">A primer on  the math, logic, and pragmatic application of JS Divergence — including how it is best used in drift monitoring</p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://arxiv.org/pdf/1802.05968">1802</a></div>
    <p class="card-excerpt"></p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://towardsdatascience.com/information-theory-a-gentle-introduction-6abaf99835ac?source=rss----7f60cf5620c9---4">Information Theory: A Gentle Introduction</a></div>
    <p class="card-excerpt">This is the first in a series of articles about Information Theory and its relationship to data driven enterprises and strategy. While…</p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://towardsdatascience.com/link-prediction-and-information-theory-a-tutorial-a67ecc73e7f9?source=rss----7f60cf5620c9---4">Link Prediction and Information Theory: A Tutorial</a></div>
    <div class="card-image"><a href="https://towardsdatascience.com/link-prediction-and-information-theory-a-tutorial-a67ecc73e7f9?source=rss----7f60cf5620c9---4"><img src="https://miro.medium.com/v2/resize:fit:1200/1*VK7jOqx6Vogs_-liMWe5cg.png" alt=""></a></div>
    <p class="card-excerpt">Using Mutual Information to measure the likelihood of candidate links in a graph.</p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://towardsdatascience.com/essential-math-for-data-science-information-theory-5d0380232ca1?source=rss----7f60cf5620c9---4">Essential Math for Data Science: Information Theory</a></div>
    <div class="card-image"><a href="https://towardsdatascience.com/essential-math-for-data-science-information-theory-5d0380232ca1?source=rss----7f60cf5620c9---4"><img src="https://miro.medium.com/v2/resize:fit:967/1*7BxImLe30p6sT_UqwcDgNw.jpeg" alt=""></a></div>
    <p class="card-excerpt">Entropy, cross-entropy, log loss, and KL divergence</p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://machinelearningmastery.com/what-is-information-entropy">A Gentle Introduction to Information Entropy - MachineLearningMastery.com</a></div>
    <div class="card-image"><a href="https://machinelearningmastery.com/what-is-information-entropy"><img src="https://machinelearningmastery.com/wp-content/uploads/2019/10/Plot-of-Probability-Distribution-vs-Entropy.png" alt=""></a></div>
    <p class="card-excerpt">Information theory is a subfield of mathematics concerned with transmitting data across a noisy channel. A cornerstone of information theory is the idea of quantifying how much information there is in a message. More generally, this can be used to quantify the information in an event and a random variable, called entropy, and is calculated using probability. Calculating information and…</p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://towardsdatascience.com/entropy-and-information-gain-b738ca8abd2a?source=rss----7f60cf5620c9---4">Entropy and Information Gain</a></div>
    <div class="card-image"><a href="https://towardsdatascience.com/entropy-and-information-gain-b738ca8abd2a?source=rss----7f60cf5620c9---4"><img src="https://miro.medium.com/v2/da:true/resize:fit:1200/0*i_v4XULShj9hU4nV" alt=""></a></div>
    <p class="card-excerpt">Yet another tool used to make Decision Tree splits.</p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://notamonadtutorial.com/a-brief-introduction-to-the-beauty-of-information-theory-8357f5b6a355">A brief introduction to the beauty of Information Theory</a></div>
    <p class="card-excerpt">Lambdaclass's blog about distributed systems, machine learning, compilers, operating systems, security and cryptography.</p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://en.wikipedia.org/wiki/Gini_coefficient">Gini coefficient</a></div>
    <div class="card-image"><a href="https://en.wikipedia.org/wiki/Gini_coefficient"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Map_of_countries_by_GINI_coefficient_%281990_to_2020%29.svg/1200px-Map_of_countries_by_GINI_coefficient_%281990_to_2020%29.svg.png" alt=""></a></div>
    <p class="card-excerpt">In economics, the Gini coefficient, also known as the Gini index or Gini ratio, is a measure of statistical dispersion intended to represent the income inequality, the wealth inequality, or the consumption inequality within a nation or a social group. It was developed by Italian statistician and sociologist Corrado Gini.</p>
  </div>
</div>
</body></html>
