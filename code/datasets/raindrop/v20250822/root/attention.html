<html><head><meta charset="utf-8"><title>attention</title>
<style>.cards { display:block; }
.card {
  border: 1px solid #e2e2e2;
  border-radius: 12px;
  padding: 12px 14px;
  margin: 10px 0;
  box-shadow: 0 1px 2px rgba(0,0,0,0.04);
}
.card-title {
  margin: 0 0 6px 0;
  font-weight: 600;
  font-size: 1.05rem;
  line-height: 1.3;
}
.card-title a { text-decoration: none; }
.card-image { margin: 6px 0 8px 0; }
.card-image img { display:block; max-width:100%; height:auto; border-radius: 8px; }
.card-excerpt {
  margin: 0;
  font-size: .9rem;
  color: #444;
}
.nav {
  margin: 0 0 12px 0;
  font-size: .9rem;
}
.nav a { text-decoration: none; }
</style></head><body>
<div class="nav">⟵ <a href="index.html">Up</a> &nbsp;|&nbsp; <a href="index.html">Index</a></div>
<h1>attention</h1>
<div class="cards">
  <div class="card">
    <div class="card-title"><a href="https://huggingface.co/blog/Kseniase/attentions?fbclid=IwY2xjawJgrQhleHRuA2FlbQIxMQABHjZ2_OJkyTAVEb-K0wkUoq4VLTCrgIWQ4125yvu1GyuwHh6iHTCmJVAMstBF_aem_advNV4bDfVovH5jPMetwpQ">Topic 33: Slim Attention, KArAt, XAttention and Multi-Token Attention Explained – What’s Really Changing in Transformers?</a></div>
    <div class="card-image"><a href="https://huggingface.co/blog/Kseniase/attentions?fbclid=IwY2xjawJgrQhleHRuA2FlbQIxMQABHjZ2_OJkyTAVEb-K0wkUoq4VLTCrgIWQ4125yvu1GyuwHh6iHTCmJVAMstBF_aem_advNV4bDfVovH5jPMetwpQ"><img src="https://cdn-thumbnails.huggingface.co/social-thumbnails/blog/Kseniase/attentions.png" alt=""></a></div>
    <p class="card-excerpt">A Blog post by Ksenia Se on Hugging Face</p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://arstechnica.com/ai/2024/12/why-ai-language-models-choke-on-too-much-text/">Why AI language models choke on too much text</a></div>
    <div class="card-image"><a href="https://arstechnica.com/ai/2024/12/why-ai-language-models-choke-on-too-much-text/"><img src="https://cdn.arstechnica.net/wp-content/uploads/2024/12/LLM-soup-1152x648.jpg" alt=""></a></div>
    <p class="card-excerpt">Compute costs scale with the square of the input size. That’s not great.</p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://planetbanatt.net/articles/mla.html">On MLA</a></div>
    <div class="card-image"><a href="https://planetbanatt.net/articles/mla.html"><img src="https://planetbanatt.net/images/mla/manifold_perturbation.png" alt=""></a></div>
    <p class="card-excerpt"></p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://www.marktechpost.com/2024/09/13/flashsigmoid-a-hardware-aware-and-memory-efficient-implementation-of-sigmoid-attention-yielding-a-17-inference-kernel-speed-up-over-flashattention-2-on-h100-gpus">FlashSigmoid: A Hardware-Aware and Memory-Efficient Implementation of Sigmoid Attention Yielding a 1</a></div>
    <div class="card-image"><a href="https://www.marktechpost.com/2024/09/13/flashsigmoid-a-hardware-aware-and-memory-efficient-implementation-of-sigmoid-attention-yielding-a-17-inference-kernel-speed-up-over-flashattention-2-on-h100-gpus"><img src="https://www.marktechpost.com/wp-content/uploads/2024/09/Screenshot-2024-09-13-at-5.26.21-PM.png" alt=""></a></div>
    <p class="card-excerpt">Large Language Models (LLMs) have gained significant prominence in modern machine learning, largely due to the attention mechanism. This mechanism employs a sequence-to-sequence mapping to construct context-aware token representations. Traditionally, attention relies on the softmax function (SoftmaxAttn) to generate token representations as data-dependent convex combinations of values. However, despite its widespread adoption and effectiveness, SoftmaxAttn faces several challenges. One key issue is the tendency of the softmax function to concentrate attention on a limited number of features, potentially overlooking other informative aspects of the input data. Also, the application of SoftmaxAttn necessitates a row-wise reduction along the input sequence length,</p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://towardsdatascience.com/understanding-positional-embeddings-in-transformers-from-absolute-to-rotary-31c082e16b26">Understanding Positional Embeddings in Transformers: From Absolute to Rotar</a></div>
    <div class="card-image"><a href="https://towardsdatascience.com/understanding-positional-embeddings-in-transformers-from-absolute-to-rotary-31c082e16b26"><img src="https://miro.medium.com/v2/resize:fit:1200/1*EWz8ImltNHpDjMB8bOq_tQ.png" alt=""></a></div>
    <p class="card-excerpt">A deep dive into absolute, relative, and rotary positional embeddings with code examples</p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://pytorch.org/blog/flashattention-3">FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-preci</a></div>
    <div class="card-image"><a href="https://pytorch.org/blog/flashattention-3"><img src="https://pytorch.org/assets/images/social-share.jpg" alt=""></a></div>
    <p class="card-excerpt">Attention, as a core layer of the ubiquitous Transformer architecture, is a bottleneck for large language models and long-context applications. FlashAttention (and FlashAttention-2) pioneered an approach to speed up attention on GPUs by minimizing memory reads/writes, and is now used by most libraries to accelerate Transformer training and inference. This has contributed to a massive increase in LLM context length in the last two years, from 2-4K (GPT-3, OPT) to 128K (GPT-4), or even 1M (Llama 3). However, despite its success, FlashAttention has yet to take advantage of new capabilities in modern hardware, with FlashAttention-2 achieving only 35% utilization of theoretical max FLOPs on the H100 GPU. In this blogpost, we describe three main techniques to speed up attention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to (1) overlap overall computation and data movement via warp-specialization and (2) interleave block-wise matmul and softmax operations, and (3) incoherent processing that leverages hardware support for FP8 low-precision.</p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://www.marktechpost.com/2024/04/12/deep-learning-architectures-from-cnn-rnn-gan-and-transformers-to-encoder-decoder-architectures">Deep Learning Architectures From CNN, RNN, GAN, and Transformers To Encoder</a></div>
    <div class="card-image"><a href="https://www.marktechpost.com/2024/04/12/deep-learning-architectures-from-cnn-rnn-gan-and-transformers-to-encoder-decoder-architectures"><img src="https://www.marktechpost.com/wp-content/uploads/2024/04/Hn6UWRfcQGS1sXgZTUWw6A.png" alt=""></a></div>
    <p class="card-excerpt">Deep learning architectures have revolutionized the field of artificial intelligence, offering innovative solutions for complex problems across various domains, including computer vision, natural language processing, speech recognition, and generative models. This article explores some of the most influential deep learning architectures: Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Generative Adversarial Networks (GANs), Transformers, and Encoder-Decoder architectures, highlighting their unique features, applications, and how they compare against each other. Convolutional Neural Networks (CNNs) CNNs are specialized deep neural networks for processing data with a grid-like topology, such as images. A CNN automatically detects the important features without any human supervision.</p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://www.quantamagazine.org/how-chain-of-thought-reasoning-helps-neural-networks-compute-20240321">How Chain-of-Thought Reasoning Helps Neural Networks Compute</a></div>
    <div class="card-image"><a href="https://www.quantamagazine.org/how-chain-of-thought-reasoning-helps-neural-networks-compute-20240321"><img src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2024/03/ChainOfThought-byNickSlater-Social.webp" alt=""></a></div>
    <p class="card-excerpt">Large language models do better at solving problems when they show their work. Researchers are beginning to understand why.</p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://www.choicehacking.com/2024/03/03/von-restorff-effect-a-guaranteed-way-to-capture-attention">Von Restorff Effect: A Guaranteed Way to Capture Attention</a></div>
    <div class="card-image"><a href="https://www.choicehacking.com/2024/03/03/von-restorff-effect-a-guaranteed-way-to-capture-attention"><img src="https://www.choicehacking.com/wp-content/uploads/2024/05/Copy-of-Choice-Hacking-Podcast-Social-Image-Client-Logos-21.png" alt=""></a></div>
    <p class="card-excerpt">Brands spend millions every year tracking and analyzing what their competition is doing. And it's not always so they can steal their competition's best ideas. They know this surprising marketing truth: When you do the opposite of what your competition is doing, you'll capture more attention. Why does this work? It's down to a psychological</p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://towardsdatascience.com/attention-for-vision-transformers-explained-70f83984c673?source=rss----7f60cf5620c9---4">Attention for Vision Transformers, Explained</a></div>
    <div class="card-image"><a href="https://towardsdatascience.com/attention-for-vision-transformers-explained-70f83984c673?source=rss----7f60cf5620c9---4"><img src="https://miro.medium.com/v2/da:true/resize:fit:1200/0*XLYx5OALyd914wu7" alt=""></a></div>
    <p class="card-excerpt">The Math and the Code Behind Attention Layers in Computer Vision</p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://open.substack.com/pub/nintyzeros/p/how-do-transformer-workdesign-a-multi">How do transformers work?+Design a Multi-class Sentiment Analysis for Custo</a></div>
    <div class="card-image"><a href="https://open.substack.com/pub/nintyzeros/p/how-do-transformer-workdesign-a-multi"><img src="https://substackcdn.com/image/fetch/w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b143728-e85e-4a5e-af07-98662aa67d5a_727x1024.png" alt=""></a></div>
    <p class="card-excerpt">We will deep dive into understanding how transformer model work like BERT(Non-mathematical Explanation of course!). system design to use the transformer to build a Sentiment Analysis</p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://www.noupe.com/essentials/text-vs-images-which-content-format-is-effective.html">Text vs. Images: Which Content Format is Effective?</a></div>
    <div class="card-image"><a href="https://www.noupe.com/essentials/text-vs-images-which-content-format-is-effective.html"><img src="https://www.noupe.com/wp-content/uploads/2024/02/yououijr0dw.jpg" alt=""></a></div>
    <p class="card-excerpt">When we talk about using different ways to share information, it's like picking the one that fits what you need! Words, pictures, and mixes of both have</p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://betterhumans.coach.me/how-to-seize-attention-with-the-secrets-of-a-sideshow-barker-6788fde4fd75">How to Seize Attention with the Secrets of a Sideshow Barker</a></div>
    <div class="card-image"><a href="https://betterhumans.coach.me/how-to-seize-attention-with-the-secrets-of-a-sideshow-barker-6788fde4fd75"><img src="https://miro.medium.com/v2/resize:fit:700/1*Y8itfAj5ZALVwzWq8kJlUQ.jpeg" alt=""></a></div>
    <p class="card-excerpt">Step right up!</p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://medium.com/re-form/how-to-pay-attention-4751adb53cb6%23.b773a2oo9">Medium</a></div>
    <p class="card-excerpt"></p>
  </div>
  <div class="card">
    <div class="card-title"><a href="http://www.instigatorblog.com/grabbing-attention-and-holding-onto-it/2015/03/09">Grabbing Attention and Holding Onto It</a></div>
    <p class="card-excerpt"></p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://www.theatlantic.com/magazine/archive/2019/09/why-are-washing-machines-learning-to-play-the-harp/594706">Why Washing Machines Are Learning to Play the Harp</a></div>
    <div class="card-image"><a href="https://www.theatlantic.com/magazine/archive/2019/09/why-are-washing-machines-learning-to-play-the-harp/594706"><img src="https://cdn.theatlantic.com/thumbor/8GRQi1Lln5V65qkbz9BswSUMt2Q=/0x43:2000x1085/1200x625/media/img/2019/07/DIS_Biz_Bliss_Sound/original.jpg" alt=""></a></div>
    <p class="card-excerpt">Appliance makers believe more and better chimes, alerts, and jingles make for happier customers. Are they right?</p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://medium.com/re-form/how-to-pay-attention-4751adb53cb6#.b773a2oo9">How To Pay Attention</a></div>
    <div class="card-image"><a href="https://medium.com/re-form/how-to-pay-attention-4751adb53cb6#.b773a2oo9"><img src="https://miro.medium.com/v2/resize:fit:1200/1*5cWSOUarBjoLkXOK9jZwGA.png" alt=""></a></div>
    <p class="card-excerpt">20 Ways To Win The War Against Seeing</p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://www.artofmanliness.com/articles/7-techniques-for-capturing-peoples-attention">7 Techniques for Capturing People's Attention</a></div>
    <div class="card-image"><a href="https://www.artofmanliness.com/articles/7-techniques-for-capturing-peoples-attention"><img src="https://content.artofmanliness.com/uploads/2019/06/attention2.jpg" alt=""></a></div>
    <p class="card-excerpt">The person who can capture and hold attention is the person who can effectively influence human behavior. Here's how to do it.</p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://thesample.ai/blog/advertising">Why I changed my mind about advertising | The Sample blog</a></div>
    <div class="card-image"><a href="https://thesample.ai/blog/advertising"><img src="https://obryant.dev/cards/33542b6fd643d2fc191b573cc2b7ce2c58dba074.png" alt=""></a></div>
    <p class="card-excerpt">I used to be very anti-advertising. Fast forward two years and several pivots, and my slightly-less-early-stage business is doing $900 per month in revenue... from ads.</p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://99u.adobe.com/articles/67148/pay-attention-the-art-of-noticing-rob-walker">Pay Attention: The Art of Noticing - Adobe 99U</a></div>
    <p class="card-excerpt"></p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://markmanson.net/attention-diet">The Attention Diet</a></div>
    <div class="card-image"><a href="https://markmanson.net/attention-diet"><img src="https://markmanson.net/wp-content/uploads/2019/06/attention-diet-cover-image.jpg" alt=""></a></div>
    <p class="card-excerpt">Distractions have become so pervasive in the digital age that we've come to accept them as normal. Here's how we can escape their grip and free our minds a little.</p>
  </div>
</div>
</body></html>
