<div class="nav">

⟵ [Up](index.html)  \|  [Index](index.html)

</div>

# activations

<div class="cards">

<div class="card">

<div class="card-title">

[The Meaning Behind Logistic Classification, from Physics \| by Tim Lou,
PhD](https://towardsdatascience.com/the-meaning-behind-logistic-classification-from-physics-291774fda579)

</div>

<div class="card-image">

[![](https://miro.medium.com/v2/resize:fit:1200/1*twSQInTLX8qC_juaAA-PcQ.png)](https://towardsdatascience.com/the-meaning-behind-logistic-classification-from-physics-291774fda579)

</div>

Why do we use the logistic and softmax functions? Thermal physics may
have an answer.

</div>

<div class="card">

<div class="card-title">

[The Dying ReLU Problem, Clearly
Explained](https://towardsdatascience.com/the-dying-relu-problem-clearly-explained-42d0c54e0d24?source=rss----7f60cf5620c9---4)

</div>

<div class="card-image">

[![](https://miro.medium.com/v2/da:true/resize:fit:1200/0*w80ldgn1iri7dhsM)](https://towardsdatascience.com/the-dying-relu-problem-clearly-explained-42d0c54e0d24?source=rss----7f60cf5620c9---4)

</div>

Keep your neural network alive by understanding the downsides of ReLU

</div>

<div class="card">

<div class="card-title">

[Math \| Obviously
Awesome](https://medium.com/@matelabs_ai/secret-sauce-behind-the-beauty-of-deep-learning-beginners-guide-to-activation-functions-a8e23a57d046)

</div>

<div class="card-image">

[![](https://miro.medium.com/v2/resize:fit:1200/1*zwp7ZQLR4cXgLjI2qrMOKQ.png)](https://medium.com/@matelabs_ai/secret-sauce-behind-the-beauty-of-deep-learning-beginners-guide-to-activation-functions-a8e23a57d046)

</div>

Activation functions are functions which take an input signal and
convert it to an output signal. Activation functions introduce…

</div>

<div class="card">

<div class="card-title">

[Math \| Obviously
Awesome](https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0)

</div>

<div class="card-image">

[![](https://miro.medium.com/v2/resize:fit:325/0*8U8_aa9hMsGmzMY2.)](https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0)

</div>

Recently, a colleague of mine asked me a few questions like “why do we
have so many activation functions?”, “why is that one works better…

</div>

<div class="card">

<div class="card-title">

[Math \| Obviously
Awesome](https://paperswithcode.com/methods/category/activation-functions)

</div>

<div class="card-image">

[![](https://production-media.paperswithcode.com/method_collections/Screen_Shot_2020-07-06_at_12.49.45_PM.png)](https://paperswithcode.com/methods/category/activation-functions)

</div>

Activation functions are functions that we apply in neural networks
after (typically) applying an affine transformation combining weights
and input features. They are typically non-linear functions. The
rectified linear unit, or ReLU, has been the most popular in the past
decade, although the choice is architecture dependent and many
alternatives have emerged in recent years. In this section, you will
find a constantly updating list of activation functions.

</div>

<div class="card">

<div class="card-title">

[5 Must-Know Neural Network Activation
Functions](https://towardsdatascience.com/5-must-know-activation-functions-used-in-neural-networks-8c5052757750?source=rss----7f60cf5620c9---4)

</div>

<div class="card-image">

[![](https://miro.medium.com/v2/resize:fit:1200/1*WGS18KomS4TvWq6S2xYkbA.jpeg)](https://towardsdatascience.com/5-must-know-activation-functions-used-in-neural-networks-8c5052757750?source=rss----7f60cf5620c9---4)

</div>

The essence of non-linearity.

</div>

<div class="card">

<div class="card-title">

[Learning the Differences between Softmax and Sigmoid for Image
Classificati](https://dev.to/rosejcday/learning-the-differences-between-softmax-and-sigmoid-for-image-classification--59c)

</div>

<div class="card-image">

[![](https://media.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fimages6.alphacoders.com%2F368%2F368992.jpg)](https://dev.to/rosejcday/learning-the-differences-between-softmax-and-sigmoid-for-image-classification--59c)

</div>

Week Two - 100 Days of Code Challenge

</div>

<div class="card">

<div class="card-title">

[Deep Learning: Which Loss and Activation Functions should I
use?](https://medium.com/@srnghn/deep-learning-which-loss-and-activation-functions-should-i-use-ac02f1c56aa8)

</div>

<div class="card-image">

[![](https://miro.medium.com/v2/resize:fit:284/1*85yYbdUgMBXpcKw1uHgqNg.png)](https://medium.com/@srnghn/deep-learning-which-loss-and-activation-functions-should-i-use-ac02f1c56aa8)

</div>

The purpose of this post is to provide guidance on which combination of
final-layer activation function and loss function should be used in…

</div>

<div class="card">

<div class="card-title">

[Choosing the right activation function in a neural
network](https://opendatascience.com/blog/choosing-the-right-activation-function-in-a-neural-network)

</div>

<div class="card-image">

[![](https://opendatascience.com/wp-content/uploads/2020/02/Untitled-1.png)](https://opendatascience.com/blog/choosing-the-right-activation-function-in-a-neural-network)

</div>

Stay up-to-date on the latest data science and AI news in the worlds of
artificial intelligence, machine learning, deep learning,
implementation, and more.

</div>

</div>
