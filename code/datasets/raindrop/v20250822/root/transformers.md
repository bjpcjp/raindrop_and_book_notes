<div class="nav">

⟵ [Up](index.html)  \|  [Index](index.html)

</div>

# transformers

<div class="cards">

<div class="card">

<div class="card-title">

[To Make Language Models Work Better, Researchers Sidestep Language \|
Quanta
Magazine](https://www.quantamagazine.org/to-make-language-models-work-better-researchers-sidestep-language-20250414/)

</div>

<div class="card-image">

[![](https://www.quantamagazine.org/wp-content/uploads/2025/04/LLMNativeLanguage-crMyriamWares-Social.jpg)](https://www.quantamagazine.org/to-make-language-models-work-better-researchers-sidestep-language-20250414/)

</div>

We insist that large language models repeatedly translate their
mathematical processes into words. There may be a better way.

</div>

<div class="card">

<div class="card-title">

[How LLMs Store and Use Knowledge? This AI Paper Introduces Knowledge
Circuits: A Framework for Understanding and Improving Knowledge Storage
in Transformer-Based
LLMs](https://www.marktechpost.com/2024/12/14/how-llms-store-and-use-knowledge-this-ai-paper-introduces-knowledge-circuits-a-framework-for-understanding-and-improving-knowledge-storage-in-transformer-based-llms/)

</div>

<div class="card-image">

[![](https://www.marktechpost.com/wp-content/uploads/2024/12/Screenshot-2024-12-14-at-8.35.53%E2%80%AFPM.png)](https://www.marktechpost.com/2024/12/14/how-llms-store-and-use-knowledge-this-ai-paper-introduces-knowledge-circuits-a-framework-for-understanding-and-improving-knowledge-storage-in-transformer-based-llms/)

</div>

Large language models (LLMs) can understand and generate human-like text
by encoding vast knowledge repositories within their parameters. This
capacity enables them to perform complex reasoning tasks, adapt to
various applications, and interact effectively with humans. However,
despite their remarkable achievements, researchers continue to
investigate the mechanisms underlying the storage and utilization of
knowledge in these systems, aiming to enhance their efficiency and
reliability further. A key challenge in using large language models is
their propensity to generate inaccurate, biased, or hallucinatory
outputs. These problems arise from a limited understanding of how such
models organize and access knowledge. Without clear

</div>

<div class="card">

<div class="card-title">

[On MLA](https://planetbanatt.net/articles/mla.html)

</div>

<div class="card-image">

[![](https://planetbanatt.net/images/mla/manifold_perturbation.png)](https://planetbanatt.net/articles/mla.html)

</div>

</div>

<div class="card">

<div class="card-title">

[Transformers Key-Value (KV) Caching
Explained](https://towardsdatascience.com/transformers-key-value-kv-caching-explained-4d71de62d22d?source=rss----7f60cf5620c9---4)

</div>

<div class="card-image">

[![](https://miro.medium.com/v2/resize:fit:1200/1*ub2DQhz0aHT0-Tyaw3hGkQ.png)](https://towardsdatascience.com/transformers-key-value-kv-caching-explained-4d71de62d22d?source=rss----7f60cf5620c9---4)

</div>

Speed up your LLM inference

</div>

<div class="card">

<div class="card-title">

[Hugging Face Releases Sentence Transformers v3.3.0: A Major Leap for
NLP
Efficiency](https://www.marktechpost.com/2024/11/11/hugging-face-releases-sentence-transformers-v3-3-0-a-major-leap-for-nlp-efficiency/)

</div>

<div class="card-image">

[![](https://www.marktechpost.com/wp-content/uploads/2024/11/Screenshot-2024-11-11-at-9.58.28%E2%80%AFAM.png)](https://www.marktechpost.com/2024/11/11/hugging-face-releases-sentence-transformers-v3-3-0-a-major-leap-for-nlp-efficiency/)

</div>

Natural Language Processing (NLP) has rapidly evolved in the last few
years, with transformers emerging as a game-changing innovation. Yet,
there are still notable challenges when using NLP tools to develop
applications for tasks like semantic search, question answering, or
document embedding. One key issue has been the need for models that not
only perform well but also work efficiently on a range of devices,
especially those with limited computational resources, such as CPUs.
Models tend to require substantial processing power to yield high
accuracy, and this trade-off often leaves developers choosing between
performance and practicality. Additionally, deploying large models

</div>

<div class="card">

<div class="card-title">

[ankane/transformers-ruby: State-of-the-art transformers for
Ruby](https://github.com/ankane/transformers-ruby)

</div>

<div class="card-image">

[![](https://opengraph.githubassets.com/8938a007c9fa8dda076ca74c9caa2ef76718a7aa95af35704fea9e3794f051e3/ankane/transformers-ruby)](https://github.com/ankane/transformers-ruby)

</div>

State-of-the-art transformers for Ruby.

</div>

<div class="card">

<div class="card-title">

[Understanding Positional Embeddings in Transformers: From Absolute to
Rotar](https://towardsdatascience.com/understanding-positional-embeddings-in-transformers-from-absolute-to-rotary-31c082e16b26)

</div>

<div class="card-image">

[![](https://miro.medium.com/v2/resize:fit:1200/1*EWz8ImltNHpDjMB8bOq_tQ.png)](https://towardsdatascience.com/understanding-positional-embeddings-in-transformers-from-absolute-to-rotary-31c082e16b26)

</div>

A deep dive into absolute, relative, and rotary positional embeddings
with code examples

</div>

<div class="card">

<div class="card-title">

[Meet Sohu: The World’s First Transformer Specialized Chip
ASIC](https://www.marktechpost.com/2024/06/26/meet-sohu-the-worlds-first-transformer-specialized-chip-asic)

</div>

<div class="card-image">

[![](https://www.marktechpost.com/wp-content/uploads/2024/06/Screenshot-2024-06-26-at-8.35.01-PM-1024x525.png)](https://www.marktechpost.com/2024/06/26/meet-sohu-the-worlds-first-transformer-specialized-chip-asic)

</div>

The Sohu AI chip by Etched is a thundering breakthrough, boasting the
title of the fastest AI chip to date. Its design is a testament to
cutting-edge innovation, aiming to redefine the possibilities within AI
computations and applications. At the center of Sohu's exceptional
performance is its advanced processing capabilities, which enable it to
handle complex computations at unprecedented speeds. With a capability
of processing over 500,000 tokens per second on the Llama 70B model, the
Sohu chip enables the creation of unattainable products with traditional
GPUs. An 8xSohu server can effectively replace 160 H100 GPUs, showcasing
their remarkable efficiency

</div>

<div class="card">

<div class="card-title">

[A Visual Guide to Vision Transformers \|
MDTURP](https://blog.mdturp.ch/posts/2024-04-05-visual_guide_to_vision_transformer.html)

</div>

This is a visual guide (scroll story) to Vision Transformers (ViTs), a
class of deep learning models that have achieved state-of-the-art
performance on image classification tasks.

</div>

<div class="card">

<div class="card-title">

[Mamba Explained](https://thegradient.pub/mamba-explained)

</div>

<div class="card-image">

[![](https://images.unsplash.com/photo-1598348341635-33a3f4205d32?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3wxMTc3M3wwfDF8c2VhcmNofDh8fHRyYW5zZm9ybWVyfGVufDB8fHx8MTcxMTM0NTEwM3ww&ixlib=rb-4.0.3&q=80&w=2000)](https://thegradient.pub/mamba-explained)

</div>

Is Attention all you need? Mamba, a novel AI model based on State Space
Models (SSMs), emerges as a formidable alternative to the widely used
Transformer models, addressing their inefficiency in processing long
sequences.

</div>

<div class="card">

<div class="card-title">

[Position Embeddings for Vision Transformers,
Explained](https://towardsdatascience.com/position-embeddings-for-vision-transformers-explained-a6f9add341d5?source=rss----7f60cf5620c9---4)

</div>

<div class="card-image">

[![](https://miro.medium.com/v2/da:true/resize:fit:1200/0*4SXrUaGxOcNmKG6e)](https://towardsdatascience.com/position-embeddings-for-vision-transformers-explained-a6f9add341d5?source=rss----7f60cf5620c9---4)

</div>

The Math and the Code Behind Position Embeddings in Vision Transformers

</div>

<div class="card">

<div class="card-title">

[Attention for Vision Transformers,
Explained](https://towardsdatascience.com/attention-for-vision-transformers-explained-70f83984c673?source=rss----7f60cf5620c9---4)

</div>

<div class="card-image">

[![](https://miro.medium.com/v2/da:true/resize:fit:1200/0*XLYx5OALyd914wu7)](https://towardsdatascience.com/attention-for-vision-transformers-explained-70f83984c673?source=rss----7f60cf5620c9---4)

</div>

The Math and the Code Behind Attention Layers in Computer Vision

</div>

<div class="card">

<div class="card-title">

[Vision Transformers,
Explained](https://towardsdatascience.com/vision-transformers-explained-a9d07147e4c8?source=rss----7f60cf5620c9---4)

</div>

<div class="card-image">

[![](https://miro.medium.com/v2/da:true/resize:fit:1200/0*5r-4kq_qAbyFBnTt)](https://towardsdatascience.com/vision-transformers-explained-a9d07147e4c8?source=rss----7f60cf5620c9---4)

</div>

A Full Walk-Through of Vision Transformers in PyTorch

</div>

<div class="card">

<div class="card-title">

[\[2302.07730\] Transformer models: an introduction and
catalog](https://arxiv.org/abs/2302.07730)

</div>

<div class="card-image">

[![](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png)](https://arxiv.org/abs/2302.07730)

</div>

In the past few years we have seen the meteoric appearance of dozens of
foundation models of the Transformer family, all of which have memorable
and sometimes funny, but not self-explanatory,...

</div>

<div class="card">

<div class="card-title">

[Hugging Face 101: A Tutorial for Absolute
Beginners!](https://dev.to/pavanbelagatti/hugging-face-101-a-tutorial-for-absolute-beginners-3b0l)

</div>

<div class="card-image">

[![](https://media.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fak3xsk6wk4hv3f6rhf6e.png)](https://dev.to/pavanbelagatti/hugging-face-101-a-tutorial-for-absolute-beginners-3b0l)

</div>

Welcome to this beginner-friendly tutorial on sentiment analysis using
Hugging Face's transformers...

</div>

<div class="card">

<div class="card-title">

[Cracking Open the Hugging Face Transformers
Library](https://towardsdatascience.com/cracking-open-the-hugging-face-transformers-library-350aa0ef0161)

</div>

<div class="card-image">

[![](https://miro.medium.com/v2/da:true/resize:fit:1200/0*Rkoquyw55K6qbFWF)](https://towardsdatascience.com/cracking-open-the-hugging-face-transformers-library-350aa0ef0161)

</div>

A quick-start guide to using open-source LLMs

</div>

<div class="card">

<div class="card-title">

[Optimizing Memory Usage for Training LLMs and Vision Transformers in
PyTorc](https://lightning.ai/pages/community/tutorial/pytorch-memory-vit-llm)

</div>

<div class="card-image">

[![](https://lightningaidev.wpengine.com/wp-content/uploads/2023/07/pytorch-memory-hero.png)](https://lightning.ai/pages/community/tutorial/pytorch-memory-vit-llm)

</div>

This article provides a series of techniques that can lower memory
consumption in PyTorch (when training vision transformers and LLMs) by
approximately 20x without sacrificing modeling performance and
prediction accuracy.

</div>

<div class="card">

<div class="card-title">

[Edge 291: Reinforcement Learning with Human
Feedback](https://thesequence.substack.com/p/edge-291-reinforcement-learning-with)

</div>

<div class="card-image">

[![](https://substackcdn.com/image/fetch/w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd1aaa4cc-10d6-4ada-bba0-f1f2f0793427_1024x1024.png)](https://thesequence.substack.com/p/edge-291-reinforcement-learning-with)

</div>

1\) Reinforcement Learning with Human Feedback(RLHF) 2) The RLHF paper,
3) The transformer reinforcement learning framework.

</div>

<div class="card">

<div class="card-title">

[Meta has built a massive new language AI—and it’s giving it away for
free](https://www.technologyreview.com/2022/05/03/1051691/meta-ai-large-language-model-gpt3-ethics-huggingface-transparency)

</div>

<div class="card-image">

[![](https://wp.technologyreview.com/wp-content/uploads/2022/05/tiles2-1.jpeg?resize=1200,600)](https://www.technologyreview.com/2022/05/03/1051691/meta-ai-large-language-model-gpt3-ethics-huggingface-transparency)

</div>

Facebook’s parent company is inviting researchers to pore over and pick
apart the flaws in its version of GPT-3

</div>

<div class="card">

<div class="card-title">

[What Are Transformer Models and How Do They
Work?](https://txt.cohere.ai/what-are-transformer-models)

</div>

<div class="card-image">

[![](https://cohere.com/_next/image?url=https%3A%2F%2Fcohere-ai.ghost.io%2Fcontent%2Fimages%2F2024%2F06%2FTransformer-Models.jpg&w=640&q=75)](https://txt.cohere.ai/what-are-transformer-models)

</div>

Transformer models are one of the most exciting new developments in
machine learning. They were introduced in the paper Attention is All You
Need. Transformers can be used to write stories, essays, poems, answer
questions, translate between languages, chat with humans, and they can
even pass exams that are hard for humans! But what are they? You’ll be
happy to know that the architecture of transformer models is not that
complex, it simply is a concatenation of some very useful components,
each o

</div>

<div class="card">

<div class="card-title">

[Hacker
News](https://magazine.sebastianraschka.com/p/understanding-large-language-models)

</div>

<div class="card-image">

[![](https://substackcdn.com/image/fetch/w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9a0766d-2e52-4af0-96c5-3e07a30d6ecb_1868x1130.png)](https://magazine.sebastianraschka.com/p/understanding-large-language-models)

</div>

A Cross-Section of the Most Relevant Literature To Get Up to Speed

</div>

<div class="card">

<div class="card-title">

[Hacker News](https://johanwind.github.io/2023/03/23/rwkv_overview.html)

</div>

I explain what is so unique about the RWKV language model.

</div>

<div class="card">

<div class="card-title">

[Optical Transformers](https://arxiv.org/abs/2302.10360)

</div>

<div class="card-image">

[![](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png)](https://arxiv.org/abs/2302.10360)

</div>

The rapidly increasing size of deep-learning models has caused renewed
and growing interest in alternatives to digital computers to
dramatically reduce the energy cost of running state-of-the-art...

</div>

<div class="card">

<div class="card-title">

[Hacker
News](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2)

</div>

Many new Transformer architecture improvements have been proposed since
my last post on “The Transformer Family” about three years ago. Here I
did a big refactoring and enrichment of that 2020 post — restructure the
hierarchy of sections and improve many sections with more recent papers.
Version 2.0 is a superset of the old version, about twice the length.
Notations Symbol Meaning \$d\$ The model size / hidden state dimension /
positional encoding size.

</div>

<div class="card">

<div class="card-title">

[lucidrains/vit-pytorch: Implementation of Vision Transformer, a simple
way to achieve SOTA in vision classification with only a single
transformer encoder, in
Pytorch](https://github.com/lucidrains/vit-pytorch)

</div>

<div class="card-image">

[![](https://opengraph.githubassets.com/9edc731f2a99d5a99777494dd7aaa43716ebad2f0f59b90b9e38e48aaecebb2c/lucidrains/vit-pytorch)](https://github.com/lucidrains/vit-pytorch)

</div>

Implementation of Vision Transformer, a simple way to achieve SOTA in
vision classification with only a single transformer encoder, in
Pytorch - lucidrains/vit-pytorch

</div>

<div class="card">

<div class="card-title">

[All you need to know about ‘Attention’ and ‘Transformers’ — In-depth
Unders](https://towardsdatascience.com/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-2-bf2403804ada)

</div>

<div class="card-image">

[![](https://miro.medium.com/v2/resize:fit:1200/0*Wvg_pNDViACfg-IK.png)](https://towardsdatascience.com/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-2-bf2403804ada)

</div>

Attention, Self-Attention, Multi-head Attention, Masked Multi-head
Attention, Transformers, BERT, and GPT

</div>

<div class="card">

<div class="card-title">

[All you need to know about ‘Attention’ and ‘Transformers’ — In-depth
Unders](https://towardsdatascience.com/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021)

</div>

<div class="card-image">

[![](https://miro.medium.com/v2/resize:fit:925/1*Rv_pntt-N2WL7LMbIptHxQ.png)](https://towardsdatascience.com/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021)

</div>

Attention, Self-Attention, Multi-head Attention, and Transformers

</div>

<div class="card">

<div class="card-title">

[Transformers](https://e2eml.school/transformers.html)

</div>

</div>

<div class="card">

<div class="card-title">

[GPT-J-6B: 6B JAX-Based Transformer – Aran
Komatsuzaki](https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j)

</div>

<div class="card-image">

[![](https://arankomatsuzaki.wordpress.com/wp-content/uploads/2021/05/jax_logo.png?w=1200)](https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j)

</div>

Summary: We have released GPT-J-6B, 6B JAX-based (Mesh) Transformer LM
(Github).GPT-J-6B performs nearly on par with 6.7B GPT-3 (or Curie) on
various zero-shot down-streaming tasks.You can try out …

</div>

<div class="card">

<div class="card-title">

[2106](https://arxiv.org/pdf/2106.04554.pdf)

</div>

</div>

<div class="card">

<div class="card-title">

[NielsRogge/Transformers-Tutorials: This repository contains demos I
made with the Transformers library by
HuggingFace.](https://email.mg2.substack.com/c/eJwlkE1vwyAMhn9NuS3iIwFy4LDLjjtMvUd8uJSVQARkVf79SCtZtmzLev0-VjfwuRxqy7WhMy3t2EAleNYIrUFBe4WyBKcIYdMoZ-TU6IicJAp1uRWAVYeoWtkBbbuJweoWcjoPqJwZFeiuRiLsbI1xYJnUTFvDRi2oBYuJJOP81tW7C5AsKPiDcuQEKKp7a1u9sM8L_erhQ7vvZrB57c13gFh_svfQm2vRqd5yWaHUj-vecgk6VhQUxZRgjll_fqR8IAOfhaZMymniBBNiwFCwhhOjRw5YssuIV0-HupvatH2cYqgo87vZ363v_On2Nexml17XPYV2LJC0ieDeHNqb5ovM4iFB6ZTdopsinFJBMRZYdjIv2x0UE1hwwQjqqi73q6QcwBZBlxSSfwI84vEPRvWPwA)

</div>

<div class="card-image">

[![](https://opengraph.githubassets.com/4f3c8f4c810cac2343d9c600ee9266cf541122b3afad7d56b25d31a26739f230/NielsRogge/Transformers-Tutorials)](https://email.mg2.substack.com/c/eJwlkE1vwyAMhn9NuS3iIwFy4LDLjjtMvUd8uJSVQARkVf79SCtZtmzLev0-VjfwuRxqy7WhMy3t2EAleNYIrUFBe4WyBKcIYdMoZ-TU6IicJAp1uRWAVYeoWtkBbbuJweoWcjoPqJwZFeiuRiLsbI1xYJnUTFvDRi2oBYuJJOP81tW7C5AsKPiDcuQEKKp7a1u9sM8L_erhQ7vvZrB57c13gFh_svfQm2vRqd5yWaHUj-vecgk6VhQUxZRgjll_fqR8IAOfhaZMymniBBNiwFCwhhOjRw5YssuIV0-HupvatH2cYqgo87vZ363v_On2Nexml17XPYV2LJC0ieDeHNqb5ovM4iFB6ZTdopsinFJBMRZYdjIv2x0UE1hwwQjqqi73q6QcwBZBlxSSfwI84vEPRvWPwA)

</div>

This repository contains demos I made with the Transformers library by
HuggingFace. - NielsRogge/Transformers-Tutorials

</div>

<div class="card">

<div class="card-title">

[The Illustrated Transformer – Jay Alammar – Visualizing machine
learning on](https://jalammar.github.io/illustrated-transformer)

</div>

Discussions: Hacker News (65 points, 4 comments), Reddit
r/MachineLearning (29 points, 3 comments) Translations: Arabic, Chinese
(Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian,
Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese
Watch: MIT’s Deep Learning State of the Art lecture referencing this
post Featured in courses at Stanford, Harvard, MIT, Princeton, CMU and
others In the previous post, we looked at Attention – a ubiquitous
method in modern deep learning models. Attention is a concept that
helped improve the performance of neural machine translation
applications. In this post, we will look at The Transformer – a model
that uses attention to boost the speed with which these models can be
trained. The Transformer outperforms the Google Neural Machine
Translation model in specific tasks. The biggest benefit, however, comes
from how The Transformer lends itself to parallelization. It is in fact
Google Cloud’s recommendation to use The Transformer as a reference
model to use their Cloud TPU offering. So let’s try to break the model
apart and look at how it functions. The Transformer was proposed in the
paper Attention is All You Need. A TensorFlow implementation of it is
available as a part of the Tensor2Tensor package. Harvard’s NLP group
created a guide annotating the paper with PyTorch implementation. In
this post, we will attempt to oversimplify things a bit and introduce
the concepts one by one to hopefully make it easier to understand to
people without in-depth knowledge of the subject matter. 2020 Update:
I’ve created a “Narrated Transformer” video which is a gentler approach
to the topic: A High-Level Look Let’s begin by looking at the model as a
single black box. In a machine translation application, it would take a
sentence in one language, and output its translation in another.

</div>

<div class="card">

<div class="card-title">

[Understanding Transformers, the machine learning model behind
GPT-3](https://thenextweb.com/news/understanding-transformers-the-machine-learning-model-behind-gpt-3-machine-learning-ai-syndication)

</div>

<div class="card-image">

[![](https://img-cdn.tnwcdn.com/image/tnw-blurple?filter_last=1&fit=1280%2C640&url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2021%2F05%2FAI-Transformers-abstract-hed.jpg&signature=dab3715e95a415da68eaecb9b4aebcc7)](https://thenextweb.com/news/understanding-transformers-the-machine-learning-model-behind-gpt-3-machine-learning-ai-syndication)

</div>

How this novel neural network architecture changes the way we analyze
complex data types, and powers revolutionary models like GPT-3 and BERT.

</div>

<div class="card">

<div class="card-title">

[How Transformers work in deep learning and NLP: an intuitive
introduction \| AI Summer](https://theaisummer.com/transformer)

</div>

<div class="card-image">

[![](https://theaisummer.com/static/6122618d7e1466853e88473ba375cdc7/ee604/transformer.png)](https://theaisummer.com/transformer)

</div>

An intuitive understanding on Transformers and how they are used in
Machine Translation. After analyzing all subcomponents one by one such
as self-attention and positional encodings , we explain the principles
behind the Encoder and Decoder and why Transformers work so well

</div>

</div>
