<div class="nav">

⟵ [Up](index.html)  \|  [Index](index.html)

</div>

# bandits

<div class="cards">

<div class="card">

<div class="card-title">

[An Overview of Contextual
Bandits](https://towardsdatascience.com/an-overview-of-contextual-bandits-53ac3aa45034?source=rss----7f60cf5620c9---4)

</div>

<div class="card-image">

[![](https://miro.medium.com/v2/resize:fit:1200/1*YOsWxbHyU-J7C0s9KOyUDw.png)](https://towardsdatascience.com/an-overview-of-contextual-bandits-53ac3aa45034?source=rss----7f60cf5620c9---4)

</div>

A dynamic approach to treatment personalization

</div>

<div class="card">

<div class="card-title">

[Dynamic Pricing with Multi-Armed Bandit: Learning by
Doing!](https://towardsdatascience.com/dynamic-pricing-with-multi-armed-bandit-learning-by-doing-3e4550ed02ac)

</div>

<div class="card-image">

[![](https://miro.medium.com/v2/da:true/resize:fit:1200/0*PtB_85QrbCPNJXXb)](https://towardsdatascience.com/dynamic-pricing-with-multi-armed-bandit-learning-by-doing-3e4550ed02ac)

</div>

Applying Reinforcement Learning strategies to real-world use cases,
especially in dynamic pricing, can reveal many surprises

</div>

<div class="card">

<div class="card-title">

[Introduction to Multi-Armed Bandit
Problems](https://www.kdnuggets.com/2023/01/introduction-multiarmed-bandit-problems.html)

</div>

<div class="card-image">

[![](https://www.kdnuggets.com/wp-content/uploads/popovic_introduction_multiarmed_bandit_problems_1.png)](https://www.kdnuggets.com/2023/01/introduction-multiarmed-bandit-problems.html)

</div>

Delve deeper into the concept of multi-armed bandits, reinforcement
learning, and exploration vs. exploitation dilemma.

</div>

<div class="card">

<div class="card-title">

[The Data Incubator is Now Pragmatic Data \| Pragmatic
Institute](https://blog.thedataincubator.com/2016/07/multi-armed-bandits-2)

</div>

<div class="card-image">

[![](https://www.pragmaticinstitute.com/resources/wp-content/uploads/sites/6/2024/01/tditopragmatic.jpg)](https://blog.thedataincubator.com/2016/07/multi-armed-bandits-2)

</div>

As of 2024, The Data Incubator is now Pragmatic Data! Explore Pragmatic
Institute’s new offerings, learn about team training opportunities, and
more.

</div>

<div class="card">

<div class="card-title">

[Multi-armed bandits for dynamic movie
recommendations](https://blog.insightdatascience.com/multi-armed-bandits-for-dynamic-movie-recommendations-5eb8f325ed1d)

</div>

<div class="card-image">

[![](https://miro.medium.com/v2/resize:fit:1200/0*k3FsfPU592DBgMfx.)](https://blog.insightdatascience.com/multi-armed-bandits-for-dynamic-movie-recommendations-5eb8f325ed1d)

</div>

Making the best recommendations to anonymous audiences

</div>

<div class="card">

<div class="card-title">

[Multi-Armed Bandit Algorithms: Thompson
Sampling](https://towardsdatascience.com/multi-armed-bandit-algorithms-thompson-sampling-6d91a88145db?source=rss----7f60cf5620c9---4)

</div>

<div class="card-image">

[![](https://miro.medium.com/v2/da:true/resize:fit:1200/0*9YMbE-00b8VRceig)](https://towardsdatascience.com/multi-armed-bandit-algorithms-thompson-sampling-6d91a88145db?source=rss----7f60cf5620c9---4)

</div>

Intuition, Bayes, and an example

</div>

<div class="card">

<div class="card-title">

[stitchfix/mab: Library for multi-armed bandit selection strategies,
including efficient deterministic implementations of Thompson sampling
and epsilon-greedy.](https://github.com/stitchfix/mab)

</div>

<div class="card-image">

[![](https://repository-images.githubusercontent.com/340162521/d1711100-774c-11eb-86cb-e4a5c793ebc8)](https://github.com/stitchfix/mab)

</div>

Library for multi-armed bandit selection strategies, including efficient
deterministic implementations of Thompson sampling and epsilon-greedy. -
stitchfix/mab

</div>

<div class="card">

<div class="card-title">

[Thompson Sampling using Conjugate
Priors](https://towardsdatascience.com/thompson-sampling-using-conjugate-priors-e0a18348ea2d)

</div>

<div class="card-image">

[![](https://miro.medium.com/v2/da:true/resize:fit:712/1*_Gqr8su3G4inxRnEuTbC4Q.gif)](https://towardsdatascience.com/thompson-sampling-using-conjugate-priors-e0a18348ea2d)

</div>

Multi-Armed Bandits: Part 5b

</div>

<div class="card">

<div class="card-title">

[A Comparison of Bandit
Algorithms](https://towardsdatascience.com/a-comparison-of-bandit-algorithms-24b4adfcabb?source=rss----7f60cf5620c9---4)

</div>

<div class="card-image">

[![](https://miro.medium.com/v2/da:true/resize:fit:1200/0*e7T9wQtgE2cro-mC)](https://towardsdatascience.com/a-comparison-of-bandit-algorithms-24b4adfcabb?source=rss----7f60cf5620c9---4)

</div>

Multi-Armed Bandits: Part 6

</div>

<div class="card">

<div class="card-title">

[Multi-Armed Bandits and the Stitch Fix Experimentation
Platform](https://multithreaded.stitchfix.com/blog/2020/08/05/bandits)

</div>

<div class="card-image">

[![](https://multithreaded.stitchfix.com/assets/posts/2020-08-05-bandits/multi_armed_bandit.png)](https://multithreaded.stitchfix.com/blog/2020/08/05/bandits)

</div>

We've recently built support for multi-armed bandits into the Stitch Fix
experimentation platform. This post will explain how and why.

</div>

<div class="card">

<div class="card-title">

[Bandit
Algorithms](https://towardsdatascience.com/bandit-algorithms-34fd7890cb18?source=rss----7f60cf5620c9---4)

</div>

<div class="card-image">

[![](https://miro.medium.com/v2/da:true/resize:fit:1200/0*Qh6b6kmOXe6Z87sG)](https://towardsdatascience.com/bandit-algorithms-34fd7890cb18?source=rss----7f60cf5620c9---4)

</div>

Multi-Armed Bandits: Part 3

</div>

<div class="card">

<div class="card-title">

[How We Optimized Hero Images on Hotels.com using Multi-Armed Bandit
Algorithms](https://medium.com/expedia-group-tech/how-we-optimized-hero-images-on-hotels-com-using-multi-armed-bandit-algorithms-4503c2c32eae)

</div>

<div class="card-image">

[![](https://miro.medium.com/v2/resize:fit:770/1*4u5qpI-eOTFBbUnH6dVH9w.png)](https://medium.com/expedia-group-tech/how-we-optimized-hero-images-on-hotels-com-using-multi-armed-bandit-algorithms-4503c2c32eae)

</div>

Introducing the multi-armed bandit (MAB) optimization used for hero
images on hotels.com

</div>

<div class="card">

<div class="card-title">

[Exploring the fundamentals of multi-armed
bandits](https://www.microsoft.com/en-us/research/blog/exploring-the-fundamentals-of-multi-armed-bandits)

</div>

<div class="card-image">

[![](https://www.microsoft.com/en-us/research/uploads/prod/2020/02/MSFT_Research_MultiarmedBandit_final_1400X788_final.png)](https://www.microsoft.com/en-us/research/blog/exploring-the-fundamentals-of-multi-armed-bandits)

</div>

Multi-armed bandits are a simple but very powerful framework for
algorithms that make decisions over time under uncertainty.
“Introduction to Multi-Armed Bandits” by Alex Slivkins provides an
accessible, textbook-like treatment of the subject.

</div>

<div class="card">

<div class="card-title">

[Bayesian Bandits explained simply -
MLWhiz](https://mlwhiz.com/blog/2019/07/21/bandits)

</div>

<div class="card-image">

[![](https://mlwhiz.com/images/bandits/1.png)](https://mlwhiz.com/blog/2019/07/21/bandits)

</div>

There are multiple ways to doing the same thing in Pandas, and that
might make it troublesome for the beginner user.This post is about
handling most of the data manipulation cases in Python using a
straightforward, simple, and matter of fact way.

</div>

<div class="card">

<div class="card-title">

[Bandit Algorithms](http://banditalgs.com)

</div>

<div class="card-image">

[![](https://banditalgs.com/wp-content/uploads/2019/03/cropped-bandit-1.png)](http://banditalgs.com)

</div>

</div>

<div class="card">

<div class="card-title">

<https://support.google.com/analytics/answer/2844870>

</div>

</div>

</div>
