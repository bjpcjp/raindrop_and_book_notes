<div class="nav">

⟵ [Up](index.html)  \|  [Index](index.html)

</div>

# gpus

<div class="cards">

<div class="card">

<div class="card-title">

[Writing Your First GPU Kernel in Python with Numba and CUDA -
KDnuggets](https://www.kdnuggets.com/writing-your-first-gpu-kernel-in-python-with-numba-and-cuda)

</div>

<div class="card-image">

[![](https://www.kdnuggets.com/wp-content/uploads/a-clean-vector-illustration-diagram-comp_Tz_o2ZHbQQanglazWiH3cw_IWJ0NepSQi6DkXbE8nWVYA.jpeg)](https://www.kdnuggets.com/writing-your-first-gpu-kernel-in-python-with-numba-and-cuda)

</div>

80x Faster Python? Discover How One Line Turns Your Code Into a GPU
Beast!

</div>

<div class="card">

<div class="card-title">

[Understanding the Landscape of Ampere GPU Memory
Errors](https://arxiv.org/html/2508.03513v1)

</div>

</div>

<div class="card">

<div class="card-title">

[RDNA 4's "Out-of-Order" Memory
Accesses](https://chipsandcheese.com/p/rdna-4s-out-of-order-memory-accesses)

</div>

<div class="card-image">

[![](https://substackcdn.com/image/fetch/$s_!I-Kg!,w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef664cd8-175d-4819-9870-42967d928aef_8280x5512.jpeg)](https://chipsandcheese.com/p/rdna-4s-out-of-order-memory-accesses)

</div>

Examining RDNA 4's out-of-order memory accesses in detail, and
investigating with testing

</div>

<div class="card">

<div class="card-title">

[Understanding the Landscape of Ampere GPU Memory
Errors](https://arxiv.org/abs/2508.03513)

</div>

<div class="card-image">

[![](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png)](https://arxiv.org/abs/2508.03513)

</div>

Graphics Processing Units (GPUs) have become a de facto solution for
accelerating high-performance computing (HPC) applications.
Understanding their memory error behavior is an essential step toward
achieving efficient and reliable HPC systems. In this work, we present a
large-scale cross-supercomputer study to characterize GPU memory
reliability, covering three supercomputers - Delta, Polaris, and
Perlmutter - all equipped with NVIDIA A100 GPUs. We examine error logs
spanning 67.77 million GPU device-hours across 10,693 GPUs. We compare
error rates and mean-time-between-errors (MTBE) and highlight both
shared and distinct error characteristics among these three systems.
Based on these observations and analyses, we discuss the implications
and lessons learned, focusing on the reliable operation of
supercomputers, the choice of checkpointing interval, and the comparison
of reliability characteristics with those of previous-generation GPUs.
Our characterization study provides valuable insights into
fault-tolerant HPC system design and operation, enabling more efficient
execution of HPC applications.

</div>

<div class="card">

<div class="card-title">

[Demystifying GPUs: From Core Architecture to Scalable
Systems](https://dev.to/lewis_won/demystifying-gpus-from-core-architecture-to-scalable-systems-419l)

</div>

<div class="card-image">

[![](https://media2.dev.to/dynamic/image/width=1080,height=1080,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fbozvrojxdvwmyyqmesxw.png)](https://dev.to/lewis_won/demystifying-gpus-from-core-architecture-to-scalable-systems-419l)

</div>

Table of Contents Motivation Optimization goal of GPUs Key concepts of
GPUs - software and...

</div>

<div class="card">

<div class="card-title">

[Demystifying NCCL: An In-depth Analysis of GPU Communication Protocols
and Algorithms](https://hgpu.org/?p=30035)

</div>

<div class="card-image">

[![](https://hgpu.org/img/social-logo.png)](https://hgpu.org/?p=30035)

</div>

The NVIDIA Collective Communication Library (NCCL) is a critical
software layer enabling high-performance collectives on large-scale GPU
clusters. Despite being open source with a documented API, i…

</div>

<div class="card">

<div class="card-title">

[Performance, efficiency, and cost analysis of wafer-scale AI
accelerators vs. single-chip
GPUs](https://www.cell.com/device/fulltext/S2666-9986(25)00147-4)

</div>

<div class="card-image">

[![](https://www.cell.com/cms/asset/f044f4b6-a78b-467a-a5c6-77e06011b269/fx1.jpg)](https://www.cell.com/device/fulltext/S2666-9986(25)00147-4)

</div>

This review compares wafer-scale AI accelerators and single-chip GPUs in
terms of performance, energy efficiency, and cost for high-performance
AI applications. It highlights enabling technologies, such as CoWoS, and
explores future directions including 3D integration, photonic chips, and
emerging semiconductor materials.

</div>

<div class="card">

<div class="card-title">

[Got juice? Future AI processors said to drink up to 15,360 watts of
power — titanic requirements demand exotic immersion and embedded
cooling](https://www.tomshardware.com/pc-components/cooling/future-ai-processors-said-to-consume-up-to-15-360w-massive-power-draw-will-demand-exotic-immersion-and-embedded-cooling-tech)

</div>

<div class="card-image">

[![](https://cdn.mos.cms.futurecdn.net/9CdDvmEpwPNidiZ9DCq3WG.png)](https://www.tomshardware.com/pc-components/cooling/future-ai-processors-said-to-consume-up-to-15-360w-massive-power-draw-will-demand-exotic-immersion-and-embedded-cooling-tech)

</div>

It is going to get hot.

</div>

<div class="card">

<div class="card-title">

[The new AI calculus: Google’s 80% cost edge vs. OpenAI’s
ecosystem](https://venturebeat.com/ai/the-new-ai-calculus-googles-80-cost-edge-vs-openais-ecosystem/)

</div>

<div class="card-image">

[![](https://venturebeat.com/wp-content/uploads/2025/04/ChatGPT-Image-Apr-25-2025-01_21_24-PM.png?w=1024?w=1200&strip=all)](https://venturebeat.com/ai/the-new-ai-calculus-googles-80-cost-edge-vs-openais-ecosystem/)

</div>

Explore the Google vs OpenAI AI ecosystem battle post-o3. Deep dive into
Google's huge cost advantage (TPU vs GPU), agent strategies & model
risks for enterprise

</div>

<div class="card">

<div class="card-title">

[Modern GPU core scheduling](https://arxiv.org/pdf/2503.20481)

</div>

</div>

<div class="card">

<div class="card-title">

[The Future of AI Accelerators: A Roadmap of Industry Leaders The AI… \|
Nader
EL-Masri](https://www.linkedin.com/posts/nader-el-masri-coo_the-future-of-ai-accelerators-a-roadmap-activity-7310216406921281536-EGE2?utm_source=share&utm_medium=member_ios&rcm=ACoAAAAQ_oABqroJaYAjd1pLSvoVsTKPWFYPcIQ)

</div>

<div class="card-image">

[![](https://media.licdn.com/dms/image/v2/D4E22AQH5L9oZ_HqKmw/feedshare-shrink_800/B4EZXKVKTeHUAk-/0/1742856301268?e=2147483647&v=beta&t=9sAsYWDWAetgGE6IJhvBhgTdIQKpuqJD_bDZKI_STbg)](https://www.linkedin.com/posts/nader-el-masri-coo_the-future-of-ai-accelerators-a-roadmap-activity-7310216406921281536-EGE2?utm_source=share&utm_medium=member_ios&rcm=ACoAAAAQ_oABqroJaYAjd1pLSvoVsTKPWFYPcIQ)

</div>

The Future of AI Accelerators: A Roadmap of Industry Leaders The AI
hardware race is heating up, with major players like NVIDIA, AMD, Intel,
Google, Amazon, and more unveiling their upcoming AI accelerators.
Here’s a quick breakdown of the latest trends: Key Takeaways: NVIDIA
Dominance: NVIDIA continues to lead with a robust roadmap, extending
from H100 to future Rubin and Rubin Ultra chips with HBM4 memory by
2026-2027. AMD’s Competitive Push: AMD’s MI300 series is already
competing, with MI350 and future MI400 models on the horizon. Intel’s AI
Ambitions: Gaudi accelerators are growing, with Falcon Shores on track
for a major memory upgrade. Google & Amazon’s Custom Chips: Google’s TPU
lineup expands rapidly, while Amazon’s Trainium & Inferentia gain
traction. Microsoft & Meta’s AI Expansion: Both companies are pushing
their AI chip strategies with Maia and MTIA projects, respectively.
Broadcom & ByteDance Join the Race: New challengers are emerging,
signaling increased competition in AI hardware. What This Means: With
the growing demand for AI and LLMs, companies are racing to deliver
high-performance AI accelerators with advanced HBM (High Bandwidth
Memory) configurations. The next few years will be crucial in shaping
the AI infrastructure landscape. \$NVDA \$AMD \$INTC \$GOOGL \$AMZN
\$META \$AVGO \$ASML \$BESI

</div>

<div class="card">

<div class="card-title">

[AMD's Strix Halo - Under the
Hood](https://chipsandcheese.com/p/amds-strix-halo-under-the-hood)

</div>

<div class="card-image">

[![](https://substackcdn.com/image/fetch/f_auto,q_auto:best,fl_progressive:steep/https%3A%2F%2Fchipsandcheese.substack.com%2Fapi%2Fv1%2Fpost_preview%2F154782290%2Ftwitter.jpg%3Fversion%3D4)](https://chipsandcheese.com/p/amds-strix-halo-under-the-hood)

</div>

Hello you fine Internet folks,

</div>

<div class="card">

<div class="card-title">

[Understanding PTX, the Assembly Language of CUDA GPU Computing \|
NVIDIA Technical
Blog](https://developer.nvidia.com/blog/understanding-ptx-the-assembly-language-of-cuda-gpu-computing/)

</div>

<div class="card-image">

[![](https://developer-blogs.nvidia.com/wp-content/uploads/2025/03/geometric-abstract.jpg)](https://developer.nvidia.com/blog/understanding-ptx-the-assembly-language-of-cuda-gpu-computing/)

</div>

Parallel thread execution (PTX) is a virtual machine instruction set
architecture that has been part of CUDA from its beginning. You can
think of PTX as the…

</div>

<div class="card">

<div class="card-title">

[We Were Wrong About GPUs](https://fly.io/blog/wrong-about-gpu/)

</div>

<div class="card-image">

[![](https://fly.io/blog/wrong-about-gpu/assets/choices-choices-cover.webp)](https://fly.io/blog/wrong-about-gpu/)

</div>

Do my tears surprise you? Strong CEOs also cry.

</div>

<div class="card">

<div class="card-title">

[Demystifying GPU Compute
Architectures](https://open.substack.com/pub/thechipletter/p/demystifying-gpu-compute-architectures?r=oc5d&utm_campaign=post&utm_medium=web&showWelcomeOnShare=false)

</div>

<div class="card-image">

[![](https://substackcdn.com/image/fetch/w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F304b8eae-e3ec-4d5b-a673-75183232003c_1625x1080.jpeg)](https://open.substack.com/pub/thechipletter/p/demystifying-gpu-compute-architectures?r=oc5d&utm_campaign=post&utm_medium=web&showWelcomeOnShare=false)

</div>

Getting 'low level' with Nvidia and AMD GPUs

</div>

<div class="card">

<div class="card-title">

[Inside the AMD Radeon Instinct MI300A's Giant Memory
Subsystem](https://open.substack.com/pub/chipsandcheese/p/inside-the-amd-radeon-instinct-mi300as?r=oc5d&utm_medium=ios)

</div>

<div class="card-image">

[![](https://substackcdn.com/image/fetch/w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1693a082-e333-44b9-9819-18526ca6c365_1661x909.png)](https://open.substack.com/pub/chipsandcheese/p/inside-the-amd-radeon-instinct-mi300as?r=oc5d&utm_medium=ios)

</div>

AMD acquired ATI in 2006, hoping ATI's GPU expertise would combine with
AMD's CPU know-how to create integrated solutions worth more than the
sum of their parts.

</div>

<div class="card">

<div class="card-title">

[Apple-Nvidia collaboration speeds up AI model
production](https://appleinsider.com/articles/24/12/19/apple-nvidia-collaboration-triples-speed-of-ai-model-production)

</div>

<div class="card-image">

[![](https://photos5.appleinsider.com/gallery/62088-128621-57461-117058-56228-114301-lede-xl-xl-xl.jpg)](https://appleinsider.com/articles/24/12/19/apple-nvidia-collaboration-triples-speed-of-ai-model-production)

</div>

Apple's latest machine learning research could make creating models for
Apple Intelligence faster, by coming up with a technique to almost
triple the rate of generating tokens when using Nvidia GPUs.

</div>

<div class="card">

<div class="card-title">

[Intel Arc B580 "Battlemage" GPU Leak Confirms 12 GB Memory, Custom
Models With Standard Power Connectors, Up To 2.8 GHz
Clocks](https://wccftech.com/intel-arc-b580-battlemage-gpu-leak-confirms-12-gb-memory-standard-power-connectors-up-to-2-8-ghz/)

</div>

<div class="card-image">

[![](https://cdn.wccftech.com/wp-content/uploads/2024/11/Intel-Arc-B580-GPU.jpg)](https://wccftech.com/intel-arc-b580-battlemage-gpu-leak-confirms-12-gb-memory-standard-power-connectors-up-to-2-8-ghz/)

</div>

Intel's first Arc B580 GPUs based on the Xe2 "Battlemage" architecture
have been leaked & they look quite compelling.

</div>

<div class="card">

<div class="card-title">

[HPC Gets A Reconfigurable Dataflow Engine To Take On CPUs And
GPUs](https://www.nextplatform.com/2024/10/29/hpc-gets-a-reconfigurable-dataflow-engine-to-take-on-cpus-and-gpus/)

</div>

<div class="card-image">

[![](https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-chip-shot-logo.jpg)](https://www.nextplatform.com/2024/10/29/hpc-gets-a-reconfigurable-dataflow-engine-to-take-on-cpus-and-gpus/)

</div>

No matter how elegant and clever the design is for a compute engine, the
difficulty and cost of moving existing – and sometimes very old – code
from the

</div>

<div class="card">

<div class="card-title">

[\$2 H100s: How the GPU Bubble Burst - by Eugene
Cheah](https://www.latent.space/p/gpu-bubble)

</div>

<div class="card-image">

[![](https://substackcdn.com/image/fetch/w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png)](https://www.latent.space/p/gpu-bubble)

</div>

H100s used to be \$8/hr if you could get them. Now there's 7 different
places sometimes selling them under \$2. What happened?

</div>

<div class="card">

<div class="card-title">

[NVIDIA "Blackwell" GPUs are Sold Out for 12 Months Customers Ordering
in 100K GPU
Quantities](https://www.techpowerup.com/327588/nvidia-blackwell-gpus-are-sold-out-for-12-months-customers-ordering-in-100k-gpu-quantities)

</div>

<div class="card-image">

[![](https://www.techpowerup.com/img/73KcGEAgvEKb8OZL.jpg)](https://www.techpowerup.com/327588/nvidia-blackwell-gpus-are-sold-out-for-12-months-customers-ordering-in-100k-gpu-quantities)

</div>

NVIDIA's "Blackwell" series of GPUs, including B100, B200, and GB200,
are reportedly sold out for 12 months or an entire year. This directly
means that if a new customer is willing to order a new Blackwell GPU
now, there is a 12-month waitlist to get that GPU. Analyst from Morgan
Stanley Joe Moore c...

</div>

<div class="card">

<div class="card-title">

[Why GPU Utilization Falls Short: Understanding Streaming Multiprocessor
(SM) Efficiency for Better
L](https://www.marktechpost.com/2024/09/03/why-gpu-utilization-falls-short-understanding-streaming-multiprocessor-sm-efficiency-for-better-llm-performance)

</div>

<div class="card-image">

[![](https://www.marktechpost.com/wp-content/uploads/2024/09/Screenshot-2024-09-03-at-6.18.46-PM.png)](https://www.marktechpost.com/2024/09/03/why-gpu-utilization-falls-short-understanding-streaming-multiprocessor-sm-efficiency-for-better-llm-performance)

</div>

Large Language Models (LLMs) have gained significant prominence in
recent years, driving the need for efficient GPU utilization in machine
learning tasks. However, researchers face a critical challenge in
accurately assessing GPU performance. The commonly used metric, GPU
Utilization, accessed through nvidia-smi or integrated observability
tools, has proven to be an unreliable indicator of actual computational
efficiency. Surprisingly, 100% GPU utilization can be achieved merely by
reading and writing to memory without performing any computations. This
revelation has sparked a reevaluation of performance metrics and
methodologies in the field of machine learning, prompting researchers to
seek more accurate ways to

</div>

<div class="card">

<div class="card-title">

[The economics of GPUs: How to train your AI model without going
broke](https://venturebeat.com/ai/the-economics-of-gpus-how-to-train-your-ai-model-without-going-broke)

</div>

<div class="card-image">

[![](https://venturebeat.com/wp-content/uploads/2024/08/a-striking-infographic-representation-of-a-modern-ogENkAlsSay9V8TmDeh4JA-F-qBsWnmTRamaqmsAtOp3Q-transformed.jpeg?w=1024?w=1200&strip=all)](https://venturebeat.com/ai/the-economics-of-gpus-how-to-train-your-ai-model-without-going-broke)

</div>

AI won't replace you, but someone using AI will — so it’s time to
embrace AI, and it’s possible to do so even on a low budget.

</div>

<div class="card">

<div class="card-title">

[NVIDIA To Ship 150K-200K Blackwell GB200 AI Servers In Q4 2024 Alone &
500-550K Units In Q1 2025, Microsoft Being The Leading
Buyer](https://wccftech.com/nvidia-ship-150k-200k-blackwell-gb200-ai-servers-q4-2024-500-550k-units-q1-2025/)

</div>

<div class="card-image">

[![](https://cdn.wccftech.com/wp-content/uploads/2024/07/NVIDIA-Blackwell.jpg)](https://wccftech.com/nvidia-ship-150k-200k-blackwell-gb200-ai-servers-q4-2024-500-550k-units-q1-2025/)

</div>

NVIDIA's Blackwell AI servers to witness a massive shipment volume in Q4
2024, with Microsoft being the most "aggressive" acquirer.

</div>

<div class="card">

<div class="card-title">

[Tenstorrent Launches Wormhole AI Processors: 466 FP8 TFLOPS at
300W](https://www.anandtech.com/show/21482/tenstorrent-launches-wormhole-ai-processors-466-fp8-tflops-at-300w)

</div>

<div class="card-image">

[![](https://images.anandtech.com/doci/21482/tesntorrent-wormhole-678_678x452.jpg)](https://www.anandtech.com/show/21482/tenstorrent-launches-wormhole-ai-processors-466-fp8-tflops-at-300w)

</div>

</div>

<div class="card">

<div class="card-title">

[Meet Warp: A Python Framework for Writing High-Performance Simulation
and
G](https://www.marktechpost.com/2024/07/16/meet-warp-a-python-framework-for-writing-high-performance-simulation-and-graphics-code)

</div>

<div class="card-image">

[![](https://www.marktechpost.com/wp-content/uploads/2024/07/Screenshot-2024-07-15-at-11.44.46-PM-1024x704.png)](https://www.marktechpost.com/2024/07/16/meet-warp-a-python-framework-for-writing-high-performance-simulation-and-graphics-code)

</div>

Speed and efficiency are crucial in computer graphics and simulation. It
can be challenging to create high-performance simulations that can run
smoothly on various hardware setups. Traditional methods can be slow and
may not fully utilize the power of modern graphics processing units
(GPUs). This creates a bottleneck for real-time or near-real-time
feedback applications, such as video games, virtual reality
environments, and scientific simulations. Existing solutions for this
problem include using general-purpose computing on graphics processing
units (GPGPU) frameworks like CUDA and OpenCL. These frameworks allow
developers to write programs that can run on GPUs, but they often
require a

</div>

<div class="card">

<div class="card-title">

[FlashAttention-3: Fast and Accurate Attention with Asynchrony and
Low-preci](https://pytorch.org/blog/flashattention-3)

</div>

<div class="card-image">

[![](https://pytorch.org/assets/images/social-share.jpg)](https://pytorch.org/blog/flashattention-3)

</div>

Attention, as a core layer of the ubiquitous Transformer architecture,
is a bottleneck for large language models and long-context applications.
FlashAttention (and FlashAttention-2) pioneered an approach to speed up
attention on GPUs by minimizing memory reads/writes, and is now used by
most libraries to accelerate Transformer training and inference. This
has contributed to a massive increase in LLM context length in the last
two years, from 2-4K (GPT-3, OPT) to 128K (GPT-4), or even 1M (Llama 3).
However, despite its success, FlashAttention has yet to take advantage
of new capabilities in modern hardware, with FlashAttention-2 achieving
only 35% utilization of theoretical max FLOPs on the H100 GPU. In this
blogpost, we describe three main techniques to speed up attention on
Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to (1)
overlap overall computation and data movement via warp-specialization
and (2) interleave block-wise matmul and softmax operations, and (3)
incoherent processing that leverages hardware support for FP8
low-precision.

</div>

<div class="card">

<div class="card-title">

[AMD’s Instinct MI300X AI Throughput Performance & Latency Improved By
7x
Wi](https://wccftech.com/amd-instinct-mi300x-gemm-tuning-ai-throughput-latency-increase-7x)

</div>

<div class="card-image">

[![](https://cdn.wccftech.com/wp-content/uploads/2023/12/AMD-Instinct-MI300A-Germany-Supercomputers-Hunter-Herder.jpg)](https://wccftech.com/amd-instinct-mi300x-gemm-tuning-ai-throughput-latency-increase-7x)

</div>

Nscale has tested AMD's flagship Instinct MI300X AI accelerator
utilizing the GEMM tuning framework, achieving 7x faster performance.

</div>

<div class="card">

<div class="card-title">

[Nvidia Conquers Latest AI
Tests​](https://spectrum.ieee.org/mlperf-nvidia-conquers)

</div>

<div class="card-image">

[![](https://spectrum.ieee.org/media-library/row-upon-row-of-computers-emerging-from-the-darkness.jpg?id=52442384&width=1200&height=600&coordinates=0%2C20%2C0%2C20)](https://spectrum.ieee.org/mlperf-nvidia-conquers)

</div>

GPU maker tops new MLPerf benchmarks on graph neural nets and LLM
fine-tuning

</div>

<div class="card">

<div class="card-title">

[AMD Announces Instinct MI325X Today, CDNA4 To
Come](https://open.substack.com/pub/morethanmoore/p/amd-announces-instinct-mi325x-today?r=oc5d)

</div>

<div class="card-image">

[![](https://substackcdn.com/image/fetch/w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3ea6fdf3-ccbc-4b8f-8dcc-0c1e3edc2639_2723x780.png)](https://open.substack.com/pub/morethanmoore/p/amd-announces-instinct-mi325x-today?r=oc5d)

</div>

A New Annual Cadence for ML

</div>

<div class="card">

<div class="card-title">

[How To Make More Money Renting A GPU Than Nvidia Makes Selling
It](https://www.nextplatform.com/2024/05/02/how-to-make-more-money-renting-a-gpu-than-nvidia-makes-selling-it)

</div>

<div class="card-image">

[![](https://www.nextplatform.com/wp-content/uploads/2024/05/coreweave-logo.jpg)](https://www.nextplatform.com/2024/05/02/how-to-make-more-money-renting-a-gpu-than-nvidia-makes-selling-it)

</div>

It is not a coincidence that the companies that got the most “Hopper”
H100 allocations from Nvidia in 2023 were also the hyperscalers and
cloud builders,

</div>

<div class="card">

<div class="card-title">

[Biden has brought the ban hammer down on US export of AI chips to
China](https://www.theregister.com/2023/10/19/china_biden_ai)

</div>

<div class="card-image">

[![](https://regmedia.co.uk/2023/10/19/hammerdown.jpg)](https://www.theregister.com/2023/10/19/china_biden_ai)

</div>

Datacenter GPUs and some consumer cards now exceed performance limits

</div>

<div class="card">

<div class="card-title">

[Intel preps export-friendly lower-power Gaudi 3 AI chips made for
China](https://www.theregister.com/2024/04/12/intel_paudi_3_china)

</div>

<div class="card-image">

[![](https://regmedia.co.uk/2024/04/12/intel_shutterstock.jpg)](https://www.theregister.com/2024/04/12/intel_paudi_3_china)

</div>

Beijing will be thrilled by this nerfed silicon

</div>

<div class="card">

<div class="card-title">

[Los Alamos Pushes The Memory Wall With “Venado”
Supercomputer](https://www.nextplatform.com/2024/04/15/los-alamos-pushes-the-memory-wall-with-venado-supercomputer)

</div>

<div class="card-image">

[![](https://www.nextplatform.com/wp-content/uploads/2024/04/LANL-Venado-Racks-1024x1024.jpg)](https://www.nextplatform.com/2024/04/15/los-alamos-pushes-the-memory-wall-with-venado-supercomputer)

</div>

Today is the ribbon-cutting ceremony for the “Venado” supercomputer,
which was hinted at back in April 2021 when Nvidia announced its plans
for its first

</div>

<div class="card">

<div class="card-title">

[Intel’s “Gaudi 3” AI accelerator chip may give Nvidia’s H100 a run for
its](https://arstechnica.com/information-technology/2024/04/intels-gaudi-3-ai-accelerator-chip-may-give-nvidias-h100-a-run-for-the-money)

</div>

<div class="card-image">

[![](https://cdn.arstechnica.net/wp-content/uploads/2024/04/newsroom-intel-gaudi-3-2.jpg.rendition.intel_.web_.1648.927.jpg)](https://arstechnica.com/information-technology/2024/04/intels-gaudi-3-ai-accelerator-chip-may-give-nvidias-h100-a-run-for-the-money)

</div>

Intel claims 50% more speed when running AI language models vs. the
market leader.

</div>

<div class="card">

<div class="card-title">

[Nvidia Blackwell Perf TCO Analysis - B100 vs B200 vs
GB200NVL72](https://open.substack.com/pub/semianalysis/p/nvidia-blackwell-perf-tco-analysis?r=oc5d)

</div>

<div class="card-image">

[![](https://substackcdn.com/image/fetch/w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff90660a7-f3df-4ce4-83a5-42642215a7a6_1792x1024.webp)](https://open.substack.com/pub/semianalysis/p/nvidia-blackwell-perf-tco-analysis?r=oc5d)

</div>

GPT-4 Profitability, Cost, Inference Simulator, Parallelism Explained,
Performance TCO Modeling In Large & Small Model Inference and Training

</div>

<div class="card">

<div class="card-title">

[How To Build A Better “Blackwell” GPU Than Nvidia
Did](https://www.nextplatform.com/2024/03/28/how-to-build-a-better-blackwell-gpu-than-nvidia-did)

</div>

<div class="card-image">

[![](https://www.nextplatform.com/wp-content/uploads/2024/03/eliyan-logo2.jpg)](https://www.nextplatform.com/2024/03/28/how-to-build-a-better-blackwell-gpu-than-nvidia-did)

</div>

While a lot of people focus on the floating point and integer processing
architectures of various kinds of compute engines, we are spending more
and more

</div>

<div class="card">

<div class="card-title">

[AMD ROCm Going Open-Source: Will Include Software Stack & Hardware
Document](https://wccftech.com/amd-rocm-going-open-source-will-include-software-stack-hardware-documentation)

</div>

<div class="card-image">

[![](https://cdn.wccftech.com/wp-content/uploads/2023/12/AMD-ROCm-6.0-Now-Available-Main.png)](https://wccftech.com/amd-rocm-going-open-source-will-include-software-stack-hardware-documentation)

</div>

AMD plans to open-source portions of its ROCm software stack and
hardware documentation in a future update to refine its ecosystem.

</div>

<div class="card">

<div class="card-title">

[Lenovo Shows Huge Optimism Towards AMD’s Instinct MI300X AI
Accelerators](https://wccftech.com/lenovo-shows-huge-optimism-towards-amds-instinct-mi300x-ai-accelerators)

</div>

<div class="card-image">

[![](https://cdn.wccftech.com/wp-content/uploads/2023/06/AMD-Instinct-MI300X-GPU-g-low_res-scale-2_00x-Custom-728x361.png)](https://wccftech.com/lenovo-shows-huge-optimism-towards-amds-instinct-mi300x-ai-accelerators)

</div>

Lenovo, the firm emerging as a driving force behind AI computing, has
expressed tremendous optimism about AMD's Instinct MI300X accelerator.

</div>

<div class="card">

<div class="card-title">

[How Nvidia Blackwell Systems Attack 1 Trillion Parameter AI
Models](https://www.nextplatform.com/2024/03/19/how-nvidia-blackwell-systems-attack-1-trillion-parameter-ai-models)

</div>

<div class="card-image">

[![](https://www.nextplatform.com/wp-content/uploads/2024/03/nvidia-blackwell-platform-logo-scaled.jpg)](https://www.nextplatform.com/2024/03/19/how-nvidia-blackwell-systems-attack-1-trillion-parameter-ai-models)

</div>

We like datacenter compute engines here at The Next Platform, but as the
name implies, what we really like are platforms – how compute, storage,

</div>

<div class="card">

<div class="card-title">

[AMD Quietly Funded A Drop-In CUDA Implementation Built On ROCm: It's
Now Op](https://www.phoronix.com/review/radeon-cuda-zluda)

</div>

<div class="card-image">

[![](https://www.phoronix.net/image.php?id=radeon-cuda-zluda&image=radeon_cuda_1)](https://www.phoronix.com/review/radeon-cuda-zluda)

</div>

While there have been efforts by AMD over the years to make it easier to
port codebases targeting NVIDIA's CUDA API to run atop HIP/ROCm, it
still requires work on the part of developers.

</div>

<div class="card">

<div class="card-title">

[Nvidia’s Big Tech Rivals Put Their Own A.I. Chips on the Table - The
New
Yo](https://www.nytimes.com/2024/01/29/technology/ai-chips-nvidia-amazon-google-microsoft-meta.html)

</div>

<div class="card-image">

[![](https://static01.nyt.com/images/2023/12/07/business/00ai-bigtech-chips/00ai-bigtech-chips-largeHorizontalJumbo.jpg)](https://www.nytimes.com/2024/01/29/technology/ai-chips-nvidia-amazon-google-microsoft-meta.html)

</div>

Chafing at their dependence, Amazon, Google, Meta and Microsoft are
racing to cut into Nvidia’s dominant share of the market.

</div>

<div class="card">

<div class="card-title">

[How AMD May Get Across the CUDA
Moat](https://www.hpcwire.com/2023/10/05/how-amd-may-get-across-the-cuda-moat)

</div>

<div class="card-image">

[![](https://www.hpcwire.com/wp-content/uploads/2023/10/AMD-MI300A.png)](https://www.hpcwire.com/2023/10/05/how-amd-may-get-across-the-cuda-moat)

</div>

When discussing GenAI, the term "GPU" almost always enters the
conversation and the topic often moves toward performance and access.
Interestingly, the word "GPU" is assumed to mean "Nvidia" products. (As
an aside, the popular Nvidia hardware used in GenAI are not
technically...

</div>

<div class="card">

<div class="card-title">

[AMD’s Radeon Instinct MI210: GCN Lives
On](https://chipsandcheese.com/2023/07/27/amds-radeon-instinct-mi210-gcn-lives-on)

</div>

<div class="card-image">

[![](https://substackcdn.com/image/fetch/w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0a4027fe-c5c4-4d32-aa25-62a59553af47_1277x715.jpeg)](https://chipsandcheese.com/2023/07/27/amds-radeon-instinct-mi210-gcn-lives-on)

</div>

AMD, Nvidia, and Intel have all diverged their GPU architectures to
separately optimize for compute and graphics.

</div>

<div class="card">

<div class="card-title">

[Installation — Triton
documentation](https://triton-lang.org/main/getting-started/installation.html)

</div>

</div>

<div class="card">

<div class="card-title">

[Introducing Triton: Open-source GPU programming for neural
networks](https://openai.com/research/triton)

</div>

<div class="card-image">

[![](https://images.ctfassets.net/kftzwdyauwt9/cdce1ebd-19a2-4848-a08ec8c44e18/55b924fc6628318148b7c5c4902551e7/image-18.webp?w=1600&h=900&fit=fill)](https://openai.com/research/triton)

</div>

We’re releasing Triton 1.0, an open-source Python-like programming
language which enables researchers with no CUDA experience to write
highly efficient GPU code—most of the time on par with what an expert
would be able to produce.

</div>

<div class="card">

<div class="card-title">

[Calculate Computational Efficiency of Deep Learning Models with FLOPs
and
M](https://www.kdnuggets.com/2023/06/calculate-computational-efficiency-deep-learning-models-flops-macs.html)

</div>

<div class="card-image">

[![](https://www.kdnuggets.com/wp-content/uploads/li_calculate_computational_efficiency_deep_learning_models_flops_macs_2.png)](https://www.kdnuggets.com/2023/06/calculate-computational-efficiency-deep-learning-models-flops-macs.html)

</div>

In this article we will learn about its definition, differences and how
to calculate FLOPs and MACs using Python packages.

</div>

<div class="card">

<div class="card-title">

[AI Capacity Constraints - CoWoS and HBM Supply
Chain](https://www.semianalysis.com/p/ai-capacity-constraints-cowos-and)

</div>

<div class="card-image">

[![](https://substackcdn.com/image/fetch/w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7200eed5-4750-42ef-9709-755af8ec5f0e_2387x2025.png)](https://www.semianalysis.com/p/ai-capacity-constraints-cowos-and)

</div>

Quarterly Ramp for Nvidia, Broadcom, Google, AMD, AMD Embedded (Xilinx),
Amazon, Marvell, Microsoft, Alchip, Alibaba T-Head, ZTE Sanechips,
Samsung, Micron, and SK Hynix

</div>

<div class="card">

<div class="card-title">

[Micron to Introduce GDDR7 Memory in 1H
2024](https://www.tomshardware.com/news/micron-to-introduce-gddr7-memory-in-1h-2024)

</div>

<div class="card-image">

[![](https://cdn.mos.cms.futurecdn.net/Dwk4v483zJSHF7iD4kp7R-1200-80.png)](https://www.tomshardware.com/news/micron-to-introduce-gddr7-memory-in-1h-2024)

</div>

GDDR7 is getting closer, says Micron.

</div>

<div class="card">

<div class="card-title">

[Micron Announces GDDR7 for GPUs Coming in First Half of
2024](https://www.extremetech.com/gaming/micron-announces-gddr7-for-gpus-coming-in-first-half-of-2024)

</div>

<div class="card-image">

[![](https://i.extremetech.com/imagery/content-types/07kN5oasl2Qeky7WDxOa5YW/hero-image.fill.size_1200x675.jpg)](https://www.extremetech.com/gaming/micron-announces-gddr7-for-gpus-coming-in-first-half-of-2024)

</div>

Though it'll arrive just in time for mid-cycle refresh from AMD, Nvidia,
and Intel, it's unclear if there will be any takers just yet.

</div>

<div class="card">

<div class="card-title">

[AI Server Cost Analysis – Memory Is The Biggest
Loser](https://www.semianalysis.com/p/ai-server-cost-analysis-memory-is)

</div>

<div class="card-image">

[![](https://substackcdn.com/image/fetch/w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f05107e-aad1-4b2d-922c-ddbcafb39085_1252x704.jpeg)](https://www.semianalysis.com/p/ai-server-cost-analysis-memory-is)

</div>

Micron \$MU looks very weak in AI

</div>

<div class="card">

<div class="card-title">

[AMD Expands AI/HPC Product Lineup With Flagship GPU-only Instinct
Mi300X
wi](https://www.anandtech.com/show/18915/amd-expands-mi300-family-with-mi300x-gpu-only-192gb-memory)

</div>

<div class="card-image">

[![](https://images.anandtech.com/doci/18915/AMD-Instinct-MI300X-Xray_678x452.jpg)](https://www.anandtech.com/show/18915/amd-expands-mi300-family-with-mi300x-gpu-only-192gb-memory)

</div>

</div>

<div class="card">

<div class="card-title">

[The Third Time Charm Of AMD’s Instinct
GPU](https://www.nextplatform.com/2023/06/14/the-third-time-charm-of-amds-instinct-gpu)

</div>

<div class="card-image">

[![](https://www.nextplatform.com/wp-content/uploads/2023/06/amd-mi300a-die-package-shot-updated.jpg)](https://www.nextplatform.com/2023/06/14/the-third-time-charm-of-amds-instinct-gpu)

</div>

The great thing about the Cambrian explosion in compute that has been
forced by the end of Dennard scaling of clock frequencies and Moore’s
Law lowering

</div>

<div class="card">

<div class="card-title">

[AMD’s RX 7600: Small RDNA 3
Appears](https://chipsandcheese.com/2023/06/04/amds-rx-7600-small-rdna-3-appears)

</div>

<div class="card-image">

[![](https://substackcdn.com/image/fetch/w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F66a8f3e0-7ffa-439f-99b9-480c4cbb7e29_1789x923.png)](https://chipsandcheese.com/2023/06/04/amds-rx-7600-small-rdna-3-appears)

</div>

Editor’s Note (6/14/2023): We have a new article that reevaluates the
cache latency of Navi 31, so please refer to that article for some new
latency data.

</div>

<div class="card">

<div class="card-title">

[The Case for Running AI on CPUs Isn’t Dead
Yet](https://spectrum.ieee.org/ai-cpu)

</div>

<div class="card-image">

[![](https://spectrum.ieee.org/media-library/an-intel-xeon-processor-on-a-black-backdrop-the-processor-is-shown-from-both-above-and-below-displaying-the-thousands-of-conta.jpg?id=33743986&width=1200&height=600&coordinates=0%2C698%2C0%2C698)](https://spectrum.ieee.org/ai-cpu)

</div>

GPUs may dominate, but CPUs could be perfect for smaller AI models

</div>

<div class="card">

<div class="card-title">

[Google dives into the ‘supercomputer’ game by knitting together
purpose-bui](https://venturebeat.com/ai/google-dives-into-the-supercomputer-game-knitting-together-purpose-built-gpus-for-llm-training)

</div>

<div class="card-image">

[![](https://venturebeat.com/wp-content/uploads/2022/12/VB_dictionary-page_romain-vignes-ywqa9IZB-dU-unsplash.jpg?w=1024?w=1200&strip=all)](https://venturebeat.com/ai/google-dives-into-the-supercomputer-game-knitting-together-purpose-built-gpus-for-llm-training)

</div>

Google's new machines combine Nvidia H100 GPUs with Google’s high-speed
interconnections for AI tasks like training very large language models.

</div>

<div class="card">

<div class="card-title">

[Wtf is a kdf? \|
blog.dataparty](https://blog.dataparty.xyz/blog/wtf-is-a-kdf)

</div>

<div class="card-image">

[![](https://blog.dataparty.xyz/images/kdf-images/prison-flames.jpg)](https://blog.dataparty.xyz/blog/wtf-is-a-kdf)

</div>

Earlier this week a letter from an activist imprisoned in France was
posted to the internet. Contained within Ivan Alococo’s dispatch from
the Villepinte prison

</div>

<div class="card">

<div class="card-title">

[Nvidia Tackles Chipmaking Process, Claims 40X Speed Up with
cuLitho](https://www.tomshardware.com/news/nvidia-tackles-chipmaking-process-claims-40x-speed-up-with-culitho)

</div>

<div class="card-image">

[![](https://cdn.mos.cms.futurecdn.net/Vu6N9RDjut8Yy6FiGKNSCB-1200-80.png)](https://www.tomshardware.com/news/nvidia-tackles-chipmaking-process-claims-40x-speed-up-with-culitho)

</div>

Faster masks, less power.

</div>

<div class="card">

<div class="card-title">

[Towards a Benchmarking Suite for Kernel
Tuners](https://hgpu.org/?p=28050)

</div>

<div class="card-image">

[![](https://hgpu.org/img/social-logo.png)](https://hgpu.org/?p=28050)

</div>

As computing system become more complex, it is becoming harder for
programmers to keep their codes optimized as the hardware gets updated.
Autotuners try to alleviate this by hiding as many archite…

</div>

<div class="card">

<div class="card-title">

[Meet the \$10,000 Nvidia chip powering the race for
A.I.](https://www.cnbc.com/2023/02/23/nvidias-a100-is-the-10000-chip-powering-the-race-for-ai-.html)

</div>

<div class="card-image">

[![](https://image.cnbcfm.com/api/v1/image/107199183-1677187677231-nvidia-a100-sxm4-80gb_mid.jpg?v=1678712289&w=1920&h=1080)](https://www.cnbc.com/2023/02/23/nvidias-a100-is-the-10000-chip-powering-the-race-for-ai-.html)

</div>

The \$10,000 Nvidia A100has become one of the most critical tools in the
artificial intelligence industry,

</div>

<div class="card">

<div class="card-title">

[Hacker
News](https://timdettmers.com/2023/01/16/which-gpu-for-deep-learning)

</div>

<div class="card-image">

[![](https://timdettmers.com/wp-content/uploads/2023/01/GPUs_Ada_performance_per_dollar6.png)](https://timdettmers.com/2023/01/16/which-gpu-for-deep-learning)

</div>

Here, I provide an in-depth analysis of GPUs for deep learning/machine
learning and explain what is the best GPU for your use-case and budget.

</div>

<div class="card">

<div class="card-title">

[CUDA Toolkit 12.0 Released for General
Availability](https://developer.nvidia.com/blog/cuda-toolkit-12-0-released-for-general-availability)

</div>

<div class="card-image">

[![](https://developer-blogs.nvidia.com/wp-content/uploads/2022/01/cuda-image-16x9-1.jpg)](https://developer.nvidia.com/blog/cuda-toolkit-12-0-released-for-general-availability)

</div>

NVIDIA announces the newest CUDA Toolkit software release, 12.0. This
release is the first major release in many years and it focuses on new
programming models and CUDA application acceleration…

</div>

<div class="card">

<div class="card-title">

[How to Accelerate your PyTorch GPU Training with
XLA](https://towardsdatascience.com/how-to-accelerate-your-pytorch-training-with-xla-on-aws-3d599bc8f6a9)

</div>

<div class="card-image">

[![](https://miro.medium.com/v2/da:true/resize:fit:1200/0*U3cCzkY7bhrJxvv-)](https://towardsdatascience.com/how-to-accelerate-your-pytorch-training-with-xla-on-aws-3d599bc8f6a9)

</div>

The Power of PyTorch/XLA and how Amazon SageMaker Training Compiler
Simplifies its use

</div>

<div class="card">

<div class="card-title">

[New Video Gives Us One More Reason to Never Buy a Used Mining
GPU](https://www.extremetech.com/computing/339849-new-video-gives-us-one-more-reason-to-never-buy-a-used-mining-gpu)

</div>

<div class="card-image">

[![](https://i.extremetech.com/imagery/content-types/06MF724Fc0rBICjz2ZXxRkF/hero-image.fill.size_1200x675.jpg)](https://www.extremetech.com/computing/339849-new-video-gives-us-one-more-reason-to-never-buy-a-used-mining-gpu)

</div>

A new video making the rounds purports to show Vietnamese crypto miners
preparing used GPUs for resale by blasting them with a pressure washer.

</div>

<div class="card">

<div class="card-title">

[Nvidia Research Plots A Course To Multiple Multichip GPU
Engines](https://www.nextplatform.com/2022/01/06/nvidia-research-plots-a-course-to-multiple-multichip-gpu-engines)

</div>

<div class="card-image">

[![](https://www.nextplatform.com/wp-content/uploads/2020/12/nvidia-dpu-logo.jpg)](https://www.nextplatform.com/2022/01/06/nvidia-research-plots-a-course-to-multiple-multichip-gpu-engines)

</div>

There are two types of packaging that represent the future of computing,
and both will have validity in certain domains: Wafer scale integration
and

</div>

<div class="card">

<div class="card-title">

[GPUCC - An Open-Source GPGPU
Compiler](https://research.google/pubs/pub45226)

</div>

<div class="card-image">

[![](https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg)](https://research.google/pubs/pub45226)

</div>

</div>

<div class="card">

<div class="card-title">

[3D Stacking Could Boost GPU Machine
Learning](https://www.nextplatform.com/2017/03/14/3d-stacking-boost-gpu-machine-learning)

</div>

<div class="card-image">

[![](https://www.nextplatform.com/wp-content/uploads/2015/11/TeslaGPU2.jpg)](https://www.nextplatform.com/2017/03/14/3d-stacking-boost-gpu-machine-learning)

</div>

Nvidia has staked its growth in the datacenter on machine learning. Over
the past few years, the company has rolled out features in its GPUs
aimed neural

</div>

<div class="card">

<div class="card-title">

[20161018 li](https://pure.tue.nl/ws/files/39759895/20161018_Li.pdf)

</div>

</div>

<div class="card">

<div class="card-title">

[How FPGAs Can Take On GPUs And Knights
Landing](https://www.nextplatform.com/2016/03/17/fpgas-can-take-gpus-knights-landing)

</div>

<div class="card-image">

[![](https://www.nextplatform.com/wp-content/uploads/2016/02/nallatech-t510-bw.jpg)](https://www.nextplatform.com/2016/03/17/fpgas-can-take-gpus-knights-landing)

</div>

Nallatech doesn't make FPGAs, but it does have several decades of
experience turning FPGAs into devices and systems that companies can
deploy to solve

</div>

<div class="card">

<div class="card-title">

[Analysis and Comparison of Performance and Power Consumption of Neural
Netw](https://hgpu.org/?p=25937)

</div>

<div class="card-image">

[![](https://hgpu.org/img/social-logo.png)](https://hgpu.org/?p=25937)

</div>

In this work, we analyze the performance of neural networks on a variety
of heterogenous platforms. We strive to find the best platform in terms
of raw benchmark performance, performance per watt a…

</div>

<div class="card">

<div class="card-title">

[baidu-research/warp-ctc](https://github.com/baidu-research/warp-ctc)

</div>

<div class="card-image">

[![](https://opengraph.githubassets.com/cc9d14d9a9af338e6735ce2004834e2cdc107b05355abd8fe0c30fbf595d678f/baidu-research/warp-ctc)](https://github.com/baidu-research/warp-ctc)

</div>

Fast parallel CTC.

</div>

<div class="card">

<div class="card-title">

[NVIDIA Develops NVLink Switch: NVSwitch, 18 Ports For DGX-2 &
More](https://www.anandtech.com/show/12581/nvidia-develops-nvlink-switch-nvswitch-18-ports-for-dgx2-more)

</div>

<div class="card-image">

[![](https://images.anandtech.com/doci/12581/nvswitch_678x452.jpg)](https://www.anandtech.com/show/12581/nvidia-develops-nvlink-switch-nvswitch-18-ports-for-dgx2-more)

</div>

</div>

<div class="card">

<div class="card-title">

[NVLink Takes GPU Acceleration To The Next
Level](https://www.nextplatform.com/2016/05/04/nvlink-takes-gpu-acceleration-next-level)

</div>

<div class="card-image">

[![](https://www.nextplatform.com/wp-content/uploads/2016/04/nvidia-nvlink-graphic-bw.jpg)](https://www.nextplatform.com/2016/05/04/nvlink-takes-gpu-acceleration-next-level)

</div>

One of the breakthrough moments in computing, which was compelled by
necessity, was the advent of symmetric multiprocessor, or SMP,
clustering to make two

</div>

<div class="card">

<div class="card-title">

[Stacking Up AMD MI200 Versus Nvidia A100 Compute
Engines](https://www.nextplatform.com/2021/12/06/stacking-up-amd-mi200-versus-nvidia-a100-compute-engines)

</div>

<div class="card-image">

[![](https://www.nextplatform.com/wp-content/uploads/2021/11/amd-mi200-aldebaran-package.jpg)](https://www.nextplatform.com/2021/12/06/stacking-up-amd-mi200-versus-nvidia-a100-compute-engines)

</div>

The modern GPU compute engine is a microcosm of the high performance
computing datacenter at large. At every level of HPC – across systems in
the

</div>

<div class="card">

<div class="card-title">

[Survey paper on Deep Learning on GPUs](https://hgpu.org/?p=19047)

</div>

<div class="card-image">

[![](https://hgpu.org/img/social-logo.png)](https://hgpu.org/?p=19047)

</div>

The rise of deep-learning (DL) has been fuelled by the improvements in
accelerators. GPU continues to remain the most widely used accelerator
for DL applications. We present a survey of architectur…

</div>

<div class="card">

<div class="card-title">

[Fast Multi-GPU collectives with NCCL \| NVIDIA Technical
Blog](https://devblogs.nvidia.com/parallelforall/fast-multi-gpu-collectives-nccl)

</div>

<div class="card-image">

[![](https://developer-blogs.nvidia.com/wp-content/uploads/2016/04/image03.png)](https://devblogs.nvidia.com/parallelforall/fast-multi-gpu-collectives-nccl)

</div>

Today many servers contain 8 or more GPUs. In principle then, scaling an
application from one to many GPUs should provide a tremendous
performance boost. But in practice, this benefit can be difficult…

</div>

<div class="card">

<div class="card-title">

[GPU Computing for Data
Science](https://www.slideshare.net/dominodatalab/gpu-computing-for-data-science)

</div>

<div class="card-image">

[![](https://cdn.slidesharecdn.com/ss_thumbnails/dominogpuwebinarslides1-160202162602-thumbnail.jpg?width=640&height=640&fit=bounds)](https://www.slideshare.net/dominodatalab/gpu-computing-for-data-science)

</div>

GPU Computing for Data Science - Download as a PDF or view online for
free

</div>

<div class="card">

<div class="card-title">

[A Graph-based Model for GPU Caching
Problems](https://arxiv.org/abs/1605.02043)

</div>

<div class="card-image">

[![](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png)](https://arxiv.org/abs/1605.02043)

</div>

Modeling data sharing in GPU programs is a challenging task because of
the massive parallelism and complex data sharing patterns provided by
GPU architectures. Better GPU caching efficiency can be...

</div>

<div class="card">

<div class="card-title">

<https://blog.riseml.com/comparing-google-tpuv2-against-nvidia-v100-on-resnet-50-c2bbb6a51e5e>

</div>

</div>

<div class="card">

<div class="card-title">

[Lemeire2016 microbenchmarks for gpu characteristics for
pdp2016](http://parallel.vub.ac.be/~jan/papers/Lemeire2016_Microbenchmarks_for_GPU_characteristics_forPDP2016.pdf)

</div>

</div>

<div class="card">

<div class="card-title">

[1804](https://arxiv.org/pdf/1804.06826.pdf)

</div>

</div>

<div class="card">

<div class="card-title">

[http://research.baidu.com/bringing-hpc-techniques-deep-learning/](http://research.baidu.com/bringing-hpc-techniques-deep-learning)

</div>

</div>

<div class="card">

<div class="card-title">

[Beyond GPU Memory Limits with Unified Memory on Pascal \| NVIDIA
Technical
Blog](https://devblogs.nvidia.com/parallelforall/beyond-gpu-memory-limits-unified-memory-pascal)

</div>

<div class="card-image">

[![](https://developer-blogs.nvidia.com/wp-content/uploads/2016/12/dme_jet_300px.gif)](https://devblogs.nvidia.com/parallelforall/beyond-gpu-memory-limits-unified-memory-pascal)

</div>

Unified Memory on NVIDIA Pascal GPUs enables applications to run
out-of-the-box with larger memory footprints and achieve great baseline
performance.

</div>

<div class="card">

<div class="card-title">

[A Look at Baidu’s Industrial-Scale GPU Training
Architecture](https://www.nextplatform.com/2021/06/25/a-look-at-baidus-industrial-scale-gpu-training-architecture)

</div>

<div class="card-image">

[![](https://www.nextplatform.com/wp-content/uploads/2018/04/baidu_front.jpg)](https://www.nextplatform.com/2021/06/25/a-look-at-baidus-industrial-scale-gpu-training-architecture)

</div>

Like its U.S. counterpart, Google, Baidu has made significant
investments to build robust, large-scale systems to support global
advertising programs. As

</div>

<div class="card">

<div class="card-title">

[Mythic Resizes its AI
Chip](https://www.eetimes.com/mythic-resizes-its-analog-ai-chip)

</div>

<div class="card-image">

[![](https://www.eetimes.com/wp-content/uploads/Mythic-M.2-AE.png?fit=662%2C893)](https://www.eetimes.com/mythic-resizes-its-analog-ai-chip)

</div>

Its second analog AI chip is optimized for different card sizes, but
still aimed at computer vision workloads at the edge.

</div>

<div class="card">

<div class="card-title">

[What Happens When Multipliers No Longer Define AI
Accelerators?](https://www.nextplatform.com/2021/06/24/what-happens-when-multiplication-no-longer-defines-ai-accelerators)

</div>

<div class="card-image">

[![](https://www.nextplatform.com/wp-content/uploads/2020/02/ab_quantum_general.jpg)](https://www.nextplatform.com/2021/06/24/what-happens-when-multiplication-no-longer-defines-ai-accelerators)

</div>

Current custom AI hardware devices are built around super-efficient,
high performance matrix multiplication. This category of accelerators
includes the

</div>

<div class="card">

<div class="card-title">

[How to Accelerate Signal Processing in
Python](https://developer.nvidia.com/blog/how-to-accelerate-signal-processing-in-python)

</div>

<div class="card-image">

[![](https://developer-blogs.nvidia.com/wp-content/uploads/2021/03/Screen-Shot-2021-03-28-at-10.12.42-PM-2.png)](https://developer.nvidia.com/blog/how-to-accelerate-signal-processing-in-python)

</div>

This post is the seventh installment of the series of articles on the
RAPIDS ecosystem. The series explores and discusses various aspects of
RAPIDS that allow its users solve ETL (Extract, Transform…

</div>

<div class="card">

<div class="card-title">

[CPU-based algorithm trains deep neural nets up to 15 times faster than
top](https://techxplore.com/news/2021-04-rice-intel-optimize-ai-commodity.html)

</div>

<div class="card-image">

[![](https://scx2.b-cdn.net/gfx/news/2021/riceintelopt.jpg)](https://techxplore.com/news/2021-04-rice-intel-optimize-ai-commodity.html)

</div>

Rice University computer scientists have demonstrated artificial
intelligence (AI) software that runs on commodity processors and trains
deep neural networks 15 times faster than platforms based on graphics
...

</div>

<div class="card">

<div class="card-title">

[State of the art NLP at scale with RAPIDS, HuggingFace and
Dask](https://medium.com/rapids-ai/state-of-the-art-nlp-at-scale-with-rapids-huggingface-and-dask-a885c19ce87b)

</div>

<div class="card-image">

[![](https://miro.medium.com/v2/da:true/resize:fit:1200/0*w9WCIHyMuQ9h5Pee)](https://medium.com/rapids-ai/state-of-the-art-nlp-at-scale-with-rapids-huggingface-and-dask-a885c19ce87b)

</div>

See how to build end-to-end NLP pipelines in a fast and scalable way on
GPUs — from feature engineering to inference.

</div>

<div class="card">

<div class="card-title">

[GPU Nomenclature History: No Shortage of GPUs
Here](https://tedium.co/2021/03/26/gpu-technology-history)

</div>

<div class="card-image">

[![](https://images.tedium.co/uploads/tedium032621.gif)](https://tedium.co/2021/03/26/gpu-technology-history)

</div>

What makes a GPU a GPU, and when did we start calling it that? Turns out
that’s a more complicated question than it sounds.

</div>

<div class="card">

<div class="card-title">

[The Rise, Fall and Revival of AMD
(2020)](https://www.techspot.com/article/2043-amd-rise-fall-revival-history)

</div>

<div class="card-image">

[![](https://www.techspot.com/articles-info/2043/images/2020-06-29-image.jpg)](https://www.techspot.com/article/2043-amd-rise-fall-revival-history)

</div>

AMD is one of the oldest designers of large scale microprocessors and
has been the subject of polarizing debate among technology enthusiasts
for nearly 50 years. Its...

</div>

<div class="card">

<div class="card-title">

[Can Graviton Win A Three-Way Compute Race At
AWS?](https://www.nextplatform.com/2021/03/17/can-graviton-win-a-three-way-compute-race-at-aws)

</div>

<div class="card-image">

[![](https://www.nextplatform.com/wp-content/uploads/2019/02/arm-neoverse-n1-64-core-block.jpg)](https://www.nextplatform.com/2021/03/17/can-graviton-win-a-three-way-compute-race-at-aws)

</div>

One of the main tenets of the hyperscalers and cloud builders is that
they buy what they can and they only build what they must. And if they
are building

</div>

<div class="card">

<div class="card-title">

[Welcome to AMD ROCm Platform — ROCm Documentation 1.0.0
documentation](https://rocmdocs.amd.com/en/latest)

</div>

AMD ROCm documentation

</div>

<div class="card">

<div class="card-title">

[Using RAPIDS with
PyTorch](https://developer.nvidia.com/blog/using-rapids-with-pytorch)

</div>

<div class="card-image">

[![](https://developer-blogs.nvidia.com/wp-content/uploads/2021/01/RAPIDSPyTorch_Image1-1.jpg)](https://developer.nvidia.com/blog/using-rapids-with-pytorch)

</div>

In this post we take a look at how to use cuDF, the RAPIDS dataframe
library, to do some of the preprocessing steps required to get the
mortgage data in a format that PyTorch can process so that we…

</div>

<div class="card">

<div class="card-title">

[Beginner’s Guide to Querying Data Using SQL on GPUs in
Python](https://developer.nvidia.com/blog/beginners-guide-to-querying-data-using-sql-on-gpus-in-python)

</div>

<div class="card-image">

[![](https://developer-blogs.nvidia.com/wp-content/uploads/2021/02/An-Introduction-to-Query-data-using-SQL-with-GPUs-in-Python-2.png)](https://developer.nvidia.com/blog/beginners-guide-to-querying-data-using-sql-on-gpus-in-python)

</div>

Historically speaking, processing large amounts of structured data has
been the domain of relational databases. Databases, consisting of tables
that can be joined together or aggregated…

</div>

<div class="card">

<div class="card-title">

[Python Pandas Tutorial – Beginner’s Guide to GPU Accelerated DataFrames
for](https://developer.nvidia.com/blog/python-pandas-tutorial-beginners-guide-to-gpu-accelerated-dataframes-for-pandas-users)

</div>

<div class="card-image">

[![](https://developer-blogs.nvidia.com/wp-content/uploads/2021/02/pexels-erik-mclean-5199661.jpeg)](https://developer.nvidia.com/blog/python-pandas-tutorial-beginners-guide-to-gpu-accelerated-dataframes-for-pandas-users)

</div>

This series on the RAPIDS ecosystem explores the various aspects that
enable you to solve extract, transform, load (ETL) problems, build
machine learning (ML) and deep learning (DL) models…

</div>

<div class="card">

<div class="card-title">

[Speculation Grows As AMD Files Patent for GPU
Design](https://hardware.slashdot.org/story/21/01/03/1820246/speculation-grows-as-amd-files-patent-for-gpu-design)

</div>

<div class="card-image">

[![](https://a.fsdn.com/sd/topics/amd_64.png)](https://hardware.slashdot.org/story/21/01/03/1820246/speculation-grows-as-amd-files-patent-for-gpu-design)

</div>

Long-time Slashdot reader UnknowingFool writes: AMD filed a patent on
using chiplets for a GPU with hints on why it has waited this long to
extend their CPU strategy to GPUs. The latency between chiplets poses
more of a performance problem for GPUs, and AMD is attempting to solve
the problem with a ...

</div>

<div class="card">

<div class="card-title">

[Install the Latest Nvidia Linux Driver -
LinuxConfig.org](https://linuxconfig.org/install-the-latest-nvidia-linux-driver)

</div>

<div class="card-image">

[![](https://linuxconfig.org/wp-content/uploads/2019/10/01-install-the-latest-nvidia-linux-driver.png)](https://linuxconfig.org/install-the-latest-nvidia-linux-driver)

</div>

Most of the modern Linux Desktop systems come with Nvidia driver
pre-installed in a form of the Nouveau open-source graphics device
driver for Nvidia video cards. Hence depending on your needs and in…

</div>

<div class="card">

<div class="card-title">

[How to Install Nvidia Driver on Ubuntu
20.04](https://linoxide.com/linux-how-to/how-to-install-nvidia-driver-on-ubuntu)

</div>

</div>

<div class="card">

<div class="card-title">

[Which GPUs to get for deep
learning](https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning)

</div>

<div class="card-image">

[![](https://timdettmers.com/wp-content/uploads/2023/01/GPUs_Ada_performance_per_dollar6.png)](https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning)

</div>

Here, I provide an in-depth analysis of GPUs for deep learning/machine
learning and explain what is the best GPU for your use-case and budget.

</div>

<div class="card">

<div class="card-title">

[Nvidia Ampere GA102 GPU Architecture
\[pdf\]](https://www.nvidia.com/content/dam/en-zz/Solutions/geforce/ampere/pdf/NVIDIA-ampere-GA102-GPU-Architecture-Whitepaper-V1.pdf)

</div>

</div>

<div class="card">

<div class="card-title">

[How Micron’s GDDR6X memory is the secret to unlocking 4K on Nvidia’s
RTX
30](https://venturebeat.com/2020/09/15/how-microns-gddr6x-memory-is-the-secret-to-unlocking-4k-on-nvidias-rtx-30-series-cards)

</div>

<div class="card-image">

[![](https://venturebeat.com/wp-content/uploads/2020/09/nvidia-GeForce-RTX-30-Series.jpg?w=1024?w=1200&strip=all)](https://venturebeat.com/2020/09/15/how-microns-gddr6x-memory-is-the-secret-to-unlocking-4k-on-nvidias-rtx-30-series-cards)

</div>

Micron's GDDR6X is one of the star components in Nvidia's RTX 3070,
3080, and 3080 video cards. It's so fast it should boost gaming past the
4K barrier.

</div>

<div class="card">

<div class="card-title">

[Diving Deep Into The Nvidia Ampere GPU
Architecture](https://www.nextplatform.com/2020/05/28/diving-deep-into-the-nvidia-ampere-gpu-architecture)

</div>

<div class="card-image">

[![](https://www.nextplatform.com/wp-content/uploads/2020/05/nvidia-ampere-logo.jpg)](https://www.nextplatform.com/2020/05/28/diving-deep-into-the-nvidia-ampere-gpu-architecture)

</div>

When you have 54.2 billion transistors to play with, you can pack a lot
of different functionality into a computing device, and this is
precisely what

</div>

<div class="card">

<div class="card-title">

[NVIDIA Ampere Unleashed: NVIDIA Announces New GPU Architecture, A100
GPU,
a](https://www.anandtech.com/show/15801/nvidia-announces-ampere-architecture-and-a100-products)

</div>

<div class="card-image">

[![](https://images.anandtech.com/doci/15801/GA100_Card_678x452.jpg)](https://www.anandtech.com/show/15801/nvidia-announces-ampere-architecture-and-a100-products)

</div>

</div>

<div class="card">

<div class="card-title">

[CUDA 11 Features Revealed \| NVIDIA Developer
Blog](https://devblogs.nvidia.com/cuda-11-features-revealed)

</div>

<div class="card-image">

[![](https://developer-blogs.nvidia.com/wp-content/uploads/2020/05/cuda-11-2.jpg)](https://devblogs.nvidia.com/cuda-11-features-revealed)

</div>

The new NVIDIA A100 GPU based on the NVIDIA Ampere GPU architecture
delivers the greatest generational leap in accelerated computing. The
A100 GPU has revolutionary hardware capabilities and we’re…

</div>

<div class="card">

<div class="card-title">

[Getting started with the NVIDIA Jetson Nano -
PyImageSearch](https://www.pyimagesearch.com/2019/05/06/getting-started-with-the-nvidia-jetson-nano)

</div>

<div class="card-image">

[![](https://pyimagesearch.com/wp-content/uploads/2019/05/jetson_nano_geting_started_header.jpg)](https://www.pyimagesearch.com/2019/05/06/getting-started-with-the-nvidia-jetson-nano)

</div>

In this tutorial, you will learn how to get started with your NVIDIA
Jetson Nano, including installing Keras + TensorFlow, accessing the
camera, and performing image classification and object detection.

</div>

<div class="card">

<div class="card-title">

[Part 1 - An Overview of AMD's GPU
Architectures](https://www.reddit.com/r/hardware/comments/dr59gg/part_1_an_overview_of_amds_gpu_architectures)

</div>

<div class="card-image">

[![](https://share.redd.it/preview/post/dr59gg)](https://www.reddit.com/r/hardware/comments/dr59gg/part_1_an_overview_of_amds_gpu_architectures)

</div>

363 votes, 25 comments. This post has been split into a two-part series
to work around Reddit’s per-post character limit. Please find Part 2 in
the…

</div>

<div class="card">

<div class="card-title">

[H2O.ai Releases H2O4GPU, the Fastest Collection of GPU Algorithms on
the
Ma](http://blog.h2o.ai/2017/09/h2o-ai-releases-h2o4gpu-the-fastest-collection-of-gpu-algorithms-on-the-market-to-expedite-machine-learning-in-python)

</div>

</div>

<div class="card">

<div class="card-title">

[(9) How many servers does a typical data center house? -
Quora](https://www.quora.com/How-many-servers-does-a-typical-data-center-house)

</div>

<div class="card-image">

[![](https://qph.cf2.quoracdn.net/main-custom-t-8120-600x315-llughxixxjgmcasjgypruztpneurypeo.jpeg)](https://www.quora.com/How-many-servers-does-a-typical-data-center-house)

</div>

Answer (1 of 7): Lots. Definitions of the term "data centre" tend to
vary. Some would label a small machine room with 2 or 3 racks a data
centre, but that is not really a large facility by any stretch of the
imagination. Most such installations are never going to hit the usual
problems which dat...

</div>

<div class="card">

<div class="card-title">

[Semiconductor Engineering .:. Making Waves In Deep
Learning](http://semiengineering.com/making-waves-in-deep-learning)

</div>

<div class="card-image">

[![](https://semiengineering.com/wp-content/uploads/2016/10/Wave_Chip.png)](http://semiengineering.com/making-waves-in-deep-learning)

</div>

Making Waves in Deep Learning How deep learning applications will map
onto a chip.

</div>

<div class="card">

<div class="card-title">

[Memory is the Next
Platform](http://www.nextplatform.com/2016/10/10/memory-next-platform)

</div>

<div class="card-image">

[![](https://www.nextplatform.com/wp-content/uploads/2015/11/legos_oh.jpg)](http://www.nextplatform.com/2016/10/10/memory-next-platform)

</div>

A new crop of applications is driving the market along some unexpected
routes, in some cases bypassing the processor as the landmark for
performance and

</div>

</div>
