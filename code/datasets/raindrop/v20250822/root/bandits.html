<html><head><meta charset="utf-8"><title>bandits</title>
<style>.cards { display:block; }
.card {
  border: 1px solid #e2e2e2;
  border-radius: 12px;
  padding: 12px 14px;
  margin: 10px 0;
  box-shadow: 0 1px 2px rgba(0,0,0,0.04);
}
.card-title {
  margin: 0 0 6px 0;
  font-weight: 600;
  font-size: 1.05rem;
  line-height: 1.3;
}
.card-title a { text-decoration: none; }
.card-image { margin: 6px 0 8px 0; }
.card-image img { display:block; max-width:100%; height:auto; border-radius: 8px; }
.card-excerpt {
  margin: 0;
  font-size: .9rem;
  color: #444;
}
.nav {
  margin: 0 0 12px 0;
  font-size: .9rem;
}
.nav a { text-decoration: none; }
</style></head><body>
<div class="nav">⟵ <a href="index.html">Up</a> &nbsp;|&nbsp; <a href="index.html">Index</a></div>
<h1>bandits</h1>
<div class="cards">
  <div class="card">
    <div class="card-title"><a href="https://towardsdatascience.com/an-overview-of-contextual-bandits-53ac3aa45034?source=rss----7f60cf5620c9---4">An Overview of Contextual Bandits</a></div>
    <div class="card-image"><a href="https://towardsdatascience.com/an-overview-of-contextual-bandits-53ac3aa45034?source=rss----7f60cf5620c9---4"><img src="https://miro.medium.com/v2/resize:fit:1200/1*YOsWxbHyU-J7C0s9KOyUDw.png" alt=""></a></div>
    <p class="card-excerpt">A dynamic approach to treatment personalization</p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://towardsdatascience.com/dynamic-pricing-with-multi-armed-bandit-learning-by-doing-3e4550ed02ac">Dynamic Pricing with Multi-Armed Bandit: Learning by Doing!</a></div>
    <div class="card-image"><a href="https://towardsdatascience.com/dynamic-pricing-with-multi-armed-bandit-learning-by-doing-3e4550ed02ac"><img src="https://miro.medium.com/v2/da:true/resize:fit:1200/0*PtB_85QrbCPNJXXb" alt=""></a></div>
    <p class="card-excerpt">Applying Reinforcement Learning strategies to real-world use cases, especially in dynamic pricing, can reveal many surprises</p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://www.kdnuggets.com/2023/01/introduction-multiarmed-bandit-problems.html">Introduction to Multi-Armed Bandit Problems</a></div>
    <div class="card-image"><a href="https://www.kdnuggets.com/2023/01/introduction-multiarmed-bandit-problems.html"><img src="https://www.kdnuggets.com/wp-content/uploads/popovic_introduction_multiarmed_bandit_problems_1.png" alt=""></a></div>
    <p class="card-excerpt">Delve deeper into the concept of multi-armed bandits, reinforcement learning, and exploration vs. exploitation dilemma.</p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://blog.thedataincubator.com/2016/07/multi-armed-bandits-2">The Data Incubator is Now Pragmatic Data | Pragmatic Institute</a></div>
    <div class="card-image"><a href="https://blog.thedataincubator.com/2016/07/multi-armed-bandits-2"><img src="https://www.pragmaticinstitute.com/resources/wp-content/uploads/sites/6/2024/01/tditopragmatic.jpg" alt=""></a></div>
    <p class="card-excerpt">As of 2024, The Data Incubator is now Pragmatic Data! Explore Pragmatic Institute’s new offerings, learn about team training opportunities, and more.</p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://blog.insightdatascience.com/multi-armed-bandits-for-dynamic-movie-recommendations-5eb8f325ed1d">Multi-armed bandits for dynamic movie recommendations</a></div>
    <div class="card-image"><a href="https://blog.insightdatascience.com/multi-armed-bandits-for-dynamic-movie-recommendations-5eb8f325ed1d"><img src="https://miro.medium.com/v2/resize:fit:1200/0*k3FsfPU592DBgMfx." alt=""></a></div>
    <p class="card-excerpt">Making the best recommendations to anonymous audiences</p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://towardsdatascience.com/multi-armed-bandit-algorithms-thompson-sampling-6d91a88145db?source=rss----7f60cf5620c9---4">Multi-Armed Bandit Algorithms: Thompson Sampling</a></div>
    <div class="card-image"><a href="https://towardsdatascience.com/multi-armed-bandit-algorithms-thompson-sampling-6d91a88145db?source=rss----7f60cf5620c9---4"><img src="https://miro.medium.com/v2/da:true/resize:fit:1200/0*9YMbE-00b8VRceig" alt=""></a></div>
    <p class="card-excerpt">Intuition, Bayes, and an example</p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://github.com/stitchfix/mab">stitchfix/mab: Library for multi-armed bandit selection strategies, including efficient deterministic implementations of Thompson sampling and epsilon-greedy.</a></div>
    <div class="card-image"><a href="https://github.com/stitchfix/mab"><img src="https://repository-images.githubusercontent.com/340162521/d1711100-774c-11eb-86cb-e4a5c793ebc8" alt=""></a></div>
    <p class="card-excerpt">Library for multi-armed bandit selection strategies, including efficient deterministic implementations of Thompson sampling and epsilon-greedy. - stitchfix/mab</p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://towardsdatascience.com/thompson-sampling-using-conjugate-priors-e0a18348ea2d">Thompson Sampling using Conjugate Priors</a></div>
    <div class="card-image"><a href="https://towardsdatascience.com/thompson-sampling-using-conjugate-priors-e0a18348ea2d"><img src="https://miro.medium.com/v2/da:true/resize:fit:712/1*_Gqr8su3G4inxRnEuTbC4Q.gif" alt=""></a></div>
    <p class="card-excerpt">Multi-Armed Bandits: Part 5b</p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://towardsdatascience.com/a-comparison-of-bandit-algorithms-24b4adfcabb?source=rss----7f60cf5620c9---4">A Comparison of Bandit Algorithms</a></div>
    <div class="card-image"><a href="https://towardsdatascience.com/a-comparison-of-bandit-algorithms-24b4adfcabb?source=rss----7f60cf5620c9---4"><img src="https://miro.medium.com/v2/da:true/resize:fit:1200/0*e7T9wQtgE2cro-mC" alt=""></a></div>
    <p class="card-excerpt">Multi-Armed Bandits: Part 6</p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://multithreaded.stitchfix.com/blog/2020/08/05/bandits">Multi-Armed Bandits and the Stitch Fix Experimentation Platform</a></div>
    <div class="card-image"><a href="https://multithreaded.stitchfix.com/blog/2020/08/05/bandits"><img src="https://multithreaded.stitchfix.com/assets/posts/2020-08-05-bandits/multi_armed_bandit.png" alt=""></a></div>
    <p class="card-excerpt">We've recently built support for multi-armed bandits into the Stitch Fix experimentation platform. This post will explain how and why.</p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://towardsdatascience.com/bandit-algorithms-34fd7890cb18?source=rss----7f60cf5620c9---4">Bandit Algorithms</a></div>
    <div class="card-image"><a href="https://towardsdatascience.com/bandit-algorithms-34fd7890cb18?source=rss----7f60cf5620c9---4"><img src="https://miro.medium.com/v2/da:true/resize:fit:1200/0*Qh6b6kmOXe6Z87sG" alt=""></a></div>
    <p class="card-excerpt">Multi-Armed Bandits: Part 3</p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://medium.com/expedia-group-tech/how-we-optimized-hero-images-on-hotels-com-using-multi-armed-bandit-algorithms-4503c2c32eae">How We Optimized Hero Images on Hotels.com using Multi-Armed Bandit Algorithms</a></div>
    <div class="card-image"><a href="https://medium.com/expedia-group-tech/how-we-optimized-hero-images-on-hotels-com-using-multi-armed-bandit-algorithms-4503c2c32eae"><img src="https://miro.medium.com/v2/resize:fit:770/1*4u5qpI-eOTFBbUnH6dVH9w.png" alt=""></a></div>
    <p class="card-excerpt">Introducing the multi-armed bandit (MAB) optimization used for hero images on hotels.com</p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://www.microsoft.com/en-us/research/blog/exploring-the-fundamentals-of-multi-armed-bandits">Exploring the fundamentals of multi-armed bandits</a></div>
    <div class="card-image"><a href="https://www.microsoft.com/en-us/research/blog/exploring-the-fundamentals-of-multi-armed-bandits"><img src="https://www.microsoft.com/en-us/research/uploads/prod/2020/02/MSFT_Research_MultiarmedBandit_final_1400X788_final.png" alt=""></a></div>
    <p class="card-excerpt">Multi-armed bandits are a simple but very powerful framework for algorithms that make decisions over time under uncertainty. “Introduction to Multi-Armed Bandits” by Alex Slivkins provides an accessible, textbook-like treatment of the subject.</p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://mlwhiz.com/blog/2019/07/21/bandits">Bayesian Bandits explained simply - MLWhiz</a></div>
    <div class="card-image"><a href="https://mlwhiz.com/blog/2019/07/21/bandits"><img src="https://mlwhiz.com/images/bandits/1.png" alt=""></a></div>
    <p class="card-excerpt">There are multiple ways to doing the same thing in Pandas, and that might make it troublesome for the beginner user.This post is about handling most of the data manipulation cases in Python using a straightforward, simple, and matter of fact way.</p>
  </div>
  <div class="card">
    <div class="card-title"><a href="http://banditalgs.com">Bandit Algorithms</a></div>
    <div class="card-image"><a href="http://banditalgs.com"><img src="https://banditalgs.com/wp-content/uploads/2019/03/cropped-bandit-1.png" alt=""></a></div>
    <p class="card-excerpt"></p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://support.google.com/analytics/answer/2844870">https://support.google.com/analytics/answer/2844870</a></div>
    <p class="card-excerpt"></p>
  </div>
</div>
</body></html>
