<div class="nav">

‚üµ [Up](index.html) ¬†\|¬† [Index](index.html)

</div>

# linear-algebra

<div class="cards">

<div class="card">

<div class="card-title">

[Mastering Linear Algebra for Free: A Deep Dive into Jim Hefferon's
Fourth Edition (Free
PDF)](https://www.clcoding.com/2025/07/mastering-linear-algebra-for-free-deep.html)

</div>

<div class="card-image">

[![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhjdaBZ6cko46uoqOUbPy5cYR3JX256g-yPgpS6hyCsdFg4dpwsb8KSPgCG5HagYPvH3mh331zdVXfLhNOXifYNowgsL6o_1-7cs7So2U5sQjtZwo-tY6MUYIWgb_Dp1z-_4ox0BnQRdEbx2B2rfbYVIeU3FBF6Xhgj-B4_RqBI2KKl7hwRJ3Dd6oTpwRg/w1200-h630-p-k-no-nu/Mastering%20Linear%20Algebra%20for%20Free.jpg)](https://www.clcoding.com/2025/07/mastering-linear-algebra-for-free-deep.html)

</div>

üî¢ Mastering Linear Algebra for Free: A Deep Dive into Jim Hefferon's
Fourth Edition (Free PDF)

</div>

<div class="card">

<div class="card-title">

[Derivatives, Gradients, Jacobians and Hessians ‚Äì Oh
My!](https://blog.demofox.org/2025/08/16/derivatives-gradients-jacobians-and-hessians-oh-my/)

</div>

<div class="card-image">

[![](https://blog.demofox.org/wp-content/uploads/2025/08/image-4.png)](https://blog.demofox.org/2025/08/16/derivatives-gradients-jacobians-and-hessians-oh-my/)

</div>

This article explains how these four things fit together and shows some
examples of what they are used for. Derivatives Derivatives are the most
fundamental concept in calculus. If you have a funct‚Ä¶

</div>

<div class="card">

<div class="card-title">

[everything you always wanted to know about math
pdf](https://www.math.cmu.edu/~jmackey/151_128/bws_book.pdf)

</div>

</div>

<div class="card">

<div class="card-title">

[How to Interpret Matrix Expressions ‚Äî
Transformations](https://towardsdatascience.com/how-to-interpret-matrix-expressions-transformations-a5e6871cd224?source=rss----7f60cf5620c9---4)

</div>

<div class="card-image">

[![](https://miro.medium.com/v2/da:true/resize:fit:1200/0*57Fx9wZ-ds46JqFq)](https://towardsdatascience.com/how-to-interpret-matrix-expressions-transformations-a5e6871cd224?source=rss----7f60cf5620c9---4)

</div>

Matrix algebra for a data scientist

</div>

<div class="card">

<div class="card-title">

[Immersive Math](http://immersivemath.com/ila/index.html)

</div>

</div>

<div class="card">

<div class="card-title">

[Matrix Product Interpretations and
Visualizations](https://www.linearalgebraforprogrammers.com/blog/matmul_animations)

</div>

Matrix Product Interpretations and Visualizations: Learn Linear Algebra
from scratch. Build a foundation for Machine Learning and other key
technologies.

</div>

<div class="card">

<div class="card-title">

[Matrix Calculus](http://www.matrixcalculus.org)

</div>

MatrixCalculus provides matrix calculus for everyone. It is an online
tool that computes vector and matrix derivatives (matrix calculus).

</div>

<div class="card">

<div class="card-title">

[Linear Algebra: LU Decomposition, with
Python](https://towardsdatascience.com/linear-algebra-lu-decomposition-with-python-5a7b3fd87f96)

</div>

<div class="card-image">

[![](https://miro.medium.com/v2/da:true/resize:fit:1200/0*ocZHR-iX_wRMehcu)](https://towardsdatascience.com/linear-algebra-lu-decomposition-with-python-5a7b3fd87f96)

</div>

Part 4: A comprehensive step-by-step guide to solving a linear system
with LU Decomposition

</div>

<div class="card">

<div class="card-title">

[What Is an Eigenvalue? ‚Äì Nick
Higham](https://nhigham.com/2022/11/08/what-is-an-eigenvalue)

</div>

<div class="card-image">

[![](https://nhigham.com/wp-content/uploads/2022/11/eig_smoke-2.jpg)](https://nhigham.com/2022/11/08/what-is-an-eigenvalue)

</div>

An eigenvalue of a square matrix \$LATEX A\$ is a scalar \$latex
\lambda\$ such that \$latex Ax = \lambda x\$ for some nonzero vector
\$latex x\$. The vector \$latex x\$ is an eigenvector of \$LATEX A\$ and
it‚Ä¶

</div>

<div class="card">

<div class="card-title">

[Supply Chain Process Optimization using Linear
Programming](https://towardsdatascience.com/supply-chain-process-optimization-using-linear-programming-b1511800630f)

</div>

<div class="card-image">

[![](https://miro.medium.com/v2/resize:fit:640/1*EPAkQ26X1sw75TUkp2SH1g.png)](https://towardsdatascience.com/supply-chain-process-optimization-using-linear-programming-b1511800630f)

</div>

Understand how linear programming can be the most powerful tool for a
supply chain continuous improvement engineer

</div>

<div class="card">

<div class="card-title">

[Linear Algebra for Data Science -
KDnuggets](https://www.kdnuggets.com/2022/07/linear-algebra-data-science.html)

</div>

<div class="card-image">

[![](https://www.kdnuggets.com/wp-content/uploads/tayo_linear_algebra_data_science_1.jpg)](https://www.kdnuggets.com/2022/07/linear-algebra-data-science.html)

</div>

In this article, we discuss the importance of linear algebra in data
science and machine learning.

</div>

<div class="card">

<div class="card-title">

[Probabilistic Numerics \|
Textbooks](https://substack.com/redirect/bb5ad843-c722-433f-96d1-ff729a3bef7c?u=1135489)

</div>

Quantifying Uncertainty in Computation.

</div>

<div class="card">

<div class="card-title">

[Introduction to Applied Linear Algebra: Norms &
Distances](https://towardsdatascience.com/introduction-to-applied-linear-algebra-norms-distances-2451e6325925)

</div>

<div class="card-image">

[![](https://miro.medium.com/v2/resize:fit:1200/1*1UdagNhgmKANCqzD8CTp8g.jpeg)](https://towardsdatascience.com/introduction-to-applied-linear-algebra-norms-distances-2451e6325925)

</div>

This article gives an introduction to vector norms, vector distances and
their application in the field of data science

</div>

<div class="card">

<div class="card-title">

[Large scale eigenvalue decomposition and SVD with rARPACK \|
R-bloggers](https://www.r-bloggers.com/large-scale-eigenvalue-decomposition-and-svd-with-rarpack)

</div>

<div class="card-image">

[![](https://i1.wp.com/www.r-bloggers.com/wp-content/uploads/2016/02/VfmfWJi.jpg?fit=600%2C373&ssl=1)](https://www.r-bloggers.com/large-scale-eigenvalue-decomposition-and-svd-with-rarpack)

</div>

In January 2016, I was honored to receive an ‚ÄúHonorable Mention‚Äù of the
John Chambers Award 2016. This article was written for R-bloggers, whose
builder, Tal Galili, kindly invited me to write an introduction to the
rARPACK package. A Short Story of rARPACK Eigenvalue decomposition is a
commonly used technique in numerous statistical problems. For example,
principal component analysis (PCA) basically conducts eigenvalue
decomposition on the sample covariance of a data matrix: the eigenvalues
are the component variances, and eigenvectors are the variable loadings.
In R, the standard way to compute eigenvalues is the eigen() function.
However, when the matrix becomes large, eigen() can be very
time-consuming: the complexity to calculate all eigenvalues of a \$n
times n\$ matrix is \$O(n^3)\$. While in real applications, we usually
only need to compute a few eigenvalues or eigenvectors, for example to
visualize high dimensional data using PCA, we may only use the first two
or three components to draw a scatterplot. Unfortunately in eigen(),
there is no option to limit the number of eigenvalues to be computed.
This means that we always need to do the full eigen decomposition, which
can cause a huge waste in computation. And this is why the rARPACK
package was developed. As the name indicates, rARPACK was originally an
R wrapper of the ARPACK library, a FORTRAN package that is used to
calculate a few eigenvalues of a square matrix. However ARPACK has
stopped development for a long time, and it has some compatibility
issues with the current version of LAPACK. Therefore to maintain rARPACK
in a good state, I wrote a new backend for rARPACK, and that is the C++
library Spectra. The name of rARPACK was POORLY designed, I admit.
Starting from version 0.8-0, rARPACK no longer relies on ARPACK, but due
to CRAN polices and reverse dependence, I have to keep using the old
name. Features and Usage The usage of rARPACK is simple. If you want to
calculate some eigenvalues of a square matrix A, just call the function
eigs() and tells it how many eigenvalues you want (argument k), and
which eigenvalues to calculate (argument which). By default, which =
"LM" means to pick the eigenvalues with the largest magnitude (modulus
for complex numbers and absolute value for real numbers). If the matrix
is known to be symmetric, calling eigs_sym() is preferred since it
guarantees that the eigenvalues are real. library(rARPACK) set.seed(123)
\## Some random data x = matrix(rnorm(1000 \* 100), 1000) \## If retvec
== FALSE, we don't calculate eigenvectors eigs_sym(cov(x), k = 5, which
= "LM", opts = list(retvec = FALSE)) For really large data, the matrix
is usually in sparse form. rARPACK supports several sparse matrix types
defined in the Matrix package, and you can even pass an implicit matrix
defined by a function to eigs(). See ?rARPACK::eigs for details.
library(Matrix) spmat = as(cov(x), "dgCMatrix") eigs_sym(spmat, 2) \##
Implicitly define the matrix by a function that calculates A %\*% x \##
Below represents a diagonal matrix diag(c(1:10)) fmat = function(x,
args) { return(x \* (1:10)) } eigs_sym(fmat, 3, n = 10, args = NULL)
From Eigenvalue to SVD An extension to eigenvalue decomposition is the
singular value decomposition (SVD), which works for general rectangular
matrices. Still take PCA as an example. To calculate variable loadings,
we can perform an SVD on the centered data matrix, and the loadings will
be contained in the right singular vectors. This method avoids computing
the covariance matrix, and is generally more stable and accurate than
using cov() and eigen(). Similar to eigs(), rARPACK provides the
function svds() to conduct partial SVD, meaning that only part of the
singular pairs (values and vectors) are to be computed. Below shows an
example that computes the first three PCs of a 2000x500 matrix, and I
compare the timings of three different algorithms:
library(microbenchmark) set.seed(123) \## Some random data x =
matrix(rnorm(2000 \* 500), 2000) pc = function(x, k) { \## First center
data xc = scale(x, center = TRUE, scale = FALSE) \## Partial SVD decomp
= svds(xc, k, nu = 0, nv = k) return(list(loadings = decomp\$v, scores =
xc %\*% decomp\$v)) } microbenchmark(princomp(x), prcomp(x), pc(x, 3),
times = 5) The princomp() and prcomp() functions are the standard
approaches in R to do PCA, which will call eigen() and svd()
respectively. On my machine (Fedora Linux 23, R 3.2.3 with optimized
single-threaded OpenBLAS), the timing results are as follows: Unit:
milliseconds expr min lq mean median uq max neval princomp(x) 274.7621
276.1187 304.3067 288.7990 289.5324 392.3211 5 prcomp(x) 306.4675
391.9723 408.9141 396.8029 397.3183 552.0093 5 pc(x, 3) 162.2127
163.0465 188.3369 163.3839 186.1554 266.8859 5 Applications SVD has some
interesting applications, and one of them is image compression. The
basic idea is to perform a partial SVD on the image matrix, and then
recover it using the calculated singular values and singular vectors.
Below shows an image of size 622x1000: (Orignal image) If we use the
first five singular pairs to recover the image, then we need to store
8115 elements, which is only 1.3% of the original data size. The
recovered image will look like below: (5 singular pairs) Even if the
recovered image is quite blurred, it already reveals the main structure
of the original image. And if we increase the number of singular pairs
to 50, then the difference is almost imperceptible, as is shown below.
(50 singular pairs) There is also a nice Shiny App developed by Nan
Xiao, Yihui Xie and Tong He that allows users to upload an image and
visualize the effect of compression using this algorithm. The code is
available on GitHub. Performance Finally, I would like to use some
benchmark results to show the performance of rARPACK. As far as I know,
there are very few packages available in R that can do the partial
eigenvalue decomposition, so the results here are based on partial SVD.
The first plot compares different SVD functions on a 1000x500 matrix,
with dense format on the left panel, and sparse format on the right. The
second plot shows the results on a 5000x2500 matrix. The functions used
corresponding to the axis labels are as follows: svd: svd() from base R,
which computes the full SVD irlba: irlba() from irlba package, partial
SVD propack, trlan: propack.svd() and trlan.svd() from svd package,
partial SVD svds: svds() from rARPACK The code for benchmark and the
environment to run the code can be found here.

</div>

<div class="card">

<div class="card-title">

[Efficient matrix
multiplication](https://gist.github.com/nadavrot/5b35d44e8ba3dd718e595e40184d03f0)

</div>

<div class="card-image">

[![](https://raw.githubusercontent.com/gist/nadavrot/5b35d44e8ba3dd718e595e40184d03f0/raw/75f183012236045a53ebc148aaea0265963c0273/zcounters.png)](https://gist.github.com/nadavrot/5b35d44e8ba3dd718e595e40184d03f0)

</div>

Efficient matrix multiplication ¬∑ GitHub

</div>

<div class="card">

<div class="card-title">

[\[2106.10860v1\] Multiplying Matrices Without
Multiplying](https://arxiv.org/abs/2106.10860v1)

</div>

<div class="card-image">

[![](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png)](https://arxiv.org/abs/2106.10860v1)

</div>

Multiplying matrices is among the most fundamental and compute-intensive
operations in machine learning. Consequently, there has been significant
work on efficiently approximating matrix...

</div>

<div class="card">

<div class="card-title">

[Poisson's Equation is the Most Powerful Tool not yet in your
Toolbox](https://mattferraro.dev/posts/poissons-equation)

</div>

<div class="card-image">

[![](https://www.mattferraro.dev/images/poissons-equation/separatrix_heightmap.png)](https://mattferraro.dev/posts/poissons-equation)

</div>

Poisson's Equation is an incredibly powerful tool...

</div>

<div class="card">

<div class="card-title">

[The torch.linalg module: Accelerated Linear Algebra with Autograd in
PyTorch](https://pytorch.org/blog/torch-linalg-autograd)

</div>

<div class="card-image">

[![](https://pytorch.org/assets/images/cholesky-decomposition.png)](https://pytorch.org/blog/torch-linalg-autograd)

</div>

Linear algebra is essential to deep learning and scientific computing,
and it‚Äôs always been a core part of PyTorch. PyTorch 1.9 extends
PyTorch‚Äôs support for linear algebra operations with the torch.linalg
module. This module, documented here, has 26 operators, including faster
and easier to use versions of older PyTorch operators, every function
from NumPy‚Äôs linear algebra module extended with accelerator and
autograd support, and a few operators that are completely new. This
makes the torch.linalg immediately familiar to NumPy users and an
exciting update to PyTorch‚Äôs linear algebra support.

</div>

<div class="card">

<div class="card-title">

[What Happens When Multipliers No Longer Define AI
Accelerators?](https://www.nextplatform.com/2021/06/24/what-happens-when-multiplication-no-longer-defines-ai-accelerators)

</div>

<div class="card-image">

[![](https://www.nextplatform.com/wp-content/uploads/2020/02/ab_quantum_general.jpg)](https://www.nextplatform.com/2021/06/24/what-happens-when-multiplication-no-longer-defines-ai-accelerators)

</div>

Current custom AI hardware devices are built around super-efficient,
high performance matrix multiplication. This category of accelerators
includes the

</div>

<div class="card">

<div class="card-title">

[numerical-linear-algebra/README.md at master ¬∑
fastai/numerical-linear-alge](https://github.com/fastai/numerical-linear-algebra/blob/master/README.md)

</div>

<div class="card-image">

[![](https://opengraph.githubassets.com/8e1a4aba84be091781119bb9c36d26023c5cc4fa5873e6b42872e20f2fd9a3a7/fastai/numerical-linear-algebra)](https://github.com/fastai/numerical-linear-algebra/blob/master/README.md)

</div>

Free online textbook of Jupyter notebooks for fast.ai Computational
Linear Algebra course - fastai/numerical-linear-algebra

</div>

<div class="card">

<div class="card-title">

[Essential Linear Algebra for Data Science and Machine Learning -
KDnuggets](https://www.kdnuggets.com/2021/05/essential-linear-algebra-data-science-machine-learning.html)

</div>

Linear algebra is foundational in data science and machine learning.
Beginners starting out along their learning journey in data science--as
well as established practitioners--must develop a strong familiarity
with the essential concepts in linear algebra.

</div>

<div class="card">

<div class="card-title">

[What Really IS a Matrix
Determinant?](https://towardsdatascience.com/what-really-is-a-matrix-determinant-89c09884164c?source=rss----7f60cf5620c9---4)

</div>

<div class="card-image">

[![](https://miro.medium.com/v2/resize:fit:1200/1*rbJgdAmyywsaHiwIzC9gvw.png)](https://towardsdatascience.com/what-really-is-a-matrix-determinant-89c09884164c?source=rss----7f60cf5620c9---4)

</div>

The geometric intuition behind determinants could change how you think
about them.

</div>

<div class="card">

<div class="card-title">

[Home Page - Intuitive Math](https://intuitive-math.club)

</div>

Intuitive Math Descriptions

</div>

<div class="card">

<div class="card-title">

[The Matrix Calculus You Need For Deep
Learning](http://parrt.cs.usfca.edu/doc/matrix-calculus/index.html)

</div>

<div class="card-image">

[![](https://explained.ai/matrix-calculus/images/neuron.png)](http://parrt.cs.usfca.edu/doc/matrix-calculus/index.html)

</div>

Most of us last saw calculus in school, but derivatives are a critical
part of machine learning, particularly deep neural networks, which are
trained by optimizing a loss function. This article is an attempt to
explain all the matrix calculus you need in order to understand the
training of deep neural networks. We assume no math knowledge beyond
what you learned in calculus 1, and provide links to help you refresh
the necessary math where needed.

</div>

<div class="card">

<div class="card-title">

[Eigenvectors and Eigenvalues explained
visually](http://setosa.io/ev/eigenvectors-and-eigenvalues)

</div>

<div class="card-image">

[![](https://setosa.io/ev/eigenvectors-and-eigenvalues/fb-thumb.png)](http://setosa.io/ev/eigenvectors-and-eigenvalues)

</div>

</div>

<div class="card">

<div class="card-title">

[Linear Algebra Cheat Sheet for Machine
Learning](https://machinelearningmastery.com/linear-algebra-cheat-sheet-for-machine-learning)

</div>

<div class="card-image">

[![](https://machinelearningmastery.com/wp-content/uploads/2018/02/Linear-Algebra-Cheat-Sheet-for-Machine-Learning.jpg)](https://machinelearningmastery.com/linear-algebra-cheat-sheet-for-machine-learning)

</div>

All of the Linear Algebra Operations that You Need to Use in NumPy for
Machine Learning. The Python numerical computation library called NumPy
provides many linear algebra functions that may be useful as a machine
learning practitioner. In this tutorial, you will discover the key
functions for working with vectors and matrices that you may find useful
as a machine‚Ä¶

</div>

</div>
