<html><head><meta charset="utf-8"><title>tokens</title>
<style>.cards { display:block; }
.card {
  border: 1px solid #e2e2e2;
  border-radius: 12px;
  padding: 12px 14px;
  margin: 10px 0;
  box-shadow: 0 1px 2px rgba(0,0,0,0.04);
}
.card-title {
  margin: 0 0 6px 0;
  font-weight: 600;
  font-size: 1.05rem;
  line-height: 1.3;
}
.card-title a { text-decoration: none; }
.card-image { margin: 6px 0 8px 0; }
.card-image img { display:block; max-width:100%; height:auto; border-radius: 8px; }
.card-excerpt {
  margin: 0;
  font-size: .9rem;
  color: #444;
}
.nav {
  margin: 0 0 12px 0;
  font-size: .9rem;
}
.nav a { text-decoration: none; }
</style></head><body>
<div class="nav">⟵ <a href="index.html">Up</a> &nbsp;|&nbsp; <a href="index.html">Index</a></div>
<h1>tokens</h1>
<div class="cards">
  <div class="card">
    <div class="card-title"><a href="https://www.marktechpost.com/2025/02/16/a-step-by-step-guide-to-setting-up-a-custom-bpe-tokenizer-with-tiktoken-for-advanced-nlp-applications-in-python/">A Step-by-Step Guide to Setting Up a Custom BPE Tokenizer with Tiktoken for Advanced NLP Applications in Python</a></div>
    <div class="card-image"><a href="https://www.marktechpost.com/2025/02/16/a-step-by-step-guide-to-setting-up-a-custom-bpe-tokenizer-with-tiktoken-for-advanced-nlp-applications-in-python/"><img src="https://www.marktechpost.com/wp-content/uploads/2025/02/Screenshot-2025-02-16-at-10.17.33 PM.png" alt=""></a></div>
    <p class="card-excerpt">A Step-by-Step Guide to Setting Up a Custom BPE Tokenizer with Tiktoken for Advanced NLP Applications in Python</p>
  </div>
  <div class="card">
    <div class="card-title"><a href="https://www.marktechpost.com/2024/03/09/unlocking-the-best-tokenization-strategies-how-greedy-inference-and-sage-lead-the-way-in-nlp-models">Unlocking the Best Tokenization Strategies: How Greedy Inference and SaGe L</a></div>
    <div class="card-image"><a href="https://www.marktechpost.com/2024/03/09/unlocking-the-best-tokenization-strategies-how-greedy-inference-and-sage-lead-the-way-in-nlp-models"><img src="https://www.marktechpost.com/wp-content/uploads/2024/03/Screenshot-2024-03-09-at-10.30.54-PM.png" alt=""></a></div>
    <p class="card-excerpt">The inference method is crucial for NLP models in subword tokenization. Methods like BPE, WordPiece, and UnigramLM offer distinct mappings, but their performance differences must be better understood. Implementations like Huggingface Tokenizers often need to be clearer or limit inference choices, complicating compatibility with vocabulary learning algorithms. Whether a matching inference method is necessary or optimal for tokenizer vocabularies is uncertain. Previous research focused on developing vocabulary construction algorithms such as BPE, WordPiece, and UnigramLM, exploring optimal vocabulary size and multilingual vocabularies. Some studies examined the effects of vocabularies on downstream performance, information theory, and cognitive plausibility. Limited work on</p>
  </div>
</div>
</body></html>
