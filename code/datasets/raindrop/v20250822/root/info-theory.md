<div class="nav">

⟵ [Up](index.html)  \|  [Index](index.html)

</div>

# info-theory

<div class="cards">

<div class="card">

<div class="card-title">

[Cross-entropy and KL divergence - Eli Bendersky's
website](https://eli.thegreenplace.net/2025/cross-entropy-and-kl-divergence/)

</div>

<div class="card-image">

[![](https://eli.thegreenplace.net/images/2025/distrib-1-0s.png)](https://eli.thegreenplace.net/2025/cross-entropy-and-kl-divergence/)

</div>

</div>

<div class="card">

<div class="card-title">

[Understanding KL Divergence Entropy and Related
Concepts](https://towardsdatascience.com/understanding-kl-divergence-entropy-and-related-concepts-75e766a2fd9e?source=rss----7f60cf5620c9---4)

</div>

<div class="card-image">

[![](https://miro.medium.com/v2/resize:fit:1200/1*MDhu1KyK_OCx71Uuh8I59A.png)](https://towardsdatascience.com/understanding-kl-divergence-entropy-and-related-concepts-75e766a2fd9e?source=rss----7f60cf5620c9---4)

</div>

Important concepts in information theory, machine learning, and
statistics

</div>

<div class="card">

<div class="card-title">

[How to Understand and Use the Jensen-Shannon
Divergence](https://towardsdatascience.com/how-to-understand-and-use-jensen-shannon-divergence-b10e11b03fd6?source=rss----7f60cf5620c9---4)

</div>

<div class="card-image">

[![](https://miro.medium.com/v2/resize:fit:1020/1*iXqnpz9Rlxl2X83fAYxyLg.jpeg)](https://towardsdatascience.com/how-to-understand-and-use-jensen-shannon-divergence-b10e11b03fd6?source=rss----7f60cf5620c9---4)

</div>

A primer on the math, logic, and pragmatic application of JS
Divergence — including how it is best used in drift monitoring

</div>

<div class="card">

<div class="card-title">

[1802](https://arxiv.org/pdf/1802.05968)

</div>

</div>

<div class="card">

<div class="card-title">

[Information Theory: A Gentle
Introduction](https://towardsdatascience.com/information-theory-a-gentle-introduction-6abaf99835ac?source=rss----7f60cf5620c9---4)

</div>

This is the first in a series of articles about Information Theory and
its relationship to data driven enterprises and strategy. While…

</div>

<div class="card">

<div class="card-title">

[Link Prediction and Information Theory: A
Tutorial](https://towardsdatascience.com/link-prediction-and-information-theory-a-tutorial-a67ecc73e7f9?source=rss----7f60cf5620c9---4)

</div>

<div class="card-image">

[![](https://miro.medium.com/v2/resize:fit:1200/1*VK7jOqx6Vogs_-liMWe5cg.png)](https://towardsdatascience.com/link-prediction-and-information-theory-a-tutorial-a67ecc73e7f9?source=rss----7f60cf5620c9---4)

</div>

Using Mutual Information to measure the likelihood of candidate links in
a graph.

</div>

<div class="card">

<div class="card-title">

[Essential Math for Data Science: Information
Theory](https://towardsdatascience.com/essential-math-for-data-science-information-theory-5d0380232ca1?source=rss----7f60cf5620c9---4)

</div>

<div class="card-image">

[![](https://miro.medium.com/v2/resize:fit:967/1*7BxImLe30p6sT_UqwcDgNw.jpeg)](https://towardsdatascience.com/essential-math-for-data-science-information-theory-5d0380232ca1?source=rss----7f60cf5620c9---4)

</div>

Entropy, cross-entropy, log loss, and KL divergence

</div>

<div class="card">

<div class="card-title">

[A Gentle Introduction to Information Entropy -
MachineLearningMastery.com](https://machinelearningmastery.com/what-is-information-entropy)

</div>

<div class="card-image">

[![](https://machinelearningmastery.com/wp-content/uploads/2019/10/Plot-of-Probability-Distribution-vs-Entropy.png)](https://machinelearningmastery.com/what-is-information-entropy)

</div>

Information theory is a subfield of mathematics concerned with
transmitting data across a noisy channel. A cornerstone of information
theory is the idea of quantifying how much information there is in a
message. More generally, this can be used to quantify the information in
an event and a random variable, called entropy, and is calculated using
probability. Calculating information and…

</div>

<div class="card">

<div class="card-title">

[Entropy and Information
Gain](https://towardsdatascience.com/entropy-and-information-gain-b738ca8abd2a?source=rss----7f60cf5620c9---4)

</div>

<div class="card-image">

[![](https://miro.medium.com/v2/da:true/resize:fit:1200/0*i_v4XULShj9hU4nV)](https://towardsdatascience.com/entropy-and-information-gain-b738ca8abd2a?source=rss----7f60cf5620c9---4)

</div>

Yet another tool used to make Decision Tree splits.

</div>

<div class="card">

<div class="card-title">

[A brief introduction to the beauty of Information
Theory](https://notamonadtutorial.com/a-brief-introduction-to-the-beauty-of-information-theory-8357f5b6a355)

</div>

Lambdaclass's blog about distributed systems, machine learning,
compilers, operating systems, security and cryptography.

</div>

<div class="card">

<div class="card-title">

[Gini coefficient](https://en.wikipedia.org/wiki/Gini_coefficient)

</div>

<div class="card-image">

[![](https://upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Map_of_countries_by_GINI_coefficient_%281990_to_2020%29.svg/1200px-Map_of_countries_by_GINI_coefficient_%281990_to_2020%29.svg.png)](https://en.wikipedia.org/wiki/Gini_coefficient)

</div>

In economics, the Gini coefficient, also known as the Gini index or Gini
ratio, is a measure of statistical dispersion intended to represent the
income inequality, the wealth inequality, or the consumption inequality
within a nation or a social group. It was developed by Italian
statistician and sociologist Corrado Gini.

</div>

</div>
