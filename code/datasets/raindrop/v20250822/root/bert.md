<div class="nav">

⟵ [Up](index.html)  \|  [Index](index.html)

</div>

# bert

<div class="cards">

<div class="card">

<div class="card-title">

[BERT](https://dataconomy.com/2025/02/19/what-is-bert-and-how-it-works/)

</div>

<div class="card-image">

[![](https://dataconomy.com/wp-content/uploads/2022/12/DC-logo-emblem_multicolor.png)](https://dataconomy.com/2025/02/19/what-is-bert-and-how-it-works/)

</div>

BERT is an open source machine learning framework for natural language
processing (NLP) that helps computers understand ambiguous language by
using context

</div>

<div class="card">

<div class="card-title">

[BERT — Intuitively and Exhaustively
Explained](https://towardsdatascience.com/bert-intuitively-and-exhaustively-explained-48a24ecc1c8a?source=rss----7f60cf5620c9---4)

</div>

<div class="card-image">

[![](https://miro.medium.com/v2/resize:fit:1024/1*VwxU1MuBBVaWYXKL8Rlkgw.png)](https://towardsdatascience.com/bert-intuitively-and-exhaustively-explained-48a24ecc1c8a?source=rss----7f60cf5620c9---4)

</div>

Baking General Understanding into Language Models

</div>

<div class="card">

<div class="card-title">

[Deploy an NLP pipeline. Flask Heroku
Bert.](https://towardsdatascience.com/deploy-an-nlp-pipeline-flask-heroku-bert-f13a302efd9d)

</div>

<div class="card-image">

[![](https://miro.medium.com/v2/resize:fit:1200/0*5mrPjkeY-esxQrL-.jpg)](https://towardsdatascience.com/deploy-an-nlp-pipeline-flask-heroku-bert-f13a302efd9d)

</div>

A simple quick solution for deploying an NLP project and challenges you
may faced during the process.

</div>

<div class="card">

<div class="card-title">

[A Beginner’s Guide to Use BERT for the First
Time](https://towardsdatascience.com/a-beginners-guide-to-use-bert-for-the-first-time-2e99b8c5423?source=rss----7f60cf5620c9---4)

</div>

<div class="card-image">

[![](https://miro.medium.com/v2/da:true/resize:fit:1200/0*w3-dy3whMFXPz2XI)](https://towardsdatascience.com/a-beginners-guide-to-use-bert-for-the-first-time-2e99b8c5423?source=rss----7f60cf5620c9---4)

</div>

From predicting single sentence to fine-tuning using custom dataset to
finding the best hyperparameter configuration.

</div>

<div class="card">

<div class="card-title">

[A version of the BERT language model that’s 20 times as
fast](https://www.amazon.science/blog/a-version-of-the-bert-language-model-thats-20-times-as-fast)

</div>

<div class="card-image">

[![](https://assets.amazon.science/dims4/default/7d37b6f/2147483647/strip/true/crop/951x499+0+17/resize/1200x630!/quality/90/?url=http%3A%2F%2Famazon-topics-brightspot.s3.amazonaws.com%2Fscience%2F91%2Fc9%2F4aab043e4973805be90d1737bba1%2Fagora.png)](https://www.amazon.science/blog/a-version-of-the-bert-language-model-thats-20-times-as-fast)

</div>

Determining the optimal architectural parameters reduces network size by
84% while improving performance on natural-language-understanding tasks.

</div>

<div class="card">

<div class="card-title">

[AI devs created a lean, mean, GPT-3-beating machine that uses 99.9%
fewer
p](https://thenextweb.com/neural/2020/09/21/ai-devs-created-a-lean-mean-gpt-3-beating-machine-that-uses-99-9-fewer-parameters)

</div>

<div class="card-image">

[![](https://img-cdn.tnwcdn.com/image/tnw-blurple?filter_last=1&fit=1280%2C640&url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2017%2F07%2Frobots.jpg&signature=b11955d66bd7e3e547541908e602d0b2)](https://thenextweb.com/neural/2020/09/21/ai-devs-created-a-lean-mean-gpt-3-beating-machine-that-uses-99-9-fewer-parameters)

</div>

AI researchers from the Ludwig Maximilian University (LMU) of Munich
have developed a bite-sized text generator capable of besting OpenAI’s
state of the art GPT-3 using only a tiny fraction of its parameters.
GPT-3 is a monster of an AI sys

</div>

<div class="card">

<div class="card-title">

[AI Democratization in the Era of
GPT-3](https://thegradient.pub/ai-democratization-in-the-era-of-gpt-3)

</div>

<div class="card-image">

[![](https://thegradient.pub/content/images/2020/09/main.jpg)](https://thegradient.pub/ai-democratization-in-the-era-of-gpt-3)

</div>

What does Microsoft getting an "exclusive license" to GPT-3 mean for the
future of AI democratization?

</div>

<div class="card">

<div class="card-title">

[NLP — BERT & Transformer - Jonathan Hui -
Medium](https://medium.com/@jonathan_hui/nlp-bert-transformer-7f0ac397f524)

</div>

<div class="card-image">

[![](https://miro.medium.com/v2/resize:fit:792/1*AaRWGD95loQWAHq_ulm2LA.jpeg)](https://medium.com/@jonathan_hui/nlp-bert-transformer-7f0ac397f524)

</div>

Google published an article “Understanding searches better than ever
before” and positioned BERT as one of the most important updates to…

</div>

<div class="card">

<div class="card-title">

[google-research/bert: TensorFlow code and pre-trained models for
BERT](https://github.com/google-research/bert)

</div>

<div class="card-image">

[![](https://opengraph.githubassets.com/23e10f4b72d311b346e00c30e935be92f78d0085246b7dadd1e4fe41d1457c37/google-research/bert)](https://github.com/google-research/bert)

</div>

TensorFlow code and pre-trained models for BERT.

</div>

<div class="card">

<div class="card-title">

[The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)
– J](http://jalammar.github.io/illustrated-bert)

</div>

Discussions: Hacker News (98 points, 19 comments), Reddit
r/MachineLearning (164 points, 20 comments) Translations: Chinese
(Simplified), French 1, French 2, Japanese, Korean, Persian, Russian,
Spanish 2021 Update: I created this brief and highly accessible video
intro to BERT The year 2018 has been an inflection point for machine
learning models handling text (or more accurately, Natural Language
Processing or NLP for short). Our conceptual understanding of how best
to represent words and sentences in a way that best captures underlying
meanings and relationships is rapidly evolving. Moreover, the NLP
community has been putting forward incredibly powerful components that
you can freely download and use in your own models and pipelines (It’s
been referred to as NLP’s ImageNet moment, referencing how years ago
similar developments accelerated the development of machine learning in
Computer Vision tasks).

</div>

<div class="card">

<div class="card-title">

[Jay Alammar – Visualizing machine learning one concept at a
time](http://jalammar.github.io)

</div>

Visualizing machine learning one concept at a time.

</div>

<div class="card">

<div class="card-title">

[Vincent Boucher on LinkedIn: \#transformer \#bert
\#nlp](https://www.linkedin.com/posts/activity-6639302449037406208-LJJ1)

</div>

<div class="card-image">

[![](https://media.licdn.com/dms/image/v2/C4E22AQERV7utC98QDw/feedshare-shrink_800/feedshare-shrink_800/0/1582933054953?e=2147483647&v=beta&t=2rnxyQHfr19Qeo7X9VvVj-AiFlBIbxTBoDMZdyYOgzk)](https://www.linkedin.com/posts/activity-6639302449037406208-LJJ1)

</div>

Pre-training SmallBERTa - A tiny model to train on a tiny dataset An end
to end colab notebook that allows you to train your own LM (using
HuggingFace…

</div>

<div class="card">

<div class="card-title">

[Lit BERT: NLP Transfer Learning In 3 Steps - Towards Data
Science](https://towardsdatascience.com/lit-bert-nlp-transfer-learning-in-3-steps-272a866570db)

</div>

<div class="card-image">

[![](https://miro.medium.com/v2/da:true/resize:fit:440/1*S0pwe67pA780cdQETmGblw.gif)](https://towardsdatascience.com/lit-bert-nlp-transfer-learning-in-3-steps-272a866570db)

</div>

In this tutorial we learn to quickly train Huggingface BERT using
PyTorch Lightning for transfer learning on any NLP task

</div>

<div class="card">

<div class="card-title">

[BERT Explained: A Complete Guide with Theory and
Tutorial](https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial)

</div>

<div class="card-image">

[![](https://towardsml.wordpress.com/wp-content/uploads/2019/09/bert.png?w=1200)](https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial)

</div>

Unless you have been out of touch with the Deep Learning world, chances
are that you have heard about BERT —  it has been the talk of the town
for the last one year. At the end of 2018 researchers …

</div>

<div class="card">

<div class="card-title">

[Deconstructing BERT: Distilling 6 Patterns from 100 Million
Parameters](https://towardsdatascience.com/deconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77)

</div>

<div class="card-image">

[![](https://miro.medium.com/v2/resize:fit:1200/1*dDp0F9AGCuP8MaieI2y3ww.jpeg)](https://towardsdatascience.com/deconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77)

</div>

From BERT’s tangled web of attention, some intuitive patterns emerge.

</div>

</div>
