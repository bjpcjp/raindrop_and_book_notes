<div class="nav">

⟵ [Up](index.html)  \|  [Index](index.html)

</div>

# benchmarks

<div class="cards">

<div class="card">

<div class="card-title">

[Chatbot Arena (formerly LMSYS): Free AI Chat to Compare & Test Best AI
Chatbots](https://lmarena.ai/)

</div>

<div class="card-image">

[![](https://storage.googleapis.com/public-arena-asset/lmsys.jpg)](https://lmarena.ai/)

</div>

</div>

<div class="card">

<div class="card-title">

[319 Top Ecommerce Sites Ranked by User Experience Performance –
Baymard](https://baymard.com/ux-benchmark?_hsenc=p2ANqtz-8vKdTwWLUAiR59qo6YQ_JNSaLK6trzBlYG6V0DtlCCHvakteivmgUcdACP140zbIAtp2EVem2uJQoyPAknl5-ThPCilA&_hsmi=115628620)

</div>

See the ranked UX performance of the 319 leading ecommerce sites in the
US and Europe. The chart summarizes 100,000+ UX performance ratings.

</div>

<div class="card">

<div class="card-title">

[AI Model & API Providers Analysis \| Artificial
Analysis](https://artificialanalysis.ai/)

</div>

<div class="card-image">

[![](https://artificialanalysis.ai/img/open-graph/og-image.png)](https://artificialanalysis.ai/)

</div>

Comparison and analysis of AI models and API hosting providers.
Independent benchmarks across key performance metrics including quality,
price, output speed & latency.

</div>

<div class="card">

<div class="card-title">

[A Technical Roadmap to Context Engineering in LLMs: Mechanisms,
Benchmarks, and Open
Challenges](https://www.marktechpost.com/2025/08/03/a-technical-roadmap-to-context-engineering-in-llms-mechanisms-benchmarks-and-open-challenges/)

</div>

<div class="card-image">

[![](https://www.marktechpost.com/wp-content/uploads/2025/08/Screenshot-2025-08-03-at-2.21.48-PM.png)](https://www.marktechpost.com/2025/08/03/a-technical-roadmap-to-context-engineering-in-llms-mechanisms-benchmarks-and-open-challenges/)

</div>

Context engineering for large language models—frameworks, architectures,
and strategies to optimize AI reasoning, and scalability

</div>

<div class="card">

<div class="card-title">

[The Leaderboard Illusion](https://arxiv.org/abs/2504.20879)

</div>

<div class="card-image">

[![](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png)](https://arxiv.org/abs/2504.20879)

</div>

Measuring progress is fundamental to the advancement of any scientific
field. As benchmarks play an increasingly central role, they also grow
more susceptible to distortion. Chatbot Arena has emerged as the go-to
leaderboard for ranking the most capable AI systems. Yet, in this work
we identify systematic issues that have resulted in a distorted playing
field. We find that undisclosed private testing practices benefit a
handful of providers who are able to test multiple variants before
public release and retract scores if desired. We establish that the
ability of these providers to choose the best score leads to biased
Arena scores due to selective disclosure of performance results. At an
extreme, we identify 27 private LLM variants tested by Meta in the
lead-up to the Llama-4 release. We also establish that proprietary
closed models are sampled at higher rates (number of battles) and have
fewer models removed from the arena than open-weight and open-source
alternatives. Both these policies lead to large data access asymmetries
over time. Providers like Google and OpenAI have received an estimated
19.2% and 20.4% of all data on the arena, respectively. In contrast, a
combined 83 open-weight models have only received an estimated 29.7% of
the total data. We show that access to Chatbot Arena data yields
substantial benefits; even limited additional data can result in
relative performance gains of up to 112% on the arena distribution,
based on our conservative estimates. Together, these dynamics result in
overfitting to Arena-specific dynamics rather than general model
quality. The Arena builds on the substantial efforts of both the
organizers and an open community that maintains this valuable evaluation
platform. We offer actionable recommendations to reform the Chatbot
Arena's evaluation framework and promote fairer, more transparent
benchmarking for the field

</div>

<div class="card">

<div class="card-title">

[QOpt / QOBLIB - Quantum Optimization Benchmarking Library ·
GitLab](https://git.zib.de/qopt/qoblib-quantum-optimization-benchmarking-library)

</div>

<div class="card-image">

[![](https://git.zib.de/assets/twitter_card-570ddb06edf56a2312253c5872489847a0f385112ddbcd71ccfa1570febab5d2.jpg)](https://git.zib.de/qopt/qoblib-quantum-optimization-benchmarking-library)

</div>

This is the ZIB GitLab instance

</div>

<div class="card">

<div class="card-title">

[A look at the ARC-AGI exam designed by French computer scientist
François Chollet to show the gulf between AI models' memorized answers
and “fluid intelligence”](http://www.techmeme.com/250407/p2#a250407p2)

</div>

<div class="card-image">

[![](https://i.imgur.com/q0p1thB.jpg)](http://www.techmeme.com/250407/p2#a250407p2)

</div>

By Matteo Wong / The Atlantic. View the full context on Techmeme.

</div>

<div class="card">

<div class="card-title">

[The Man Out to Prove How Dumb AI Still
Is](https://www.theatlantic.com/technology/archive/2025/04/arc-agi-chollet-test/682295/?gift=2iIN4YrefPjuvZ5d2Kh3089M3DxlABplHmODO9XssmE&utm_source=copy-link&utm_medium=social&utm_campaign=share)

</div>

<div class="card-image">

[![](https://cdn.theatlantic.com/thumbor/TQ7unWwrL3WWTg13JHKKLeS9j1w=/0x43:2000x1085/1200x625/filters:watermark(https://cdn.theatlantic.com/media/files/badge_2x.png,-20,20,0,33)/media/img/mt/2025/04/THE_ATLANTIC_ANIMATION_V2/original.gif)](https://www.theatlantic.com/technology/archive/2025/04/arc-agi-chollet-test/682295/?gift=2iIN4YrefPjuvZ5d2Kh3089M3DxlABplHmODO9XssmE&utm_source=copy-link&utm_medium=social&utm_campaign=share)

</div>

François Chollet has constructed the ultimate test for the bots.

</div>

<div class="card">

<div class="card-title">

[LLM Benchmarking: Fundamental Concepts \| NVIDIA Technical
Blog](https://developer.nvidia.com/blog/llm-benchmarking-fundamental-concepts/)

</div>

<div class="card-image">

[![](https://developer-blogs.nvidia.com/wp-content/uploads/2025/03/data-center.png)](https://developer.nvidia.com/blog/llm-benchmarking-fundamental-concepts/)

</div>

The past few years have witnessed the rise in popularity of generative
AI and large language models (LLMs), as part of a broad AI revolution.

</div>

<div class="card">

<div class="card-title">

[What is METEOR score? -
Dataconomy](https://dataconomy.com/2025/04/02/what-is-meteor-score/)

</div>

<div class="card-image">

[![](https://dataconomy.com/wp-content/uploads/2022/12/DC-logo-emblem_multicolor.png)](https://dataconomy.com/2025/04/02/what-is-meteor-score/)

</div>

METEOR Score is a metric used to evaluate the quality of machine
translation based on precision, recall, word alignment, and linguistic
flexibility.

</div>

<div class="card">

<div class="card-title">

[LLM Leaderboard](https://artificialanalysis.ai/leaderboards/models)

</div>

<div class="card-image">

[![](https://artificialanalysis.ai/img/open-graph/og-image.png)](https://artificialanalysis.ai/leaderboards/models)

</div>

Comparison and ranking the performance of over 30 AI models (LLMs)
across key metrics including quality, price, performance and speed
(output speed - tokens per second & latency - TTFT), context window &
others.

</div>

<div class="card">

<div class="card-title">

[aidanmclaughlin/AidanBench: Aidan Bench attempts to measure in
LLMs.](https://github.com/aidanmclaughlin/AidanBench)

</div>

<div class="card-image">

[![](https://repository-images.githubusercontent.com/838396720/3107078a-5021-424f-a295-6306de266b1e)](https://github.com/aidanmclaughlin/AidanBench)

</div>

Aidan Bench attempts to measure in LLMs. - aidanmclaughlin/AidanBench

</div>

<div class="card">

<div class="card-title">

[Key Metrics for Evaluating Large Language Models
(LLMs)](https://www.marktechpost.com/2024/06/19/key-metrics-for-evaluating-large-language-models-llms)

</div>

<div class="card-image">

[![](https://www.marktechpost.com/wp-content/uploads/2024/06/Screenshot-2024-06-19-at-7.20.10-PM-1024x906.png)](https://www.marktechpost.com/2024/06/19/key-metrics-for-evaluating-large-language-models-llms)

</div>

Evaluating Large Language Models (LLMs) is a challenging problem in
language modeling, as real-world problems are complex and variable.
Conventional benchmarks frequently fail to fully represent LLMs'
all-encompassing performance. A recent LinkedIn post has emphasized a
number of important measures that are essential to comprehend how well
new models function, which are as follows. MixEval Achieving a balance
between thorough user inquiries and effective grading systems is
necessary for evaluating LLMs. Conventional standards based on ground
truth and LLM-as-judge benchmarks encounter difficulties such as biases
in grading and possible contamination over time.  MixEval solves these
problems by combining real-world user

</div>

<div class="card">

<div class="card-title">

[Nvidia Conquers Latest AI
Tests​](https://spectrum.ieee.org/mlperf-nvidia-conquers)

</div>

<div class="card-image">

[![](https://spectrum.ieee.org/media-library/row-upon-row-of-computers-emerging-from-the-darkness.jpg?id=52442384&width=1200&height=600&coordinates=0%2C20%2C0%2C20)](https://spectrum.ieee.org/mlperf-nvidia-conquers)

</div>

GPU maker tops new MLPerf benchmarks on graph neural nets and LLM
fine-tuning

</div>

<div class="card">

<div class="card-title">

[Open LLM Leaderboard : a Hugging Face Space by
HuggingFaceH4](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)

</div>

<div class="card-image">

[![](https://cdn-thumbnails.huggingface.co/social-thumbnails/spaces/open-llm-leaderboard/open_llm_leaderboard.png)](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)

</div>

Track, rank and evaluate open LLMs and chatbots

</div>

<div class="card">

<div class="card-title">

[Asking 60+ LLMs a set of 20
questions](https://benchmarks.llmonitor.com)

</div>

Human-readable benchmarks of 60+ open-source and proprietary LLMs.

</div>

<div class="card">

<div class="card-title">

[A Deep Dive Into LLaMA, Falcon, Llama 2 and Their Remarkable Fine-Tuned
Ver](https://www.turingpost.com/p/top3llmsope)

</div>

<div class="card-image">

[![](https://beehiiv-images-production.s3.amazonaws.com/uploads/asset/file/f739d30a-90f3-4c02-9e23-f40e93f92c28/Frame_143.png?t=1693246280)](https://www.turingpost.com/p/top3llmsope)

</div>

Exploring the Development of the 3 Leading Open LLMs and Their Chatbot
Derivatives

</div>

<div class="card">

<div class="card-title">

[Calculate Computational Efficiency of Deep Learning Models with FLOPs
and
M](https://www.kdnuggets.com/2023/06/calculate-computational-efficiency-deep-learning-models-flops-macs.html)

</div>

<div class="card-image">

[![](https://www.kdnuggets.com/wp-content/uploads/li_calculate_computational_efficiency_deep_learning_models_flops_macs_2.png)](https://www.kdnuggets.com/2023/06/calculate-computational-efficiency-deep-learning-models-flops-macs.html)

</div>

In this article we will learn about its definition, differences and how
to calculate FLOPs and MACs using Python packages.

</div>

<div class="card">

<div class="card-title">

[Towards a Benchmarking Suite for Kernel
Tuners](https://hgpu.org/?p=28050)

</div>

<div class="card-image">

[![](https://hgpu.org/img/social-logo.png)](https://hgpu.org/?p=28050)

</div>

As computing system become more complex, it is becoming harder for
programmers to keep their codes optimized as the hardware gets updated.
Autotuners try to alleviate this by hiding as many archite…

</div>

<div class="card">

<div class="card-title">

[How to Choose the Best Machine Learning Technique: Comparison
Table](https://www.datasciencecentral.com/how-to-choose-the-best-machine-learning-technique-comparison-table)

</div>

<div class="card-image">

[![](https://www.datasciencecentral.com/wp-content/uploads/2022/11/jp3-scaled.jpg)](https://www.datasciencecentral.com/how-to-choose-the-best-machine-learning-technique-comparison-table)

</div>

How to Choose the Best Machine Learning Technique: Comparison Table

</div>

<div class="card">

<div class="card-title">

[7 Steps to Benchmark Your Product’s
UX](https://www.nngroup.com/articles/product-ux-benchmarks)

</div>

<div class="card-image">

[![](https://media.nngroup.com/media/articles/opengraph_images/7-Steps-Benchmark_Social-Media-Posts_2020.png)](https://www.nngroup.com/articles/product-ux-benchmarks)

</div>

Benchmark your UX by first determining appropriate metrics and a study
methodology. Then track these metrics across different releases of your
product by running studies that follow the same established methodology.

</div>

<div class="card">

<div class="card-title">

[12 Twitter Sentiment Analysis Algorithms
Compared](https://towardsdatascience.com/12-twitter-sentiment-analysis-algorithms-compared-23e2d2c63d90?source=rss----7f60cf5620c9---4)

</div>

<div class="card-image">

[![](https://miro.medium.com/v2/resize:fit:1200/0*kJawLKlRt398g2xy.jpg)](https://towardsdatascience.com/12-twitter-sentiment-analysis-algorithms-compared-23e2d2c63d90?source=rss----7f60cf5620c9---4)

</div>

12 sentiment analysis algorithms were compared on the accuracy of tweet
classification. The fasText deep learning system was the winner.

</div>

<div class="card">

<div class="card-title">

[Benchmark functions \| BenchmarkFcns](http://benchmarkfcns.xyz/fcns)

</div>

This website is for sale! benchmarkfcns.xyz is your first and best
source for all of the information you’re looking for. From general
topics to more of what you would expect to find here, benchmarkfcns.xyz
has it all. We hope you find what you are searching for!

</div>

<div class="card">

<div class="card-title">

[AI and Efficiency](https://openai.com/blog/ai-and-efficiency)

</div>

<div class="card-image">

[![](https://images.ctfassets.net/kftzwdyauwt9/e5cba1f8-883b-4961-dca6f726f936/be1a627ab2f509a68b91b8a7ef29f610/image_128.png?w=1600&h=900&fit=fill)](https://openai.com/blog/ai-and-efficiency)

</div>

We’re releasing an analysis showing that since 2012 the amount of
compute needed to train a neural net to the same performance on
ImageNet classification has been decreasing by a factor of 2 every 16
months. Compared to 2012, it now takes 44 times less compute to train a
neural network to the level of AlexNet (by contrast, Moore’s Law would
yield an 11x cost improvement over this period). Our results suggest
that for AI tasks with high levels of recent investment, algorithmic
progress has yielded more gains than classical hardware efficiency.

</div>

<div class="card">

<div class="card-title">

[Machine Learning Benchmarking: You’re Doing It
Wrong](http://blog.bigml.com/2020/03/20/machine-learning-benchmarking-youre-doing-it-wrong)

</div>

<div class="card-image">

[![](https://blog.bigml.com/wp-content/uploads/2020/03/fruit_apples_produce_oranges_comparison_supermaket_chalkandcheese_applesandpears-464484.jpg?w=497)](http://blog.bigml.com/2020/03/20/machine-learning-benchmarking-youre-doing-it-wrong)

</div>

I’m not going to bury the lede: Most machine learning benchmarks are
bad.  And not just kinda-sorta nit-picky bad, but catastrophically and
fundamentally flawed.  TL;DR: Please, for the love of sta…

</div>

<div class="card">

<div class="card-title">

[Benchmark Work \| Benchmarks MLCommons](https://mlperf.org)

</div>

<div class="card-image">

[![](https://mlcommons.org/wp-content/uploads/2023/10/decoration-2.png)](https://mlperf.org)

</div>

MLCommons ML benchmarks help balance the benefits and risks of AI
through quantitative tools that guide responsible AI development.

</div>

<div class="card">

<div class="card-title">

[Inference Results – MLPerf](https://mlperf.org/inference-results)

</div>

<div class="card-image">

[![](https://mlcommons.org/wp-content/uploads/2023/10/decoration-2.png)](https://mlperf.org/inference-results)

</div>

MLCommons ML benchmarks help balance the benefits and risks of AI
through quantitative tools that guide responsible AI development.

</div>

<div class="card">

<div class="card-title">

[Buyer UX ecommerce
Benchmarking](https://docs.google.com/presentation/d/1j_-tDctiFQew9PAKMZPvH87rFeXCSIcP81Ti8CDGnLg/edit?usp=embed_facebook)

</div>

<div class="card-image">

[![](https://lh7-us.googleusercontent.com/docs/AHkbwyIbKFrYx10_wOZnlNkHuuz1xGHxf5lE1gUiH17pWc4PjAVoA3XpKiXARePf-FLnzFQJFuamxVwUFGFI1BiYqqyyEQi-LRhvSXLEGYXCqeCSewEKxgTG=w1200-h630-p)](https://docs.google.com/presentation/d/1j_-tDctiFQew9PAKMZPvH87rFeXCSIcP81Ti8CDGnLg/edit?usp=embed_facebook)

</div>

Buyer Experience Benchmarking of 5 Top eCommerce Sites Dec 2018 Ken
Leaver

</div>

<div class="card">

<div class="card-title">

[One Deep Learning Benchmark to Rule Them
All](https://www.nextplatform.com/2018/08/30/one-deep-learning-benchmark-to-rule-them-all)

</div>

<div class="card-image">

[![](https://www.nextplatform.com/wp-content/uploads/2016/05/TPU_main2-1024x1024.jpg)](https://www.nextplatform.com/2018/08/30/one-deep-learning-benchmark-to-rule-them-all)

</div>

Over the last few years we have detailed the explosion in new machine
learning systems with the influx of novel architectures from deep
learning chip

</div>

<div class="card">

<div class="card-title">

[Start With Gradient Boosting, Results from Comparing 13 Algorithms on
165 D](https://machinelearningmastery.com/start-with-gradient-boosting)

</div>

<div class="card-image">

[![](https://machinelearningmastery.com/wp-content/uploads/2018/01/Algorithm-performance-improvement-via-parameter-tuning.png)](https://machinelearningmastery.com/start-with-gradient-boosting)

</div>

Which machine learning algorithm should you use? It is a central
question in applied machine learning. In a recent paper by Randal Olson
and others, they attempt to answer it and give you a guide for
algorithms and parameters to try on your problem first, before spot
checking a broader suite of algorithms. In this post, you will discover
a…

</div>

<div class="card">

<div class="card-title">

[machine learning benchmarks - Google
Search](https://www.google.com/search?aqs=chrome..69i57.7070j0j7&ie=UTF-8&oq=machine+learning+benchmarks&q=machine+learning+benchmarks&sourceid=chrome)

</div>

</div>

<div class="card">

<div class="card-title">

[Why Python is Slow: Looking Under the Hood \| Pythonic
Perambulations](http://jakevdp.github.io/blog/2014/05/09/why-python-is-slow)

</div>

</div>

</div>
