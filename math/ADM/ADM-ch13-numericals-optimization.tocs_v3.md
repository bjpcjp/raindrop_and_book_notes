[Representative image](ADM-ch13-numericals-optimization.best.png)

- **13.5 Constrained and Unconstrained Optimization**
  - **Problem Definition**
    - The goal is to find a point \( p = (p_1, \ldots, p_n) \) that maximizes or minimizes a function \( f(x_1, \ldots, x_n) \).
    - The problem applies to tuning parameters for optimal performance in various applications, such as stock evaluation.
    - Functions may represent objective criteria like "stock-goodness" weighted by coefficients.
    - Optimization arises where no problem-specific structure can be exploited.
  - **Key Questions in Optimization**
    - Differentiates between constrained and unconstrained optimization based on parameter limits.
    - Considers whether the function is given by a formula, allowing analytical derivative computation.
    - Discusses the cost of function evaluation, relevant when evaluation is expensive and slow.
    - Emphasizes the difficulty increasing with the number of parameters or dimensions.
    - Highlights the importance of function smoothness to avoid local optima traps.
  - **Methods for Unconstrained Optimization**
    - Uses derivatives or numerical estimations of derivatives to find local optima.
    - Steepest descent and conjugate gradient methods relate to numerical root-finding.
    - Advises experimenting with several methods before implementing new algorithms.
    - References "Numerical Recipes" for detailed algorithm descriptions.
  - **Methods for Constrained Optimization**
    - Finding points that satisfy all constraints is often the main challenge.
    - Penalty functions can transform constrained problems into unconstrained ones by penalizing constraint violations.
    - Simulated annealing is a robust, simple approach particularly suited for combinatorial constrained optimization.
    - Suggests adaptive penalty adjustment during optimization to ensure constraint satisfaction at completion.
  - **Software and Implementations**
    - Hans Mittlemannâ€™s Decision Tree for Optimization Software provides guidance at [plato.asu.edu/guide.html](http://plato.asu.edu/guide.html).
    - GAMS and NIST Guide to Available Mathematical Software are useful resources: [gams.nist.gov](http://gams.nist.gov).
    - NEOS provides remote optimization services supporting linear programming and unconstrained optimization: [neos.mcs.anl.gov](http://www-neos.mcs.anl.gov/).
    - Collected Algorithms of the ACM include Fortran codes for unconstrained optimization, available via Netlib.
    - Adaptive Simulated Annealing (ASA) is a popular implementation for simulated annealing in C: [asa-caltech.sourceforge.net](http://asa-caltech.sourceforge.net/).
    - Genetic algorithm libraries include JGAP (Java) and GAUL (C), though skepticism about genetic algorithms is noted.
  - **Notes and References**
    - Steepest-descent methods are covered in numerical methods literature such as [BT92] and [PFTV07].
    - Unconstrained optimization is discussed in books including [Bre73] and [Fle80].
    - Simulated annealing is based on Kirkpatrick et al.'s work and related to the Metropolis algorithm.
    - Local search heuristics, including simulated annealing, are well described in [AL97].
    - Genetic algorithms originated from Holland's work; sympathetic overviews include [LP02] and [MF00].
    - Tabu search is another heuristic with dedicated followers, detailed in [Glo90].
  - **Related Problems**
    - Linear programming is a related constrained optimization problem (see section on page 411).
    - Satisfiability problems are also related (see page 472).
