[Representative image](ADM-ch08-dynamic-programming.best.png)

- **Dynamic Programming**
  - The technique addresses optimization problems by systematically searching all possibilities while storing results to avoid recomputation.
  - Dynamic programming combines correctness guarantees of exhaustive search with efficiency of caching subproblem results.
  - Problems suited to dynamic programming often feature an inherent left-to-right order on components such as strings, trees, polygons, and sequences.
  - Further reading: [The Algorithm Design Manual, Dynamic Programming](https://doi.org/10.1007/978-1-84800-070-4)

- **Caching vs. Computation**
  - Caching stores intermediate results of recursive computations such as Fibonacci numbers to trade space for significant time savings.
  - Recursive Fibonacci calculation without caching is exponential due to repeated computation of the same subproblems.
  - Cached or iterative dynamic programming implementations reduce Fibonacci computation time to linear, with linear or constant space requirements.
  - Binomial coefficients can be computed efficiently via dynamic programming by building Pascal’s triangle using a recurrence relation.
  - Further reading: [Dynamic Programming by Example: Fibonacci](https://doi.org/10.1007/978-1-84800-070-4)

- **Approximate String Matching**
  - Defines edit distance using costs for substitution, insertion, and deletion to measure string similarity in the presence of errors.
  - Recursive algorithms for edit distance are correct but suffer from exponential runtime due to overlapping subproblems.
  - Dynamic programming improves efficiency by constructing a two-dimensional cost table storing results of subproblems, running in O(|P|·|T|) time.
  - Parent pointer tables enable reconstruction of the sequence of edit operations from the DP matrix.
  - Variations of edit distance, such as substring matching, longest common subsequence, and maximum monotone subsequence, are solved via modifying cost functions and DP boundaries.
  - Further reading: [Wagner and Fischer, The String-to-String Correction Problem](https://doi.org/10.1145/360825.360852)

- **Longest Increasing Sequence**
  - The recurrence relates the length of the longest increasing subsequence ending at each element based on previous elements smaller than the current one.
  - Overall longest increasing subsequence is the maximum of all such ending lengths.
  - Time complexity of the naive dynamic program is O(n²), with more advanced data structures enabling O(n log n) algorithms.
  - Auxiliary predecessor pointers allow reconstruction of one longest increasing subsequence.
  - Further reading: [Longest Increasing Subsequence on GeeksforGeeks](https://www.geeksforgeeks.org/longest-increasing-subsequence-dp-3/)

- **War Story: Evolution of the Lobster**
  - The image morphing problem reduces to matching line segments using dynamic programming through non-crossing interval correspondences.
  - The solution involves splitting lines into runs of pixels and computing minimal-cost matchings based on run length differences and positions.
  - Dynamic programming exploits the left-to-right order and non-crossing conditions to ensure subproblems are independent and combinatorial explosion is avoided.
  - Further reading: [Morphing Techniques in Computer Graphics](https://doi.org/10.1145/202306.202319)

- **The Partition Problem**
  - Given a list of sizes and a number k, partition the list into k contiguous parts minimizing the largest sum among parts.
  - A recursive definition expresses the minimal maximum partition sum in terms of smaller subproblems, yielding a DP with O(k n²) time complexity.
  - Prefix sums enable efficient computation of partition sums without repeated summations.
  - A backtracking table stores divider positions enabling reconstruction of the optimal partitions.
  - Further reading: [The Linear Partition Problem](https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.48.6712)

- **Parsing Context-Free Grammars**
  - Parsing is the process of deriving a string from a grammar represented using rules in Chomsky Normal Form.
  - The Cocke-Kasami-Younger (CKY) algorithm uses dynamic programming to test if substrings are generated by certain nonterminals.
  - Complexity is O(n³) time due to testing all partitions and grammar rules over O(n²) substrings.
  - Extensions include the parsimonious parser, which finds the minimal number of substitutions needed for a string to be generated by a grammar.
  - Further reading: [CKY Parsing Algorithm](https://en.wikipedia.org/wiki/CYK_algorithm)

- **Minimum Weight Triangulation**
  - Triangulations partition a polygon into non-overlapping triangles using diagonals.
  - The recurrence computes the minimal triangulation cost by choosing a vertex k to pair with a chord (i, j) and recursively triangulating sub-polygons.
  - Time complexity is cubic O(n³), and space complexity is O(n²).
  - Presence of interior points complicates the problem, making it NP-complete for general point sets.
  - Further reading: [Computational Geometry: Algorithms and Applications](https://doi.org/10.1007/978-3-540-77974-2)

- **Limitations of Dynamic Programming: TSP**
  - The longest simple path and traveling salesman problems lack inherent left-to-right order, resulting in exponential state space.
  - Dynamic programming recurrences must respect the principle of optimality; states must sufficiently summarize subproblems.
  - Encoding the path explicitly requires tracking subsets of vertices, leading to O(2^n) complexity even with dynamic programming.
  - Dynamic programming is effective primarily on problems where objects possess 1-D orderings.
  - Further reading: [Held-Karp Algorithm for TSP](https://doi.org/10.1145/3147.3165)

- **War Story: What’s Past is Prolog**
  - Minimizing trie size for rule head unification in Prolog requires finding an ordering of character positions that reduces trie edges.
  - Maintaining lexicographic leaf order imposes constraints that enable a polynomial-time dynamic programming solution.
  - Without leaf order constraints, the problem becomes NP-complete.
  - The dynamic programming recurrence computes minimal edge counts by choosing character positions and summing costs of induced subtries.
  - Further reading: [Trie Data Structures](https://en.wikipedia.org/wiki/Trie)

- **War Story: Text Compression for Bar Codes**
  - PDF-417 is a 2D bar code symbology supporting multiple text modes, each encoding a subset of alphanumeric characters.
  - Mode switches and latches change encoding context, incurring additional cost.
  - Finding the minimal-length encoding is solved precisely by dynamic programming computing minimal cost encodings per mode per position.
  - The DP runs in O(n) time with constant mode states, improving storage utilization by 8% on average over greedy approaches.
  - Further reading: [PDF417 Bar Code Specification](https://www.adobe.com/content/dam/acom/en/devnet/acrobat/pdfs/pdf417.pdf)
