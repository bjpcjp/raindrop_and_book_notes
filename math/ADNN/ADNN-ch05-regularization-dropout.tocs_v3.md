[ADNN-ch05-regularization-dropout](ADNN-ch05-regularization-dropout.best.png)

- **Regularization and Dropout**
  - **Part 5.1: Introduction to Regularization: Ridge and Lasso**
    - Regularization reduces overfitting by discouraging neural networks from memorizing training data through weight penalties.  
    - Human programmers can overfit by memorizing practice exams rather than learning problem-solving techniques.  
    - Regularization works by adding a penalty related to network weights during training, influencing their size.  
    - See [Ng, 2004] for foundational details on L1 and L2 regularization.  
  - **L1 and L2 Regularization**
    - L1 (Lasso) adds an absolute value penalty to produce sparsity by pushing weights to zero, enabling feature selection.  
    - L2 (Ridge) adds a squared penalty encouraging small weights, leading to less overfitting without sparsity.  
    - Both exclude bias terms from the penalty and can be added to gradient or objective-function training.  
    - Ridge typically yields better overall performance; Lasso is helpful for pruning unnecessary inputs.  
  - **Linear Regression**
    - Linear regression serves as an introductory context to illustrate effects of L1 and L2 penalties.  
    - The auto-mpg dataset demonstrates differences in coefficients and RMSE with and without regularization.  
  - **L1 (Lasso) Regularization**
    - Lasso creates sparse models by enforcing many weights near zero, effectively performing feature selection.  
    - Coefficient magnitudes shrink compared to linear regression, as shown with alpha tuning via cross-validation.  
  - **L2 (Ridge) Regularization**
    - Ridge penalizes the sum of squared weights, generally preserving all inputs but shrinking coefficients uniformly.  
    - Typical alpha values are below 0.1; bias terms are not penalized.  
  - **ElasticNet Regularization**
    - ElasticNet combines penalties from both L1 and L2, controlled by alpha and l1_ratio parameters.  
    - It balances sparsity and weight shrinkage, useful when neither pure Lasso nor Ridge is ideal.  
  - **Part 5.2: Using K-Fold Cross-validation with Keras**
    - Cross-validation trains multiple models on different folds to evaluate generalization, useful for hyperparameter tuning.  
    - For regression, KFold is used with randomly partitioned folds; for classification, StratifiedKFold ensures class balance.  
    - Out-of-sample predictions can be generated by averaging model outputs, selecting the best model, or retraining on full data.  
  - **Regression vs Classification K-Fold Cross-Validation**
    - Regression uses KFold with random splits without stratification; folds can vary in size.  
    - Classification requires StratifiedKFold to preserve the original class distribution across folds.  
  - **Out-of-Sample Regression Predictions with K-Fold Cross-Validation**
    - Demonstrated a 5-fold CV on the jh-simple-dataset to predict age using a neural network trained for 500 epochs.  
    - Reported fold-wise RMSE and final aggregated out-of-sample RMSE provide expected performance estimates.  
  - **Classification with Stratified K-Fold Cross-Validation**
    - Applied 5-fold stratified CV to the jh-simple-dataset for multiclass product prediction, maintaining class distribution.  
    - Raw probabilities were converted to predicted classes with accuracy measured for each fold and overall.  
  - **Training with both a Cross-Validation and a Holdout Set**
    - A holdout set (typically 10%) is reserved before CV to provide an unbiased evaluation for final model validation.  
    - CV is performed on remaining data, training multiple neural networks and reporting fold scores and final holdout RMSE.  
  - **Part 5.3: L1 and L2 Regularization to Decrease Overfitting**
    - Both L1 and L2 regularization add weight penalties during backpropagation to reduce overfitting in neural networks.  
    - L1 constrains weights toward a Laplace distribution promoting sparsity; L2 towards a Gaussian distribution promoting small values.  
    - Penalties are applied only to weights, excluding bias; in Keras, they can be added via kernel_regularizer parameters.  
    - Cross-validated classification models using L1 regularization showed measurable accuracy improvements.  
  - **Part 5.4: Drop Out for Keras to Decrease Overfitting**
    - Dropout prevents overfitting by randomly disabling neurons during training, reducing coadaptation and forcing generalization.  
    - Unlike L1/L2, dropout temporarily masks neurons without modifying weight magnitudes or pruning features.  
    - Dropout simulates ensemble learning by training numerous thinned subnetworks, improving robustness without additional models.  
    - Implemented as a layer in Keras, usually used between dense layers, with dropout probability controlling neuron retention.  
    - Demonstrated dropout usage with Keras for classification, achieving higher accuracy compared to models without dropout.  
    - See [Hinton et al., 2012] for original dropout introduction.  
  - **Part 5.5: Benchmarking Regularization Techniques**
    - Hyperparameters affecting model performance include layer counts, neuron counts, activation functions, dropout rates, and L1/L2 penalties.  
    - Bootstrapping involves iterative train/validation splits with replacement to benchmark hyperparameters over many cycles.  
    - Bootstrapping differs from cross-validation by sampling with replacement, allowing duplicate samples, and theoretically unlimited cycles.  
    - Regression bootstrapping uses ShuffleSplit; classification bootstrapping uses StratifiedShuffleSplit to maintain class balance.  
    - Demonstrated bootstrapping on the jh-simple-dataset with early stopping and extensive logging of score, epoch count, and timing.  
    - Final optimized model settings incorporated multiple layers, dropout, PReLU activation, and L2 regularization, improving log loss in classification.  
    - Additional reading includes [A Recipe for Training Neural Networks](https://cs.stanford.edu/~ang/papers/nips07-early-stopping.pdf).
