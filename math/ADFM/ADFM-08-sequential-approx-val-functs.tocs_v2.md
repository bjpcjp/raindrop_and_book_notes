- **Approximate Value Functions**
  - **Parametric Representations**
    - Value functions are approximated using parameter vectors θ with various possible forms.
    - Action extraction uses an arg max over expected rewards plus discounted future values.
    - Local approximations weight utilities from a set of states S, while global approximations combine basis functions linearly.
    - Iterative approximate dynamic programming alternates between Bellman backups and parametric refitting.
    - Further reading: [Approximate Dynamic Programming: Solving the Curses of Dimensionality](https://onlinelibrary.wiley.com/doi/book/10.1002/9781118032718)
  - **Nearest Neighbor**
    - Approximates state value as the utility of the closest—or k closest—states using a distance metric.
    - Produces piecewise constant value functions; k determines smoothness.
    - Efficiency can be improved using specialized data structures like kd-trees.
  - **Kernel Smoothing**
    - Approximates values as a weighted sum using kernel functions that decrease with state distance.
    - Kernels can be inverse distance or Gaussian, producing smooth approximations.
    - Useful for continuous state spaces and demonstrated on discrete hex world and mountain car problems.
  - **Linear Interpolation**
    - Estimates values by linearly combining utilities from vertices of a grid cell containing the state.
    - Bilinear interpolation extends this to two dimensions using four vertices.
    - Weights correspond to relative distances or areas opposite each vertex.
  - **Simplex Interpolation**
    - More efficient than multilinear interpolation in high dimensions by interpolating over d+1 vertices forming a simplex.
    - Uses Coxeter-Freudenthal-Kuhn triangulation to partition cells into simplexes for continuity.
    - Interpolates values by sorting and weighting based on ordered coordinates within the grid cell.
    - Further reading: [Simplicial Mesh Generation with Applications](https://ecommons.cornell.edu/handle/1813/6361)
  - **Linear Regression**
    - Represents the value function as a linear combination of nonlinear basis functions.
    - Parameters θ fit by minimizing squared error using matrix pseudoinverse techniques.
    - Basis functions influence approximation quality; polynomial, sinusoidal, and Fourier bases are common.
    - Suitable for global approximations with potentially nonlinear effects on state variables.
    - Further reading: [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/)
  - **Neural Network Regression**
    - Uses neural networks to represent value functions without manually selecting basis functions.
    - Network parameters correspond to weights optimized via gradient descent to reduce prediction error.
    - Enables complex, nonlinear approximations but requires more complex training compared to linear methods.
    - Further reading: [Deep Learning](https://www.deeplearningbook.org/)
  - **Summary**
    - Value function approximation is essential for large or continuous state spaces where tabular methods are infeasible.
    - Local methods rely on values of nearby known states, including nearest neighbor, kernel smoothing, linear, and simplex interpolation.
    - Global methods include linear regression with basis functions and neural network regression.
    - Approximations are iteratively refined using dynamic programming applied at selected states.
- **Exercises**
  - **Exercise 8.1: Tabular Representation as Linear Approximation**
    - Demonstrates equivalence by using indicator functions as basis functions for each discrete state or state-action pair.
  - **Exercise 8.2: Parameter Counts in Local vs. Global Approximations**
    - Local approximation parameter count equals number of state-action pairs times actions.
    - Global approximation parameter count equals number of chosen basis functions.
  - **Exercise 8.3: k-Nearest Neighbor with Different Metrics**
    - Computes approximate state value using L1, L2, and L∞ norms.
    - Shows how choice of distance metric affects neighbor selection and resulting approximation.
  - **Exercise 8.4: Validity of Weighting Functions**
    - Evaluates whether given weighting functions sum to one.
    - Shows normalization fixes invalid weighting functions.
  - **Exercise 8.5: Invariance of Bilinear Interpolation under Linear Scaling**
    - Proof that bilinear interpolation results remain unchanged if the grid axes are scaled linearly.
  - **Exercise 8.6: Bilinear Interpolation Coefficients for Given Grid and State**
    - Substitutes specific coordinates into bilinear interpolation formula to produce explicit interpolant.
  - **Exercise 8.7: Simplex Interpolation Weights Computation**
    - Calculates vertex weights for a given state within a simplex using permutation sorting and linear constraints.
