- **Exploration and Exploitation**
  - Exploration and exploitation must be balanced to accumulate reward effectively.
  - Pure exploration builds a comprehensive model but sacrifices immediate reward.
  - Pure exploitation selects the best-known action, possibly missing better options.
  - For a comprehensive review, see [Reinforcement Learning: State of the Art](https://link.springer.com/book/10.1007/978-3-642-27645-3).

- **Bandit Problems**
  - Slot machines model bandit problems, involving single-state MDPs with multiple actions.
  - Each arm yields a binary reward with unknown win probability θ.
  - Real-world applications include clinical trials and adaptive network routing.
  - Historical difficulty of bandit problems is documented in [Gittins, 1979](https://www.jstor.org/stable/2984971).

- **Bayesian Model Estimation**
  - The beta distribution models beliefs about arm win probabilities.
  - Posterior distributions update with observed wins and losses using Beta(wa + 1, `a + 1).
  - Greedy actions maximize posterior expected immediate reward.
  - Refer to Section 4.2 for details on beta distributions.

- **Undirected Exploration Strategies**
  - E-greedy selects a random arm with probability ε, otherwise a greedy arm.
  - Decaying ε via ε ← αε balances exploration over time.
  - Explore-then-commit explores randomly for k steps before committing to a greedy action.
  - Study on explore-then-commit strategies: [Garivier et al., 2016](https://papers.nips.cc/paper/2016/file/6a650950d88caa05d2c7cc60f93187a4-Paper.pdf).

- **Directed Exploration Strategies**
  - Softmax selects arms probabilistically proportional to exp(λρa), balancing exploration through λ.
  - Quantile exploration selects the arm with the highest α-quantile, promoting optimism under uncertainty.
  - UCB1 uses confidence bounds to balance exploration and exploitation.
  - Posterior sampling (Thompson sampling) selects arms based on sampled payoffs from posterior distributions.
  - For detailed tutorials, see [Russo et al., 2018](https://arxiv.org/abs/1707.02038).

- **Optimal Exploration Strategies**
  - The belief is a 2n-vector of wins and losses, representing the agent’s state.
  - Dynamic programming solves for the optimal policy considering beliefs and horizon.
  - The computation complexity is O(h²ⁿ), limiting practical application for large n or h.
  - Discounted infinite-horizon problems can be solved using Gittins indices.
  - Refer to [Gittins et al., 2011](https://onlinelibrary.wiley.com/doi/book/10.1002/9780470979738) for index calculation methods.

- **Exploration with Multiple States**
  - Reinforcement learning with multiple states requires modeling state transition dynamics.
  - The simulation loop integrates state transitions and reward sampling.
  - Exploration policies must incorporate current state information to select actions.

- **Summary**
  - Exploration-exploitation tradeoff is fundamental in reinforcement learning.
  - Beta distributions provide a Bayesian framework for bandit rewards.
  - Undirected strategies do not use past outcomes to inform exploration.
  - Directed strategies use posterior information to guide exploration more effectively.
  - Optimal policies can be computed via dynamic programming or Gittins indices but at computational cost.

- **Exercises**
  - Exercise 15.1 evaluates parameter settings for e-greedy and explore-then-commit strategies on 3-armed bandits.
  - Exercise 15.2 compares softmax, quantile, and UCB1 strategies on 3-armed bandits and their parameter tuning.
  - Exercise 15.3 provides an example of a multi-armed bandit in online article recommendations.
  - Exercise 15.4 derives posterior probability bounds from a Beta(7, 2) prior after additional pulls.
  - Exercise 15.5 analyzes action selection with e-greedy exploration and decay under sample values.
  - Exercise 15.6 computes posterior beta distributions and softmax action selection probabilities for a four-armed bandit.
  - Exercise 15.7 generalizes the optimal Q-function equation for an arbitrary Beta(α, β) prior.
  - Exercise 15.8 modifies action value functions when one arm's payoff changes.
  - Exercise 15.9 proves the number of belief states grows as O(h²ⁿ) in n-armed bandits with horizon h.
