- Imitation Learning
  - Behavioral Cloning
    - Behavioral cloning treats imitation learning as a supervised learning problem, training a stochastic policy to maximize the likelihood of expert state-action pairs. It can use discrete conditional models or represent policies with neural networks optimized via gradient ascent. Cascading errors from limited expert data coverage often degrade performance over time, as poor state generalization leads to invalid or unseen situations. For further reading, see [Pomerleau, 1991](https://doi.org/10.1162/neco.1991.3.1.88).
  - Dataset Aggregation
    - Dataset aggregation (DAgger) addresses cascading errors by iteratively training a policy on datasets augmented with expert-labeled state-action pairs from the policy's own rollouts. This method incrementally covers state space regions the policy encounters, improving generalization as more expert data accumulates. However, convergence is not guaranteed. Additional insights are found in [Ross et al., 2011](http://proceedings.mlr.press/v15/ross11a/ross11a.pdf).
  - Stochastic Mixing Iterative Learning
    - Stochastic Mixing Iterative Learning (SMILe) incrementally mixes newly trained component policies with prior policies and the expert, decaying expert influence over iterations. This approach retrains a policy on recent expert-labeled data and probabilistically blends in prior policies, improving performance by balancing exploration and expert guidance. For more, see [Ross and Bagnell, 2010](https://proceedings.mlr.press/v15/ross11a/ross11a.pdf).
  - Maximum Margin Inverse Reinforcement Learning
    - Maximum Margin IRL learns a linear reward function by matching expert feature expectations while maximizing the margin by which the expert outperforms previous policies. It models reward features as binary and uses quadratic programming to find weights, iteratively training policies to reduce this margin until convergence. The method produces a mixture policy approximating expert behavior. See [Abbeel and Ng, 2004](https://ai.stanford.edu/~ang/papers/icml04-apprentice.pdf) for details.
  - Maximum Entropy Inverse Reinforcement Learning
    - Maximum Entropy IRL resolves the ambiguity of multiple reward functions by choosing the maximum entropy distribution over trajectories matching expert feature expectations. It models trajectory probabilities proportional to exponentiated rewards normalized over all trajectories. Gradient ascent optimizes reward parameters using discounted state visitation frequencies and policies derived by reinforcement learning. Detailed exposition is in [Ziebart et al., 2008](https://www.aaai.org/Papers/AAAI/2008/AAAI08-164.pdf).
  - Generative Adversarial Imitation Learning
    - Generative Adversarial Imitation Learning (GAIL) simultaneously trains a discriminator to distinguish expert from policy-generated state-action pairs, and a policy to fool the discriminator. This adversarial setup bypasses explicit reward inference by providing a learning signal similar to a reward function. Optimization alternates between discriminator updates and policy improvement via reinforcement learning methods like trust region policy optimization. The foundational work is [Ho and Ermon, 2016](https://arxiv.org/abs/1606.03476).
  - Summary
    - Imitation learning encompasses methods that learn desired behavior solely from expert demonstrations, without explicit reward functions. Behavioral cloning optimizes action likelihoods; dataset aggregation and SMILe iteratively refine policies using expert feedback; inverse reinforcement learning infers rewards by matching expert feature frequencies, either via margin maximization or maximum entropy; and GAIL frames imitation as adversarial training between a discriminator and policy. These methods address challenges of limited data coverage and policy generalization.
