- **18 Imitation Learning**
  - **18.1 Behavioral Cloning**
    - Trains a stochastic policy to maximize the likelihood of expert state-action pairs as a supervised learning problem.
    - Cascading errors emerge as small inaccuracies compound during rollout, causing poor performance in unseen states.
    - Bayesian networks and differentiable function approximators like neural networks can represent policies.
    - [Pomerleau (1991) on neural network training](https://doi.org/10.1162/neco.1991.3.1.88)
  - **18.2 Dataset Aggregation**
    - Iteratively improves a policy by collecting expert-labeled data from states the policy visits and aggregating with previous data.
    - The method, known as DAgger, alternates between policy rollouts and expert corrections to cover likely encountered states.
    - Though effective in practice for mitigating cascading errors, convergence is not guaranteed.
    - [Ross et al. (2011) on DAgger](https://proceedings.mlr.press/v15/ross11a/ross11a.pdf)
  - **18.3 Stochastic Mixing Iterative Learning**
    - Builds policies by stochastically mixing newly-learned policies with prior ones, gradually reducing reliance on the expert.
    - Starts from the expert policy and blends in trained policies weighted by a mixing scalar, allowing smooth policy improvement.
    - Unlike dataset aggregation, SMILe trains new components only on recent data rather than accumulating datasets.
    - [Ross & Bagnell (2010) on SMILe](https://arxiv.org/abs/1011.0686)
  - **18.4 Maximum Margin Inverse Reinforcement Learning**
    - Seeks a reward function that explains expert behavior by matching feature expectations from expert data.
    - Formulates a quadratic program maximizing the margin between expert and previous policiesâ€™ feature expectations under a norm constraint.
    - Produces a mixed policy combining policies found at each iteration to approximate expert behavior.
    - [Abbeel & Ng (2004) on apprenticeship learning](http://ai.stanford.edu/~ang/papers/icml04-apprentice.pdf)
  - **18.5 Maximum Entropy Inverse Reinforcement Learning**
    - Resolves ambiguity in feature matching by selecting the maximum-entropy distribution over trajectories with expected rewards.
    - Defines trajectory probabilities exponentially weighted by cumulative reward with normalization over all trajectories.
    - Objective optimized via gradient ascent requiring discounted state visitation frequencies and optimal policy estimation.
    - [Ziebart et al. (2008) on max-entropy IRL](https://www.aaai.org/Papers/AAAI/2008/AAAI08-292.pdf)
  - **18.6 Generative Adversarial Imitation Learning**
    - Trains a policy and a discriminator adversarially, with the discriminator distinguishing expert from policy state-action pairs.
    - The policy aims to fool the discriminator by generating behavior indistinguishable from expert demonstrations.
    - Optimization alternates between discriminator gradient ascent and policy optimization via trust region policy optimization.
    - [Ho & Ermon (2016) on GAIL](https://arxiv.org/abs/1606.03476)
  - **18.7 Summary**
    - Behavioral cloning fits policies by maximizing likelihood of expert actions but suffers cascading errors.
    - Dataset aggregation and SMILe improve policies interactively by querying experts on encountered states.
    - Inverse reinforcement learning aims to infer reward functions explaining expert data; max-margin and max-entropy methods are key approaches.
    - GAIL leverages adversarial training to imitate expert behavior without explicitly recovering the reward function.
- **18.8 Exercises**
  - Explores practical considerations and extensions of imitation learning methods.
  - Highlights generalized policy parameterization to address data scarcity and the use of cost-sensitive learning.
  - Discusses limitations of max-margin IRL and preferences of maximum entropy principles.
  - Considers reward function surrogate forms in GAIL and trajectory-based discriminators.
  - Provides worked solutions grounded in content from the chapter and related literature.
