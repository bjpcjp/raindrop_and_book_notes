- Exact Solution Methods
  - Markov Decision Processes (MDPs)
    - MDPs model sequential decision-making under uncertainty with stochastic dynamics depending only on the current state and action (Markov assumption). Stationary MDPs have time-invariant transition and reward functions. The chapter introduces the MDP structure and provides an example of aircraft collision avoidance framed as an MDP. For detailed theory on MDPs, see [Puterman, Markov Decision Processes](https://www.wiley.com/en-us/Markov+Decision+Processes%3A+Discrete+Stochastic+Dynamic+Programming-p-9780471727828).
  - Policy Evaluation
    - Policy evaluation computes the expected utility (value function) of a fixed policy, either iteratively using a lookahead update or exactly by solving a system of linear equations. Iterative policy evaluation converges under contraction conditions. This section includes algorithmic implementations. For iterative methods in MDPs, consult [Sutton and Barto, Reinforcement Learning](http://incompleteideas.net/book/the-book.html).
  - Value Function Policies
    - Given a value function, a greedy policy selects actions maximizing the expected return via the lookahead equation. The action-value function Q(s,a) and advantage function A(s,a) provide alternative policy representations. Optimal policies exist as deterministic functions of state. Further exploration is found in [Puterman, Markov Decision Processes](https://www.wiley.com/en-us/Markov+Decision+Processes%3A+Discrete+Stochastic+Dynamic+Programming-p-9780471727828).
  - Policy Iteration
    - Policy iteration alternates policy evaluation and improvement steps to converge to an optimal policy in finite iterations due to the finite policy space. Modified policy iteration trades off between full evaluation and improvement. Algorithmic details and convergence proofs are provided. For advanced understanding, see [Puterman, Markov Decision Processes](https://www.wiley.com/en-us/Markov+Decision+Processes%3A+Discrete+Stochastic+Dynamic+Programming-p-9780471727828).
  - Value Iteration
    - Value iteration improves the value function directly via Bellman backups, guaranteed to converge to the optimal value function by contraction mapping. The discount factor γ heavily affects convergence speed and error bounds. Early stopping can be based on the Bellman residual. The section includes examples and algorithms. See [Bellman, Dynamic Programming](https://press.princeton.edu/books/paperback/9780691047674/dynamic-programming) for foundational theory.
  - Asynchronous Value Iteration
    - Asynchronous or Gauss-Seidel value iteration updates subsets of states in place per iteration, saving computation and sometimes accelerating convergence depending on state-update order. Convergence is guaranteed if each state updates infinitely often. Practical benefits and ordering effects are demonstrated. Further details in [Bertsekas and Tsitsiklis, Neuro-Dynamic Programming](https://web.mit.edu/dimitrib/www/NDP-book.html).
  - Linear Program Formulation
    - Finding the optimal policy in an MDP can be framed as a linear program minimizing state utilities under Bellman inequality constraints. This formulation has a polynomial-time complexity with variables equal to the number of states and constraints equal to states times actions. Though asymptotically efficient, it is often slower in practice than value iteration. Implementation details provided. For linear programming in MDPs, see [Vanderbei, Linear Programming](https://link.springer.com/book/10.1007/978-1-4419-6366-3).
  - Linear Systems with Quadratic Reward
    - Extends MDPs to continuous vector states/actions with linear transitions plus additive Gaussian noise and quadratic (negative definite) rewards, known as linear–quadratic regulators (LQR). The Bellman equation transforms into matrix Riccati equations with closed-form solutions for finite horizons. This provides exact optimal controls under these conditions. The section includes a detailed solution derivation and example. Consult [Bertsekas, Dynamic Programming and Optimal Control](http://web.mit.edu/dimitrib/www/dpchapter.html) for in-depth treatment.
