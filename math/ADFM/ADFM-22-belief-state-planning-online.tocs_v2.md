- Online Belief State Planning  
  - Lookahead with Rollouts  
    The section introduces lookahead with rollouts as an online method applicable directly to partially observed problems. It uses a generative model to sample next states, enabling handling of high-dimensional state and observation spaces. The method builds upon a function previously used in Algorithm 21.11 for sampling transitions and observations. For more details on rollout methods in POMDPs, see the survey by [Ross et al., 2008](https://jair.org/index.php/jair/article/view/10824).  
  - Forward Search  
    Forward search applies a recursive one-step lookahead in belief space for POMDPs, branching on actions and observations. It defines the Q-value recursively to a certain depth d, returning an approximate value when the depth limit is reached. While computational complexity grows exponentially in the depth and branching factors, domain knowledge can reduce effective branching by limiting action or observation consideration. Example implementations include machine replacement and crying baby problems. For foundational concepts, consult [Puterman, 1994](https://bookstore.siam.org/mpr/).  
  - Branch and Bound  
    Branch and bound extends to POMDPs by pruning parts of the forward search tree using upper and lower bounds on the value function. The method reuses algorithms from fully observable cases and relies on good bounds like the fast informed bound (upper) and point-based value iteration (lower) for efficiency. The resulting policy matches forward search if the bounds are valid. See the application in the crying baby problem. For further exploration, view [Bertsekas, 2012](http://web.mit.edu/dimitrib/www/dpchapter.html).  
  - Sparse Sampling  
    Sparse sampling approximates forward search by sampling a fixed number m of observations per action, avoiding summation over the full observation space. This reduces complexity from O(|A|^d |O|^d) to O(|A|^d m^d), making deeper searches more tractable. The method uses sampled observations and corresponding rewards to estimate Q-values recursively. Details align with those introduced for MDPs in earlier sections. See [Kearns, Mansour, and Ng, 2002](https://papers.nips.cc/paper_files/paper/2002/file/fb3d424419d044a47aee49fc3c31e152-Paper.pdf) for sparse sampling fundamentals.  
  - Monte Carlo Tree Search  
    Monte Carlo tree search (MCTS) for POMDPs operates over histories (action-observation sequences) instead of states, associating value estimates and counts with history-action pairs. The method builds a search tree alternating between action and observation nodes and uses an exploration-exploitation strategy similar to UCT, with an exploration constant c guiding exploration. MCTS is anytime and converges to the optimal action given sufficient samples. Rolling out from belief states incorporating prior knowledge improves performance. The POMCP algorithm extends these ideas. For in-depth understanding, refer to [Silver and Veness, 2010](https://papers.nips.cc/paper_files/paper/2010/file/0062b15f6e0351d19aebd9e0b42f2e58-Paper.pdf).  
  - Determinized Sparse Tree Search  
    Determinized sparse tree search reduces sampling by determinizing observations through a particle belief representation with fixed scenarios. It uses a determinizing matrix Î¦ to generate deterministic successors for each particle at each depth, collapsing uncertainty over observations and significantly shrinking the search tree complexity to O(|A|^d m). This method provides a sparse approximation of the true belief tree, maintaining a fixed set of scenarios for planning. See [Ye et al., 2017](https://jair.org/index.php/jair/article/view/11161) for details on the DESPOT algorithm.  
  - Gap Heuristic Search  
    Gap heuristic search guides belief exploration by focusing on beliefs with the largest difference (gap) between upper and lower bounds on value estimates. It iteratively improves bounds using heuristic selection of action-observation pairs and halts when the gap falls below a threshold or maximum depth is reached. Initial upper bounds often use best-action best-state heuristics, while rollout policies estimate lower bounds. This approach achieves computational savings by pruning and targeted exploration. For more, consult the description in point-based POMDP literature such as [Pineau et al., 2006](https://pure.mpg.de/rest/items/item_1651755_5/component/file_1651753/content).
