- **Collaborative Agents**
  - **Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs)**
    - Dec-POMDPs model multiple agents collaboratively maximizing a single shared reward under local partial observability.
    - Each agent selects actions based on local observation histories without full state knowledge.
    - The predator-prey problem demonstrates Dec-POMDP application with independent predator movements and a randomly moving prey.
    - See F. A. Oliehoek and C. Amato's [A Concise Introduction to Decentralized POMDPs](https://link.springer.com/book/10.1007/978-3-319-29383-2) for a comprehensive introduction.
  - **Subclasses of Dec-POMDPs**
    - Subclasses differ by the number of agents, observability type (full, partial, joint full), and communication capability.
    - Joint full observability implies combined agent observations reveal the true state; such Dec-POMDPs are Dec-MDPs.
    - Factored Dec-POMDPs allow state, transition, observation, and reward independence decompositions to reduce complexity.
    - ND-POMDPs use coordination graphs to structure rewards based on agent interactions in the hypergraph.
    - MMDPs and MPOMDPs result from free communication and full sharing of observations and actions among agents.
  - **Dynamic Programming**
    - Applies Bellman equations iteratively with pruning of dominated policies to find optimal joint policies.
    - Leverages previous POMG algorithms by aligning individual rewards with shared Dec-POMDP rewards.
  - **Iterated Best Response**
    - Iteratively updates a single agent’s policy assuming other agents’ policies fixed to find joint equilibrium policies.
    - Fast convergence is typical due to shared rewards but may find suboptimal equilibria.
    - Also known as joint equilibrium-based search for policies (JESP).
  - **Heuristic Search**
    - Explores a fixed number of joint policies guided by heuristic estimates of future beliefs and utilities.
    - Controls exponential policy growth by selecting best joint conditional plans at each iteration.
    - Memory bounded dynamic programming (MBDP) is a known variant.
  - **Nonlinear Programming**
    - Formulates fixed-size joint controller policy optimization as a nonlinear program generalizing single-agent POMDP NLPs.
    - Optimizes policy node action and observation-update probabilities under Bellman consistency constraints.
    - Employs solvers like Ipopt to optimize value functions given initial beliefs.
    - Reference: Amato, Bernstein, and Zilberstein’s work on optimizing fixed-size stochastic controllers.
  - **Exercises**
    - Highlight practical advantages of Dec-POMDPs in cooperative applications like robotics.
    - Explain joint full observability implications versus individual state knowledge.
    - Prove that transition, observation, and reward independence enable decomposition into separate MDPs.
    - Discuss leveraging MMDP or MPOMDP solutions as heuristics in Dec-POMDP policy search.
    - Describe computing best response controllers via nonlinear programming for use in iterated best response.
