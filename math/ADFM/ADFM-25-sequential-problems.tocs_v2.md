- **25 Sequential Problems**
  - **25.1 Markov Games**
    - Markov games extend simple games by including a shared state and state transitions.
    - Transitions depend on joint actions, and each agent receives individual rewards based on state and actions.
    - The joint policy defines probability distributions over joint actions given current states.
    - Markov games generalize multi-agent decision processes and can represent problems like traffic routing.
    - Refer to L. S. Shapley’s seminal paper [“Stochastic Games” (1953)](https://www.pnas.org/doi/10.1073/pnas.39.10.1095) for foundational theory.
  - **25.2 Response Models**
    - Response policies maximize expected utility given fixed policies of other agents.
    - Best response reduces to solving an MDP conditioned on other agents’ policies.
    - Softmax response introduces stochasticity weighted by a precision parameter λ.
    - Algorithms provide methods for computing best and softmax responses in Markov games.
    - For hierarchical softmax, see section 24.6 and algorithm 24.9 for algorithmic details.
  - **25.3 Nash Equilibrium**
    - Nash equilibrium extends to Markov games where all agents best-respond to one another.
    - Every finite Markov game with discounted infinite horizon has at least one Nash equilibrium.
    - Finding equilibrium involves solving a nonlinear optimization problem with utility and policy constraints.
    - The equilibrium requires policies to be valid probability distributions over actions in every state.
    - The work by A. M. Fink [“Equilibrium in a Stochastic n-Person Game” (1964)](https://pubmed.ncbi.nlm.nih.gov/17757698/) provides proof of equilibrium existence.
  - **25.4 Opponent Modeling**
    - Opponent modeling maintains maximum-likelihood counts of other agents’ state-action frequencies.
    - Tracking counts over states and actions allows estimation of opponent policies given known transition functions.
    - Utilities require computation of best responses to dynamically updated opponent models, generally via induced MDPs.
    - Frequent value iteration updates reduce utility estimation error but increase computational cost.
    - The approach generalizes fictitious play and is detailed in works by Uther and Veloso [“Adversarial Reinforcement Learning” (1997)](https://www.cs.cmu.edu/~mmv/papers/CMU-CS-03-107.pdf).
  - **25.5 Nash Q-Learning**
    - Nash Q-learning generalizes Q-learning to multiagent settings by maintaining joint action value functions.
    - Updates are performed using Nash equilibria computed from simple games constructed at successor states.
    - The update uses a learning rate inversely proportional to visit counts and employs e-greedy exploration.
    - This method models reactions of other agents through equilibrium computations.
    - Foundational reading: J. Hu and M. P. Wellman’s [“Nash Q-Learning for General-Sum Stochastic Games” (2003)](http://jmlr.org/papers/volume4/hu03a/hu03a.pdf).
  - **25.6 Summary**
    - Markov games combine sequential decision processes with multiple agents having individual rewards.
    - Nash equilibria extend to Markov games but require joint policies over all states and agents.
    - Opponent modeling leverages knowledge of transition functions to estimate opponents’ policies.
    - Nash Q-learning incorporates equilibrium computations into Q-learning for multiagent learning.
    - These methods provide foundational frameworks for multiagent reinforcement learning and strategic interaction.
  - **25.7 Exercises**
    - Exercises cover formalizing Markov games as generalizations of MDPs and simple games.
    - Investigations include the necessity of stochastic policies in equilibria and alternative policy types.
    - Discussions address computation methods for utilities in opponent modeling with respective pros and cons.
    - Solutions provide rigorous explanations of best-response determinism and policy categories.
    - The exercise section supplements learning with theoretical and practical problem formulations.
