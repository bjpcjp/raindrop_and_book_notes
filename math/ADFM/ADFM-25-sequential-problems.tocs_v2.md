- 25 Sequential Problems
  - 25.1 Markov Games
    - Markov games generalize simple games by including a shared state space where transitions depend on joint actions of multiple agents. Each agent receives individualized rewards based on the current state and joint actions. This framework extends multi-agent reinforcement learning by accounting for stochastic state transitions, as initially studied by Shapley in 1953. Further reading: [Shapley, “Stochastic Games,” PNAS, 1953](https://www.pnas.org/content/39/10/1095).
  - 25.2 Response Models
    - This section generalizes response models such as best response and softmax response to Markov games, where agent policies depend on the current state and fixed policies of other agents. Best response policies reformulate the multi-agent interaction as an MDP by fixing other agents’ strategies, whereas softmax responses introduce stochasticity controlled by a precision parameter λ. Both require solving an MDP with modified reward and transition functions. Further reading: [Littman, "Markov Games," ICML 1994](http://www.cs.brown.edu/~mlittman/pubs/mg-icml94.pdf).
  - 25.3 Nash Equilibrium
    - The Nash equilibrium concept extends to Markov games as stationary Markov perfect equilibria, where no agent gains by unilateral deviation given others’ stationary policies. The equilibrium can be found by solving a nonlinear optimization minimizing sum of utility deviations constrained by valid policy distributions over all states and joint actions. Existence is guaranteed for finite discounted Markov games. Further reading: [Fink, "Equilibrium in a Stochastic n-Person Game," Hiroshima University, 1964](https://doi.org/10.3792/pieees1963.28.1_89).
  - 25.4 Opponent Modeling
    - Opponent modeling adapts fictitious play to Markov games by maintaining maximum likelihood estimates of other agents’ state-dependent policies using counts of observed actions per state. Knowing the transition function enables model updates via asynchronous value iteration. This method iteratively updates policies and utilities based on joint interactions and state transitions, providing a practical learning approach for multi-agent sequential domains. Further reading: [Uther & Veloso, “Adversarial Reinforcement Learning,” CMU Tech Report, 1997](https://www.cs.cmu.edu/afs/cs/project/jair/pub/volume2/uther98a.pdf).
  - 25.5 Nash Q-Learning
    - Nash Q-learning extends Q-learning to multi-agent Markov games by maintaining joint state-action value estimates and updating them based on observed transitions and Nash equilibria computed over next states’ Q-values. It employs e-greedy exploration with learning rates inversely proportional to visitation counts, dynamically adapting to others’ changing policies. This algorithm leverages equilibrium computation to handle general-sum stochastic games. Further reading: [Hu & Wellman, “Nash Q-Learning,” JMLR, 2003](https://jmlr.org/papers/volume4/hu03a/hu03a.pdf).
  - 25.6 Summary
    - Markov games extend MDPs and simple games to multi-agent sequential decision making, involving individualized rewards and complex joint policies. Nash equilibria generalize to these settings but require solving nonlinear optimization over joint state-action policies. Learning-based approaches include opponent modeling and Nash Q-learning that incorporate known transition functions or joint value function updates respectively. These frameworks enable multi-agent reinforcement learning under uncertainty and strategic interaction. Further reading: [Busoniu et al., “Multi-agent Reinforcement Learning,” IEEE Trans. Systems, 2008](https://ieeexplore.ieee.org/document/4576109).
  - 25.7 Exercises
    - Exercises reinforce understanding of Markov games’ generalization of MDPs and simple games, existence and characterization of best and stochastic responses, policy classifications including behavioral and non-stationary policies, and methods for computing utilities in opponent modeling. They consolidate theoretical concepts and practical implications concerning policy forms and computational trade-offs. Further reading: [Shoham & Leyton-Brown, "Multiagent Systems," 2009](https://mitpress.mit.edu/books/multiagent-systems).
