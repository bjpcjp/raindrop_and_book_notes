- Exact Belief State Planning  
  - Belief-State Markov Decision Processes  
    - Any POMDP can be represented as a belief-state MDP where the state space consists of all beliefs and the action space matches the POMDP’s actions. The belief-state transition function is defined through summations over observations and states and involves a deterministic belief update. Rewards depend on beliefs and actions as the expected value of state-based rewards. This concept is detailed in K. J. Åström's work on optimal control with incomplete state information.  
  - Conditional Plans  
    - Policies in POMDPs can be represented as conditional plans structured as trees, where nodes are belief states annotated with actions, and edges correspond to observations. Execution involves progressing through the tree based on observations and taking corresponding actions. The utility of a conditional plan can be computed recursively starting from a state or over beliefs by averaging across states. This method relates closely to planning under uncertainty in AI as described by Kaelbling et al.  
  - Alpha Vectors  
    - Alpha vectors represent the expected utility of a conditional plan from each state, forming a piecewise-linear and convex value function over belief space. Policies can thus be represented as sets of alpha vectors paired with actions, selecting the action associated with the alpha vector that maximizes expected value at a given belief. One-step lookahead techniques can compute optimal actions using these vectors efficiently, avoiding enumerating all conditional plans. For foundations, see Cassandra et al.'s work on alpha-vector methods.  
  - Pruning  
    - Pruning techniques eliminate alpha vectors and conditional plans that are never optimal for any belief, thus improving computational efficiency. This is done by solving a linear program that identifies beliefs maximizing the utility gap of a candidate alpha vector over others. Vectors with non-positive gaps are dominated and can be removed. Algorithms iteratively update dominating sets until no further pruning is possible. Related pruning methods are discussed extensively in [Incremental Pruning literature](https://doi.org/10.1109/95.589464).  
  - Value Iteration  
    - Value iteration for POMDPs iteratively constructs conditional plans of increasing horizon length, pruning suboptimal plans at each step. The expansion step generates new plans by combining a new initial action with subplans corresponding to observations, using alpha vectors to compute expected utilities efficiently. This process converges to optimal finite-horizon policies and mitigates the combinatorial explosion in plan enumeration. Detailed treatments are available in Cassandra, Littman, and Zhang's work on POMDP value iteration algorithms.  
  - Linear Policies  
    - In problems with linear-Gaussian dynamics and quadratic reward functions, the belief state is Gaussian, and the optimal policy can be computed exactly using linear-quadratic-Gaussian (LQG) control methods. The belief mean serves as a sufficient statistic for control, updated by a linear Gaussian filter, and the optimal action arises from multiplying the belief mean by a policy matrix. This connects classical LQG theory to POMDP planning under continuous uncertainty, exemplified in control texts like [Linear-Quadratic-Gaussian Control](https://ieeexplore.ieee.org/document/509045).
