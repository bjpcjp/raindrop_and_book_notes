- **Policy Gradient Optimization**
  - **Gradient Ascent Update**
    - Gradient ascent iteratively improves policy parameters by stepping in the direction of the estimated policy gradient.  
    - Step size α controls the update magnitude, with large steps risking overshoot.  
    - Gradient scaling and clipping maintain gradient magnitude stability, limiting L2-norm to 1 or clamping component values respectively.  
    - [Algorithms for Optimization](https://mitpress.mit.edu/books/algorithms-optimization) covers these methods in detail.  
  - **Restricted Gradient Update**
    - Introduces a constrained optimization to limit step size via a quadratic form involving the identity matrix.  
    - Uses a first-order Taylor expansion of expected utility to approximate objective improvement.  
    - Update steps solve a maximization problem subject to an L2 norm constraint on parameter changes.  
  - **Natural Gradient Update**
    - Modifies the gradient ascent direction to account for variable sensitivity in parameter space using the Fisher information matrix.  
    - Constrains policy updates using a second-order Taylor approximation of the Kullback-Leibler divergence.  
    - Produces elliptical trust regions allowing anisotropic step sizes in parameter updates.  
    - Refer to S. Amari’s "Natural Gradient Works Efficiently in Learning" for foundational theory.  
  - **Trust Region Update**
    - Augments natural gradient updates with a line search along the segment between current and candidate policies.  
    - The line search maintains improvement of a surrogate objective while satisfying divergence constraints without additional simulations.  
    - Uses importance sampling to estimate surrogate objectives based on existing rollout data.  
    - Detailed in Schulman et al., "Trust Region Policy Optimization," ICML 2015.  
  - **Clamped Surrogate Objective**
    - Employs a pessimistic lower bound on the surrogate objective by clamping the probability ratio within a small range around 1 ± ε.  
    - Prevents overly optimistic policy updates and removes the need for explicit trust-region constraints or line search.  
    - Allows multiple gradient updates using the same batch of sampled trajectories.  
    - Related to Proximal Policy Optimization introduced by Schulman et al. (2017).  
  - **Summary**
    - Policy gradient optimization methods iteratively improve policies with gradients estimated from experience.  
    - Robustness techniques include gradient scaling, clipping, natural gradients, and trust region methods.  
    - Natural gradient methods use Fisher matrix constraints to adapt step sizes.  
    - Trust region policy optimization adds a line search to ensure policy improvement.  
    - Clamped surrogate objectives provide similar benefits without requiring line search or constraints.
