- Neural Representations  
  - D.1 Neural Networks  
    A neural network is a differentiable parametric function that maps inputs to outputs and is trained by minimizing a scalar loss function using gradient-based optimization like stochastic gradient descent. The differentiability enables iterative parameter updates using gradients of the loss function with respect to network parameters. Detailed treatments of neural network training can be found in the textbook [Deep Learning by Goodfellow et al.](https://www.deeplearningbook.org/).  
  - D.2 Feedforward Networks  
    Feedforward neural networks pass inputs through multiple layers composed of affine transformations followed by nonlinear activation functions, preventing collapse into a single affine transform. Nonlinearities like ReLU, sigmoid, or softmax enable modeling arbitrary nonlinear functions, critical for tasks like classification or regression. For comprehensive coverage, consult [An Introduction to Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/).  
  - D.3 Parameter Regularization  
    Parameter (weight) regularization adds a penalty term, typically an L2 norm of parameters scaled by a small positive constant, to the loss function to reduce overfitting and prevent parameter explosion. This encourages smaller parameter values and better generalization when multiple parameter configurations produce similar training loss. Classical regularization methods are reviewed in [Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/).  
  - D.4 Convolutional Neural Networks  
    Convolutional layers reduce parameter complexity and exploit translation invariance by applying small filters sliding over inputs such as images, preserving local spatial relationships. Convolutional architectures thus efficiently process high-dimensional structured inputs by recursively extracting local features through successive layers. For details, see the foundational paper [Gradient-Based Learning Applied to Document Recognition by LeCun et al.](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf).  
  - D.5 Recurrent Networks  
    Recurrent neural networks (RNNs) handle temporal or sequential data by maintaining internal recurrent states (“memory”) that propagate information across time steps, enabling modeling of variable-length sequences. Despite challenges like vanishing and exploding gradients during training, architectures such as LSTMs and GRUs use gating mechanisms to mitigate these problems. A seminal introduction is provided by [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/).  
  - D.6 Autoencoder Networks  
    Autoencoders train neural networks to produce a compressed low-dimensional representation (encoding) of high-dimensional inputs by minimizing reconstruction loss, supporting unsupervised representation learning. Variational autoencoders extend this by encoding inputs as probability distributions, optimizing both reconstruction accuracy and closeness of encoding distributions to a unit Gaussian using KL divergence, enabling smooth latent spaces and generative capabilities. For more, see [Auto-Encoding Variational Bayes by Kingma and Welling](https://arxiv.org/abs/1312.6114).  
  - D.7 Adversarial Networks  
    Generative adversarial networks (GANs) train two competing neural networks: a generator producing outputs and a discriminator distinguishing real versus generated data, forcing the generator to produce outputs indistinguishable from training samples. This adversarial process discourages unrealistic outputs and overfitting without requiring handcrafted features. The original GAN framework is described in [Generative Adversarial Nets by Goodfellow et al.](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf).
