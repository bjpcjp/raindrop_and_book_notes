- **Policy Search**
  - **Approximate Policy Evaluation**
    - Expected discounted return \( U(\pi) \) can be computed iteratively or via matrix methods in small discrete state spaces.
    - For large or continuous state spaces, \( U(\pi) \) is approximated by sampling trajectories.
    - Monte Carlo policy evaluation approximates \( U(\pi) \) by averaging returns from multiple rollouts.
    - Increasing the number of rollouts reduces variance in the utility estimate.
    - Further reading: [Monte Carlo Methods](https://en.wikipedia.org/wiki/Monte_Carlo_method)
  - **Local Search**
    - Local search incrementally improves policy parameters through small, local changes.
    - The Hooke-Jeeves method tests moves along coordinate directions and adapts step size based on improvements.
    - The method continues until the step size falls below a threshold, indicating convergence.
    - Local search may get trapped in local optima and depends on initial parameters.
    - Further reading: [Hooke-Jeeves Method](https://en.wikipedia.org/wiki/Hooke%E2%80%93Jeeves_pattern_search)
  - **Genetic Algorithms**
    - Genetic algorithms maintain a population of policy parameterizations evaluated in parallel.
    - Elite samples are selected to generate the next population by perturbing with Gaussian noise.
    - This approach draws inspiration from biological evolution to avoid local optima.
    - Evaluations can be computationally expensive but parallelizable.
    - Further reading: [Genetic Algorithms](https://en.wikipedia.org/wiki/Genetic_algorithm)
  - **Cross Entropy Method**
    - The method optimizes a search distribution over policy parameters by iteratively refitting to elite samples.
    - Search distributions, typically Gaussian, are updated via maximum likelihood estimation to focus on better policies.
    - It samples from the current distribution each iteration and selects the top-performing samples to update parameters.
    - Stopping criteria include fixed iterations or convergence of the distribution.
    - Further reading: [Cross Entropy Method for Optimization](https://link.springer.com/chapter/10.1007/3-540-34387-2_41)
  - **Evolution Strategies**
    - Evolution strategies update a parameterized search distribution by estimating gradients of expected utility.
    - The gradient is computed as the expectation of utility-weighted log likelihood derivative of the search distribution.
    - Rank shaping assigns weights to samples based on relative performance to reduce gradient variance.
    - This gradient ascent approach leverages analytic likelihood gradient formulas for distributions like Gaussian.
    - Further reading: [Natural Evolution Strategies](https://jmlr.org/papers/v15/wierstra14a.html)
  - **Isotropic Evolutionary Strategies**
    - Assumes the search distribution is an isotropic Gaussian with covariance \( \sigma^2 I \).
    - The expected utility gradient reduces to the expectation of utility times standardized perturbations.
    - Mirrored sampling (using pairs of perturbations and their negatives) reduces variance in gradient estimation.
    - Empirical results show mirrored sampling accelerates and stabilizes learning progress.
    - Further reading: [Mirrored Sampling](https://link.springer.com/chapter/10.1007/978-3-642-15844-5_19)
  - **Summary**
    - Monte Carlo evaluation estimates policy utility from rollouts.
    - Local search uses small incremental improvements.
    - Genetic algorithms explore using populations and recombination.
    - Cross entropy method fits distributions to elite policies.
    - Evolution strategies incorporate gradient information to update search distributions.
    - Isotropic strategies specialize evolution strategies with spherical Gaussian assumptions.
