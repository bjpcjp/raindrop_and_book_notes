- Policy Search
  - Approximate Policy Evaluation
    - Monte Carlo policy evaluation estimates the expected discounted return \(U(\pi)\) by sampling trajectories from an initial state distribution and averaging their returns. This method is stochastic, and its variance decreases as the number of rollouts increases. The approach applies when direct matrix or iterative computation is infeasible due to large or continuous state spaces. For further reading, see [Monte Carlo Methods](https://web.stanford.edu/class/cs221/lecture_notes/lecture10.pdf).
  - Local Search
    - Local search optimizes policy parameters by iteratively exploring neighboring points using methods like Hooke-Jeeves, which takes steps of fixed size in each coordinate direction and adapts step size based on improvement success. This approach is effective for low-dimensional vectors but can get stuck in local optima. The original Hooke-Jeeves method is described in [Direct Search Solution paper](https://dl.acm.org/doi/10.1145/321062.321069).
  - Genetic Algorithms
    - Genetic algorithms maintain a population of candidate policy parameters, evaluating their utility and evolving the population by selecting elite samples and adding Gaussian perturbations. This method supports parallel evaluation and can avoid local optima by exploring diverse solutions. It is especially useful when policy parameters represent complex neural networks. A foundational resource is Goldbergâ€™s [Genetic Algorithms book](https://mitpress.mit.edu/books/genetic-algorithms-search-optimization-and-machine-learning).
  - Cross Entropy Method
    - The cross entropy method updates a parameterized search distribution over policy parameters by sampling, selecting elite samples, and refitting the distribution iteratively. Typically using Gaussian distributions, it leverages maximum likelihood estimation to refine the distribution parameters. This method balances exploration and exploitation and can converge to optima by focusing sampling. See the original presentation in [The Cross Entropy Method for Fast Policy Search](https://icml.cc/Conferences/2003/proceedings/papers/281.pdf).
  - Evolution Strategies
    - Evolution strategies estimate the gradient of the expected utility of a search distribution over parameters and update distribution parameters accordingly. The gradient is computed using the likelihood ratio trick based on sampled utilities and their gradients, often incorporating rank shaping weights to reduce outlier impact. This approach generalizes gradient ascent to black-box optimization problems. For detailed algorithmic background, refer to [Natural Evolution Strategies](http://jmlr.org/papers/volume15/wierstra14a/wierstra14a.pdf).
  - Isotropic Evolutionary Strategies
    - Isotropic evolutionary strategies assume a spherical Gaussian search distribution (covariance \(\sigma^2 I\)) and use mirrored sampling to reduce gradient estimation variance. The gradient reduces to a simple form involving the perturbation vectors, allowing efficient updates of the mean parameter. Mirrored sampling stabilizes and accelerates learning. More on mirrored sampling is in [Mirrored Sampling for Evolution Strategies](https://link.springer.com/chapter/10.1007/978-3-642-15844-5_64).
