- **4 Parameter Learning**
  - **4.1 Maximum Likelihood Parameter Learning**
    - Maximum likelihood parameter learning identifies parameters that maximize the likelihood of observed data given a probabilistic model.
    - It assumes independent and identically distributed samples, enabling likelihood factorization into products over individual data points.
    - Log-likelihood maximization is preferred for numerical stability and analytic tractability when possible.
    - Common distributions covered include categorical, Gaussian, and Bayesian networks.
    - For further theoretical background, see [Machine Learning: A Probabilistic Perspective](https://mitpress.mit.edu/books/machine-learning).
  - **4.1.1 Maximum Likelihood Estimates for Categorical Distributions**
    - Parameter estimation for categorical distributions involves counting observations for each category.
    - The binomial distribution models the likelihood for binary outcomes like flight collision events.
    - Maximum likelihood estimates correspond to relative frequencies in the data.
    - For multi-category variables, estimated parameters are normalized counts over classes.
  - **4.1.2 Maximum Likelihood Estimates for Gaussian Distributions**
    - The log-likelihood for Gaussian distributions depends on mean and variance parameters.
    - Setting derivatives of the log-likelihood to zero yields closed-form maximum likelihood estimates for mean and variance.
    - Estimates are respectively the sample mean and sample variance of observed data.
  - **4.1.3 Maximum Likelihood Estimates for Bayesian Networks**
    - Parameters of discrete Bayesian networks factor according to their conditional distributions.
    - Counts of variable-parent instantiations are extracted from data to estimate parameters.
    - The maximum likelihood estimate for each conditional probability is the normalized count of occurrences.
    - Practical implementation involves tabulating counts and normalizing rows of conditional probability tables.
  - **4.2 Bayesian Parameter Learning**
    - Bayesian learning treats parameters as random variables, updating priors with observed data to obtain posterior distributions.
    - This approach quantifies uncertainty and can prevent unreasonable point estimates from limited data.
    - Posterior expected values and maximum a posteriori (MAP) estimates provide alternative parameter estimates.
    - Bayesian parameter learning can be viewed as inference in a Bayesian network with parameters as latent variables.
  - **4.2.1 Bayesian Learning for Binary Distributions**
    - Beta distributions serve as conjugate priors for binomial likelihoods, enabling analytic posterior updates.
    - Posterior parameters are sums of prior "pseudocounts" and observed counts.
    - The Beta distribution's mean and mode have closed-form expressions for parameter estimation.
    - Prior choice influences posterior especially with small data; uniform Beta(1,1) is a common default.
  - **4.2.2 Bayesian Learning for Categorical Distributions**
    - The Dirichlet distribution generalizes the Beta distribution to multiple categories.
    - Dirichlet parameters represent pseudocounts and admit analytic posterior updates adding observed counts.
    - Posterior means and modes follow explicit formulae in terms of Dirichlet parameters.
    - Dirichlet priors enable straightforward Bayesian parameter learning for multinomial data.
    - Visualization of Dirichlet densities provides intuition about concentration and mode behaviors.
  - **4.2.3 Bayesian Learning for Bayesian Networks**
    - Bayesian network parameters have factorized Dirichlet priors over conditional probability tables.
    - Posterior Dirichlet parameters are computed by adding observed counts to prior pseudocounts.
    - Algorithms implement generation of uniform Dirichlet priors compatible with network structures.
    - The posterior update process is an arithmetic addition between data-derived counts and priors.
  - **4.3 Nonparametric Learning**
    - Nonparametric methods, such as kernel density estimation, represent distributions without fixed finite parameters.
    - Kernel density estimation smooths observed data using kernels like Gaussian functions.
    - The kernel bandwidth controls smoothness; larger bandwidth yields smoother densities.
    - Bayesian techniques exist for bandwidth selection to optimize model fit.
    - Reference: [Kernel Density Estimation (Wikipedia)](https://en.wikipedia.org/wiki/Kernel_density_estimation).
  - **4.4 Learning with Missing Data**
    - Missing data challenges learning, making naive discarding of incomplete samples inefficient.
    - Parameter estimates from missing data require marginalization over unobserved entries, which is often computationally expensive.
    - Approaches include data imputation and expectation-maximization (EM) algorithms.
    - Missingness is assumed "missing at random," where missingness is independent of unobserved values conditioned on observed data.
    - For deeper coverage, see [Handbook of Missing Data Methodology](https://www.crcpress.com/Handbook-of-Missing-Data-Methodology/Molenberghs-Fitzmaurice-Kenward-Tsiatis-Verbecke/p/book/9781466563939).
  - **4.4.1 Data Imputation**
    - Imputation fills missing entries, approximating the complete data distribution mode or mean.
    - Marginal mode imputation replaces missing values with the most frequent observed value for categorical data.
    - For continuous data, marginal means or modes of fitted distributions are used.
    - Nearest-neighbor imputation leverages similarity in observed variables to select imputed values.
    - Probabilistic imputation can use inference algorithms to infer conditional distributions over missing variables for more accurate imputations.
  - **4.4.2 Expectation-Maximization**
    - EM iteratively improves parameter estimates by alternating between imputing missing data (E-step) and maximizing likelihood given imputations (M-step).
    - The E-step uses current parameters to infer distributions over missing entries using model-based inference or sampling.
    - The M-step maximizes likelihood or posterior using completed or weighted data.
    - EM converges to local optima and is commonly initialized with multiple restarts.
    - EM extends naturally to latent variable models like Gaussian mixture models.
  - **4.5 Summary**
    - Parameter learning estimates model parameters from data via maximum likelihood or Bayesian methods.
    - Beta and Dirichlet distributions serve as convenient conjugate priors enabling analytic Bayesian updates.
    - Nonparametric learning methods adjust model complexity with data through kernels or other representations.
    - Missing data complicates learning; techniques include imputation and EM for principled handling.
  - **4.6 Exercises**
    - Exercises cover maximum likelihood estimation for Laplace distributions and parameter estimation for Bayesian networks.
    - Additional exercises involve posterior inference with Beta priors, various imputation methods, and Gaussian posterior computations.
    - Solutions provide closed-form formulae and algorithmic reasoning.
    - Exercises reinforce theoretical concepts and practical skills in parameter learning.
