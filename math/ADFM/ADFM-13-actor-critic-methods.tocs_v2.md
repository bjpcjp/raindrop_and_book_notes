- Actor-Critic Methods  
  - Actor-Critic  
    The actor-critic method optimizes a parameterized policy using gradient ascent and relies on a critic estimating value functions Uφ(s), Qφ(s, a), or Aφ(s, a). The actor updates the policy parameters θ to maximize expected returns via gradients involving the advantage function, while the critic minimizes the temporal difference residual loss to approximate the value function. Stability concerns arise due to interdependencies of θ and φ estimates, often addressed by differing update frequencies. Relevant resource: [Actor-Critic Algorithms — OpenAI Spinning Up](https://spinningup.openai.com/en/latest/algorithms/vpg.html#actor-critic).
  - Generalized Advantage Estimation  
    Generalized Advantage Estimation (GAE) balances bias and variance in advantage function estimation by combining multiple-step temporal difference residuals using an exponential weighting parameter λ ∈ [0,1]. GAE interpolates between high-bias, low-variance temporal difference and low-bias, high-variance full rollout estimates, improving sample efficiency. It computes policy gradients based on these weighted advantage estimates. Further reading: [Generalized Advantage Estimation (Schulman et al. 2016)](https://arxiv.org/abs/1506.02438).
  - Deterministic Policy Gradient  
    The deterministic policy gradient method optimizes continuous action policies producing deterministic outputs, using a critic parameterized action value function Qφ(s, a). The critic minimizes Bellman residuals, while the actor updates policy parameters θ using gradients derived via the chain rule combining ∇θ πθ(s) and ∇_a Qφ(s, a). Exploration often involves adding Gaussian noise to actions. Stability is enhanced by experience replay and target networks. External resource: [Deterministic Policy Gradient Algorithms (Silver et al. 2014)](https://proceedings.icml.cc/static/paper_files/icml/2014/Silver14.pdf).
  - Actor-Critic with Monte Carlo Tree Search  
    This method integrates Monte Carlo tree search (MCTS) into actor-critic frameworks for stochastic policies with discrete actions, using parameterized policy πθ(a|s) and value Uφ(s). MCTS guides exploration via an upper confidence bound that balances estimated action value and policy prior probabilities. The parameters θ and φ are updated to minimize cross-entropy and squared error losses against MCTS-derived policies and value estimates, respectively. This approach is inspired by AlphaGo Zero’s methodology. Reference: [Mastering the Game of Go Without Human Knowledge (Silver et al. 2017)](https://www.nature.com/articles/nature24270).
  - Summary  
    Actor-critic methods combine policy optimization with value function estimation, generally via gradient-based updates. The base actor-critic uses temporal difference residuals to estimate advantages, while generalized advantage estimation improves variance-bias trade-off. Deterministic policy gradients extend these methods to continuous action spaces with deterministic policies. Monte Carlo tree search methods enhance policy and value learning through structured online exploration guided by prior models. For comprehensive reinforcement learning frameworks, see [Sutton & Barto, Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book.html).
