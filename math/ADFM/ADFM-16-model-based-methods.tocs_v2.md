- **16 Model-Based Methods**
  - **16.1 Maximum Likelihood Models**
    - Maximum likelihood estimates use observed transition counts and rewards to estimate model parameters.
    - Transition probabilities are approximated by normalizing state-action-state counts.
    - Reward functions are estimated as mean rewards from observed samples.
    - Prior knowledge can initialize count and reward sums non-zero to incorporate domain information.
    - For additional details, see [Chapter 15.6](#) and [Algorithm 15.9](#).
  - **16.2 Update Schemes**
    - Different update schemes balance computational efficiency and update frequency for planning with evolving models.
    - Exploration strategies such as e-greedy (Algorithm 16.3) are necessary to avoid exploitation pitfalls.
    - **Full Updates**
      - Exact solves of the maximum likelihood model after each step can be done using linear programming or value iteration.
      - This approach is computationally expensive but precise (Algorithm 16.4).
    - **Randomized Updates**
      - Updates occur at recently visited and a few random states to reduce computational cost (Algorithm 16.5).
      - This method relates to the Dyna architecture by Richard Sutton (1991).
    - **Prioritized Updates**
      - Uses a priority queue to select states that have the most significant value changes for update.
      - Prioritization is based on the magnitude of value changes weighted by transition probabilities (Algorithm 16.6).
      - Refer to Moore and Atkeson (1993) for foundational work on prioritized sweeping.
  - **16.3 Bayesian Methods**
    - Bayesian reinforcement learning maintains a posterior over model parameters rather than single point estimates.
    - The posterior over transition probabilities is represented using Dirichlet distributions parameterized by transition counts.
    - Beliefs update with experience to refine transition model uncertainty.
    - The Bayesian approach integrates exploration and exploitation naturally without heuristic parameters.
    - For an extensive survey, see Ghavamzadeh et al. (2015) on Bayesian reinforcement learning.
  - **16.4 Bayes-adaptive MDPs**
    - Bayes-adaptive MDPs augment the state space with belief states over model parameters to represent uncertainty explicitly.
    - The transition function accounts both for state transitions and belief updates using Bayesâ€™ rule.
    - The belief space is continuous and often high-dimensional, complicating exact solutions.
    - Approximations or online solution methods are required; see chapters 8 and 9 for relevant techniques.
    - Dynamic decision networks (Figure 16.1) illustrate the integration of state and belief evolution.
  - **16.5 Posterior Sampling**
    - Posterior sampling draws a model from the current belief distribution and solves the corresponding MDP to select actions.
    - This approach avoids heuristic exploration and balances exploration-exploitation effectively.
    - Solving a new MDP for each sample can be expensive but offers statistically principled exploration.
    - Algorithm 16.8 provides an implementation using samples from Dirichlet posteriors.
    - Refer to Strens (2000) for foundational introduction of posterior sampling in reinforcement learning.
  - **16.6 Summary**
    - Model-based methods estimate transition and reward models through interaction.
    - Maximum likelihood approaches require explicit exploration strategies.
    - Prioritized sweeping focuses computational effort on states with the most significant value change.
    - Bayesian methods maintain distributions over models allowing principled exploration.
    - Posterior sampling reduces complexity of Bayes-adaptive MDPs by sampling models for planning.
  - **16.7 Exercises**
    - Exercise 16.1 applies maximum likelihood estimation to simple tabulated data.
    - Exercise 16.2 discusses bounds on prioritized sweeping updates.
    - Exercise 16.3 counts parameters in Bayesian model estimation with Dirichlet priors.
    - Exercise 16.4 computes a posterior Dirichlet distribution after observing transitions.
