- **F Problems**
  - **F.1 Hex World**
    - The hex world is a simple MDP with probabilistic movement in six directions on a hexagonal tile map.
    - Moving in the intended direction occurs with probability 0.7; side neighbors with 0.15 each.
    - Terminal states yield specified rewards and transition to a unique terminal absorbing state.
    - The straight-line hex world variation illustrates reward propagation from a single reward state.
    - See L. Baird’s work on residual algorithms in reinforcement learning for related methods.
  - **F.2 2048**
    - The 2048 problem is a discrete tile game on a 4x4 board with four possible move directions.
    - Tiles merge when two equal-valued tiles collide, creating a tile with combined value and yielding reward equal to that value.
    - New tiles spawn randomly after each move, typically values 2 or 4 in empty spaces.
    - The game ends when no moves can create space; scoring strategies involve stacking larger tiles in a corner.
    - Refer to G. Cirulli’s 2014 development of the 2048 game.
  - **F.3 Cart-Pole**
    - The cart-pole problem involves balancing a pole attached to a cart moving laterally by applying left or right forces.
    - The state is continuous, comprising cart position and velocity, pole angle, and angular velocity.
    - State updates use nonlinear dynamics incorporating gravity, force, pole length, mass, and friction.
    - Reward of 1 is given each timestep the pole remains balanced within constraints; otherwise transition to a terminal state occurs.
    - See Barto, Sutton, and Anderson (1983) and OpenAI Gym implementations for details.
  - **F.4 Mountain Car**
    - Mountain car is a continuous-state control problem where a vehicle must gain momentum by moving left first to ascend a hill.
    - Actions are discrete: accelerate left, right, or coast.
    - State variables include position and velocity with deterministic transitions influenced by gravity.
    - Reward is −1 each step until reaching the goal at the hill top.
    - The problem demonstrates delayed reward knowledge propagation; see Moore (1990) and Singh & Sutton (1996).
  - **F.5 Simple Regulator**
    - The simple regulator is a linear quadratic regulator with single real-valued state and action.
    - Successor states follow a linear-Gaussian distribution; rewards depend quadratically on the state.
    - Optimal policy is to negate the state (π(s) = −s), centering states around zero.
    - Optimal value function is negative quadratic in state with an additive constant based on discount factor.
    - Classical Riccati equation methods do not apply directly due to zero action reward term.
  - **F.6 Aircraft Collision Avoidance**
    - The problem involves deciding climb, descend, or no advisory actions to avoid a head-on intruder aircraft.
    - State variables are relative altitude, vertical rate, previous action, and time to collision.
    - Dynamics include stochastic acceleration noise and action-dependent vertical acceleration with saturation limits.
    - Penalties are imposed for proximity less than 50 m at collision and for advising changes.
    - Efficient solution by discretization and backward induction; see Kochenderfer & Chryssanthacopoulos (2011).
  - **F.7 Crying Baby**
    - This POMDP models caring for a baby with two states (hungry, sated), three actions (feed, sing, ignore), and two noisy observations (crying, quiet).
    - Feeding always sates the baby; singing is an information-gathering action without cry potential when sated.
    - Transition probabilities specify baby hunger dynamics; observations depend on baby state and action.
    - Rewards penalize hunger heavily; feeding and singing incur smaller effort penalties.
    - The optimal infinite horizon policy with discount factor 0.9 balances feeding, singing, and ignoring; see Smallwood & Sondik (1973).
  - **F.8 Machine Replacement**
    - This discrete POMDP aims to maintain a machine with two components that may fail independently.
    - States represent 0 to 2 faulty components; four actions include manufacturing, examining, interrupting, and replacing.
    - Observations include defective/non-defective product detection depending on action.
    - Rewards incorporate product quality, inspection costs, interruption costs, and replacement costs.
    - Optimal policies display complex shapes with disjoint regions; see Kochenderfer (2015) for adaptations.
  - **F.9 Catch**
    - Johnny attempts to catch throws over a finite number of attempts, choosing throw distances from a discrete set.
    - Successful catch probability depends on throw distance and an unknown proficiency parameter s.
    - The proficiency s is assumed discrete and static; rewards equal throw distance if caught, zero otherwise.
    - Johnny updates beliefs over proficiencies and optimizes catch attempts accordingly.
  - **F.10 Prisoner’s Dilemma**
    - A classic two-agent game where each prisoner chooses to cooperate or defect.
    - Payoffs depend on joint actions: both cooperate incur 1 year; unilateral defection yields 0 for defector and 4 years for cooperator; both defect get 3 years.
    - Agents have discrete action sets; rewards can be structured in a standard payoff matrix.
    - Can be played once or infinitely with discount factor 0.9.
    - Tucker (1950) formalized the dilemma; see [Prisoner’s Dilemma](https://en.wikipedia.org/wiki/Prisoner%27s_dilemma) for history.
  - **F.11 Rock-Paper-Scissors**
    - Two agents simultaneously choose rock, paper, or scissors; outcomes produce unit rewards/penalties based on rock-paper-scissors rules.
    - Payoff matrix shows win/loss/zero outcome symmetry.
    - The game can be played once or repeated infinitely with discount factor 0.9.
    - The game exemplifies mixed strategy equilibria; see [Rock-Paper-Scissors](https://en.wikipedia.org/wiki/Rock_paper_scissors) for mathematical analysis.
  - **F.12 Traveler’s Dilemma**
    - Two travelers select suitcase values between $2 and $100; rewards depend on both declarations.
    - If values equal, both get that value; otherwise lower value plus $2 to lower, higher value minus $2 to higher.
    - The unique Nash equilibrium is both declaring $2, despite typical human choices clustering near $97–$100.
    - See K. Basu’s paper on paradoxes of rationality in game theory for this problem.
  - **F.13 Predator-Prey Hex World**
    - Multiagent extension of hex world with sets of predators and prey on a hex grid.
    - Predators receive rewards based on number of prey captured proportionally to number of predators; prey receive heavy penalty if captured.
    - Prey respawn at random cells after being devoured; predators pay small penalty for movement.
    - No terminal states exist; the environment is ongoing.
  - **F.14 Multi-Caregiver Crying Baby**
    - Multiagent version of the crying baby problem with two caregivers.
    - Baby state and observations are shared; actions can be individual feed, sing, or ignore.
    - Either caregiver feeding transitions baby to sated state.
    - Observation dynamics enforce caregivers perceive the same baby state observations but not necessarily each other’s actions.
    - Caregivers incur penalties for hunger and different effort costs based on action preference.
  - **F.15 Collaborative Predator-Prey Hex World**
    - Predators collaborate to capture one moving prey on a hex grid with partial-observation.
    - Each predator senses local cells, receiving noisy observations of prey presence.
    - Predators receive a large reward if any captures occur, penalized by movement cost.
    - Prey moves randomly to unoccupied neighboring cells after capture.
