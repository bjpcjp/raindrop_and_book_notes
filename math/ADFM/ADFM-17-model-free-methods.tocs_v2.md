- **17 Model-Free Methods**
  - **17.1 Incremental Estimation of the Mean**
    - Many model-free methods incrementally estimate action value functions from samples.
    - Incremental mean estimation applies a learning rate function for updates to ensure convergence.
    - Constant learning rates cause exponential decay of older samples' influence.
    - The update rule relates to stochastic gradient descent and defines the temporal difference error.
    - For further depth, see Sutton and Barto's [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book-2nd.html).
  - **17.2 Q-Learning**
    - Q-Learning incrementally updates the action value function using the Bellman equation.
    - The update uses observed rewards and next state-action values without requiring known transition models.
    - An exploration strategy like ε-greedy is necessary to guarantee convergence.
    - The Q-learning update is an off-policy method estimating the optimal policy value.
    - The original Q-learning algorithm is described by Watkins (1989).
  - **17.3 Sarsa**
    - Sarsa updates action values using the actual next action taken, unlike Q-learning's maximization.
    - It is an on-policy method directly estimating the value of the exploration policy.
    - Sarsa often converges slower than Q-learning depending on the problem.
    - Implementation maintains the last experience tuple for updates.
    - For foundational details, see Rummery and Niranjan (1994).
  - **17.4 Eligibility Traces**
    - Eligibility traces propagate reward information backward to prior states and actions.
    - They accelerate learning, especially in environments with sparse rewards.
    - Trace decay parameter λ controls the exponential decay of credit assignment.
    - Sarsa(λ) maintains a trace of visits and applies updates weighted by visit counts.
    - Off-policy methods like Q-learning require special care when using eligibility traces due to stability issues.
  - **17.5 Reward Shaping**
    - Reward shaping augments sparse reward functions with domain knowledge to speed learning.
    - Shaped rewards must satisfy \(F(s,a,s') = \gamma \beta(s') - \beta(s)\) to preserve optimal policies.
    - Shaping can involve penalties or bonuses based on distance or intermediate behaviors.
    - Correct reward shaping maintains policy invariance.
    - For theory and applications, see Ng, Harada, and Russell (1999).
  - **17.6 Action Value Function Approximation**
    - Parametric function approximation generalizes value functions to large or continuous spaces.
    - Gradient descent minimizes the loss between approximated and optimal action values.
    - The update rule uses sample-based estimates since the optimal policy is unknown.
    - Differentiable approximators include linear models and neural networks.
    - Deep reinforcement learning literature offers practical implementation insights, e.g., Graesser and Keng (2020).
  - **17.7 Experience Replay**
    - Experience replay stores past experience tuples to prevent catastrophic forgetting.
    - Random sampling from replay memory reduces correlation between training samples.
    - Replay enables multiple uses of experience tuples, improving data efficiency and stability.
    - Experience replay adapts gradient updates to use batches from stored experiences.
    - Introduced and applied in deep RL by Mnih et al. (2013); extensions include prioritized replay (Schaul et al., 2016).
  - **17.8 Summary**
    - Model-free methods learn action value functions without explicit models.
    - Incremental mean estimation underlies many learning updates.
    - Q-learning and Sarsa differ in on-policy versus off-policy updates and exploration use.
    - Eligibility traces and reward shaping enable faster or more guided learning.
    - Function approximation and experience replay address scalability and stability challenges.
  - **17.9 Exercises**
    - Exercises cover incremental mean estimation with different learning rates.
    - Update rules for value functions and complexities of Q-learning and Sarsa are analyzed.
    - Sarsa(λ) behavior, computational cost, and eligibility trace limits are explored.
    - Solutions involve step-by-step calculations and complexity assessments.
    - Exercises reinforce understanding of core model-free concepts and algorithms.
