- Model-Free Methods  
  - Incremental Estimation of the Mean  
    Many model-free methods incrementally estimate the mean from sequential samples using learning rates α(m). The update rule adjusts the estimate proportionally to the difference between the new sample and the previous estimate, allowing convergence if α(m) satisfies appropriate summability conditions. For further reading, see the foundational text [Reinforcement Learning: An Introduction](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf) by Sutton and Barto.  
  - Q-Learning  
    Q-learning incrementally estimates the action value function by applying the Bellman equation in an off-policy manner, updating the Q-values using observed rewards and the maximum predicted future reward. It requires exploration strategies such as ε-greedy to ensure adequate state-action space coverage. For extensive theory and applications, see the original dissertation [Learning from Delayed Rewards](http://www2.informatik.uni-freiburg.de/~chris/watkins-thesis.pdf) by Watkins.  
  - Sarsa  
    Sarsa is an on-policy reinforcement learning algorithm that updates the action value function using the actual next action from the policy rather than maximizing over all possible next actions. It converges to optimal policies given appropriate exploration but can differ in convergence speed compared to Q-learning. The concept is detailed in Rumery and Niranjan’s technical report [On-Line Q-Learning Using Connectionist Systems](https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-166.pdf).  
  - Eligibility Traces  
    Eligibility traces propagate reward information backwards through recently visited states and actions using exponentially decaying credit (parameter λ). This mechanism speeds up learning, especially with sparse rewards, by updating multiple prior state-action pairs rather than only the immediate predecessor. Care is needed when combining with off-policy methods due to potential instability. For foundational insights, consult Sutton’s paper [Learning to Predict by the Methods of Temporal Differences](https://webdocs.cs.ualberta.ca/~sutton/papers/sutton-88-with-erratum.pdf).  
  - Reward Shaping  
    Reward shaping modifies the reward function during training by adding a potential-based function F(s,a,s′) = γβ(s′) − β(s), ensuring the optimal policy remains invariant. This technique uses domain knowledge to guide learning when original rewards are sparse or insufficient. The policy invariance theory is elaborated by Ng, Harada, and Russell in [Policy Invariance Under Reward Transformations](http://ai.berkeley.edu/Papers/shaping_icml99.pdf).  
  - Action Value Function Approximation  
    Model-free methods can leverage parametric, differentiable approximations of the action value function, such as linear models or neural networks, to handle large or continuous spaces. Gradient descent minimizes the difference between the approximate and optimal action values using sample-based updates. For deep reinforcement learning applications, see the book [Foundations of Deep Reinforcement Learning](https://www.pearson.com/us/higher-education/program/Graesser-Foundations-of-Deep-Reinforcement-Learning/PGM334651.html).  
  - Experience Replay  
    Experience replay stores past experience tuples in a buffer, sampling them randomly during training to mitigate catastrophic forgetting and reduce correlation in sequential data samples. This improves data efficiency and stabilizes learning when applying function approximation. The method was popularized by Mnih et al. in the seminal work [Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602).
