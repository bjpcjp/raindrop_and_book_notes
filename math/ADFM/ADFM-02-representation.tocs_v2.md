- **Representation**
  - **Degrees of Belief and Probability**
    - Plausibility between propositions can be compared using operators indicating more likely, equally likely, or less likely.
    - Universal comparability and transitivity assumptions allow representing plausibility via real-valued functions.
    - Subjective probability axioms justify using probability functions P such that 0 ≤ P(A) ≤ 1.
    - See [Probability Theory: The Logic of Science](https://cambridge.org) by E. T. Jaynes for elaboration.
  - **Probability Distributions**
    - Probability distributions assign probabilities over discrete or continuous sets of values.
    - Probability mass functions represent discrete distributions with masses summing to one.
    - Continuous variables are represented via probability density functions integrating to one.
    - Distributions often combine into mixtures for flexibility.
    - See [Introduction to Probability](https://athenasc.com/probbook.html) by Bertsekas and Tsitsiklis for foundational theory.
    - **Discrete Probability Distributions**
      - Discrete distributions define probabilities over a finite set of assignments.
      - The sum of probabilities over all discrete values equals one.
      - Assigning probabilities to binary variables uses conventions like 0 for false and 1 for true.
    - **Continuous Probability Distributions**
      - Continuous distributions use density functions where probability for exact values is infinitesimal.
      - Cumulative distribution functions define cumulative probabilities up to a value.
      - Quantile functions provide inverse CDF values for given probability thresholds.
      - Common distributions include Uniform, Gaussian, truncated Gaussian, and mixtures.
      - Mixture models combine multiple unimodal distributions weighted by mixing coefficients.
  - **Joint Distributions**
    - Joint probability distributions cover assignments across multiple variables.
    - Marginal distributions are obtained by summing or integrating out other variables.
    - Discrete joint distributions can be tabulated but suffer exponential growth in parameters.
    - Independence assumptions allow factorization and large parameter savings.
    - Joint distributions can be represented as factors or decision trees for storage efficiency.
    - **Discrete Joint Distributions**
      - Tables explicitly list all possible combinations with associated probabilities summing to one.
      - Decision trees compactly represent repeated or structured values.
    - **Continuous Joint Distributions**
      - Joint continuous distributions include multivariate uniform and Gaussian distributions.
      - Piecewise constant densities and decision trees can approximate or represent continuous joint densities.
      - Multivariate Gaussians require mean vector and covariance matrix parameters.
      - Independence within variables corresponds to diagonal covariance matrices.
  - **Conditional Distributions**
    - Conditional probability distributions give probabilities of variables given evidence values.
    - Defined as P(x | y) = P(x, y) / P(y) with corresponding normalization constraints.
    - Bayes' rule relates P(x | y) and P(y | x).
    - Conditional distributions can be discrete tables, conditional Gaussians, linear Gaussian, conditional linear Gaussians, sigmoid models, or deterministic.
    - **Discrete Conditional Models**
      - Conditional tables specify probabilities for a variable given conditioning variables.
      - Number of parameters grows exponentially with conditioned variables.
      - Decision trees reduce storage by exploiting repeated patterns.
    - **Conditional Gaussian Models**
      - Represent continuous variables conditioned on discrete variables via a mixture of Gaussians.
    - **Linear Gaussian Models**
      - Model continuous variables with Gaussian noise where mean is a linear function of other continuous variables.
    - **Conditional Linear Gaussian Models**
      - Combine conditioning on discrete and continuous variables by having Gaussian means as linear functions dependent on discrete conditions.
    - **Sigmoid Models**
      - Model binary variables conditioned on continuous variables using soft thresholds with logistic sigmoid functions.
    - **Deterministic Variables**
      - Variables with values derived deterministically from evidence can be sparsely represented instead of full tables.
  - **Bayesian Networks**
    - Bayesian networks represent joint distributions with directed acyclic graphs encoding conditional dependencies.
    - Each node associates with a conditional distribution given its parents.
    - Chain rule constructs joint distribution as a product of conditional distributions.
    - Conditional independence encoded in the network reduces the number of parameters needed.
    - Implementation involves variables, conditional factors, and directed graphs.
    - For comprehensive study, see [Probabilistic Graphical Models](https://mitpress.mit.edu/books/probabilistic-graphical-models) by Koller and Friedman.
  - **Conditional Independence**
    - Conditional independence generalizes independence by stating P(X, Y | Z) = P(X | Z) P(Y | Z).
    - d-separation criteria determine conditional independence from graph structure through analysis of paths and blocking conditions.
    - Markov blankets comprise a node’s parents, children, and co-parents of children, forming a minimal conditioning set rendering the node independent of others.
    - Invalid conditional independence assumptions risk model inaccuracies.
    - For algorithms on d-separation, see Koller and Friedman’s comprehensive text.
- **Summary**
  - Degrees of belief motivate probability representations subject to axioms and ordering assumptions.
  - Probability distributions span discrete, continuous, and mixture forms.
  - Joint and conditional distributions enable modeling of multiple variables and their dependencies.
  - Bayesian networks provide graphical factorization and parameter reduction via conditional independence.
  - Conditional independence assumptions are foundational for compact and effective probabilistic modeling.

