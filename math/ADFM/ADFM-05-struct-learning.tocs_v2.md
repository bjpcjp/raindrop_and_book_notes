- Structure Learning
  - Bayesian Network Scoring  
    The section explains scoring a network structure G based on how well it models data by maximizing P(G|D) using Bayes’ rule and integrating out parameters θ. The Bayesian score balances model complexity and data fit, promoting simpler models for smaller datasets. Factors include priors and data quantity, with a uniform prior often used. For more on Bayesian network scoring, see [Probabilistic Graphical Models by Koller and Friedman](https://mitpress.mit.edu/books/probabilistic-graphical-models).
  - Directed Graph Search  
    This section discusses searching the vast and superexponential space of directed acyclic graphs to maximize the Bayesian score. Algorithms like K2 and local search (hill climbing) offer polynomial-time but non-optimal solutions. Search challenges include NP-hardness and local optima, with solutions such as simulated annealing and genetic algorithms. For algorithmic approaches, refer to [Learning Bayesian Networks is NP-Complete by Chickering](https://people.cs.ubc.ca/~russell/papers/k2-learning.pdf).
  - Markov Equivalence Classes  
    Markov equivalence occurs when different graph structures encode identical conditional independence assumptions. Equivalence requires same undirected edges and identical immoral v-structures. Bayesian scores with BDe priors remain score equivalent across Markov equivalent structures. Factoring Markov equivalence avoids ambiguity in edge directions. See [Learning Bayesian Networks: The Combination of Knowledge and Statistical Data](https://link.springer.com/article/10.1007/BF00994199) for detailed theory.
  - Partially Directed Graph Search  
    Searching over Markov equivalence classes represented by partially directed graphs (containing directed and undirected edges) can be more efficient than direct DAG search. Local operations include edge addition, removal, reversal, and creating immoral v-structures without producing cycles. Scoring involves selecting members of the equivalence class and computing Bayesian scores. For methods on this approach, consult [Learning Equivalence Classes of Bayesian-Network Structures by Chickering](http://www.jmlr.org/papers/volume2/chickering02a/chickering02a.pdf).
  - Summary  
    This section synthesizes key points: structure learning maximizes the Bayesian score, balancing complexity and data fit; the search space is superexponential and NP-hard to explore globally; heuristic search methods provide efficient, albeit suboptimal, solutions; and Markov equivalence classes allow more compact and unambiguous model representations.
  - Exercises  
    The exercises reinforce concepts of graph neighborhoods, local search iteration counts, Markov equivalence classes, and partially directed acyclic graphs with examples and solutions. They consolidate understanding of network structure complexity, search operations, and equivalence class properties. For further practice, see [Bayesian Network Structure Learning Exercises](https://www.cs.ubc.ca/~murphyk/Teaching/CS340-Fall07/bn-exercise.html).
