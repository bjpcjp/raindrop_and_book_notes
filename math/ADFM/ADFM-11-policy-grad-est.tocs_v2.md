- **Policy Gradient Estimation**
  - **Finite Difference**
    - Finite difference estimates gradients by evaluating small changes in function values along each parameter dimension.  
    - Policy gradient estimation using finite difference requires simulating rollouts to estimate utility.  
    - Variance in estimates arises from stochastic trajectories; sharing random seeds can reduce this variance.  
    - Policy parameterization scale significantly affects gradient sensitivity, as shown in Example 11.1.  
    - See PEGASUS algorithm [Ng and Jordan (2000)](https://auai.org/uai2000/papers/063.pdf).  
  - **Regression Gradient**
    - Uses linear regression on random perturbations around policy parameters to estimate gradients more robustly.  
    - Requires generating multiple perturbations and corresponding utility rollouts to perform regression.  
    - Increases sample efficiency compared to finite differences by using random directions rather than coordinate axes.  
    - Example 11.2 demonstrates successful gradient estimation on a noisy quadratic function.  
    - Review linear regression basics in section 8.6.  
  - **Likelihood Ratio**
    - Applies the likelihood ratio (log derivative) trick to express policy gradient as an expectation involving ∇θ log likelihood.  
    - Gradient estimator sums log-policy gradients weighted by trajectory returns over sampled trajectories.  
    - Can handle stochastic policies without knowledge of transition probabilities; deterministic policies require transition models.  
    - Algorithm 11.4 implements this estimator using m rollouts.  
    - For more, see Williams (1992) on REINFORCE algorithms.  
  - **Reward-to-Go**
    - Introduces the reward-to-go method to reduce variance by considering future rewards starting from each timestep.  
    - Avoids bias by ensuring gradient terms depend only on future rewards causally affected by actions at that timestep.  
    - Algorithm 11.5 implements the reward-to-go estimator, showing improvements in variance.  
    - The reward-to-go corresponds to the state-action value estimate \(Q_\theta(s,a)\).  
    - See the policy gradient theorem for theoretical grounding.  
  - **Baseline Subtraction**
    - Further variance reduction is achieved by subtracting a baseline value from the reward-to-go in gradient estimates.  
    - Baseline subtraction does not introduce bias because the expectation of baseline weighted log gradient is zero.  
    - The optimal baseline minimizes variance and can be computed from sample data as a ratio of expectations involving log-gradient squares.  
    - Using a state-dependent baseline relates to advantage functions \(A(s,a) = Q(s,a) - U(s)\).  
    - Algorithm 11.6 details the method combining likelihood ratio, reward-to-go, and baseline subtraction.  
    - See Peters and Schaal (2008) for discussion on baselines in policy gradients.  
  - **Summary**
    - Finite differences and linear regression provide straightforward gradient estimates but suffer from high variance or scaling issues.  
    - The likelihood ratio method leverages policy structure for efficient unbiased gradient estimation.  
    - Reward-to-go and baseline subtraction significantly reduce variance without biasing estimates.  
    - Policy gradients based on advantages improve learning signal quality.  
    - See comprehensive survey in Sutton et al. (2000).
