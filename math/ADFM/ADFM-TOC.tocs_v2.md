- **Preface**
  - The preface introduces the purpose and scope of the book.
  - It outlines the target audience and prerequisites.
  - It sets expectations regarding the topics and depth covered.
  - Recommendations for further preparatory reading include foundational texts on algorithms and decision theory.

- **Acknowledgments**
  - The acknowledgments credit contributors, reviewers, and supporters of the book.
  - It highlights collaboration and peer input in the book’s development.
  - Appreciation is expressed for institutional and individual support.

- **1 Introduction**
  - **1.1 Decision Making**
    - Defines decision making within uncertain environments.
    - Describes the role of preferences and rational choice.
    - References foundational models in decision theory.
  - **1.2 Applications**
    - Presents practical examples across domains like robotics and economics.
    - Explains how decision making algorithms impact these fields.
  - **1.3 Methods**
    - Details various algorithmic frameworks for decision making.
    - Contrasts model-based and model-free approaches.
  - **1.4 History**
    - Chronicles the evolution of decision theory and related algorithms.
    - Highlights seminal contributions and milestones.
  - **1.5 Societal Impact**
    - Discusses ethical considerations and the societal influence of decision algorithms.
    - Points to challenges in fairness, accountability, and transparency.
  - **1.6 Overview**
    - Summarizes the book’s structure and thematic organization.
    - Previews major topics covered in subsequent parts.

- **I Probabilistic Reasoning**
  - **2 Representation**
    - **2.1 Degrees of Belief and Probability**
      - Introduces probability as a measure of belief strength.
      - Differentiates subjective probabilities from frequencies.
      - Suggests [Probability Theory: The Logic of Science](https://bayes.wustl.edu/etj/prob/book.pdf) for deeper study.
    - **2.2 Probability Distributions**
      - Defines distributions and their key properties.
      - Explains common discrete and continuous distributions.
    - **2.3 Joint Distributions**
      - Describes the representation of joint probabilities over multiple variables.
      - Discusses factorization and complexity implications.
    - **2.4 Conditional Distributions**
      - Details the concept and calculation of conditional probabilities.
      - Emphasizes the importance for inference and learning.
    - **2.5 Bayesian Networks**
      - Presents graphical models representing conditional dependencies.
      - Explains node, edge semantics, and factorization of joint distributions.
    - **2.6 Conditional Independence**
      - Formalizes conditional independence and its role in simplifying models.

  - **3 Inference**
    - **3.1 Inference in Bayesian Networks**
      - Details computational methods for querying Bayesian networks.
      - Discusses exact and approximate techniques.
    - **3.2 Inference in Naive Bayes Models**
      - Explains simplified inference under independence assumptions.
    - **3.3 Sum-Product Variable Elimination**
      - Describes an algorithm for exact inference via message passing.
    - **3.4 Belief Propagation**
      - Covers iterative inference in tree-structured or loopy graphs.
    - **3.5 Computational Complexity**
      - Analyzes the computational challenges and NP-hardness.
    - **3.6 Direct Sampling**
      - Introduces naive sampling as an inference approach.
    - **3.7 Likelihood Weighted Sampling**
      - Discusses importance sampling weighted by evidence likelihood.
    - **3.8 Gibbs Sampling**
      - Describes a Markov Chain Monte Carlo method for approximate inference.
    - **3.9 Inference in Gaussian Models**
      - Covers inference methods specialized for Gaussian distributions.

  - **4 Parameter Learning**
    - **4.1 Maximum Likelihood Parameter Learning**
      - Introduces estimation of parameters by maximizing the likelihood function.
    - **4.2 Bayesian Parameter Learning**
      - Applies Bayesian inference to update parameter beliefs.
    - **4.3 Nonparametric Learning**
      - Explores methods that do not assume fixed parameter dimension.
    - **4.4 Learning with Missing Data**
      - Discusses algorithms for incomplete observation scenarios.
    - **4.5 Summary**
      - Reviews key concepts and methods discussed.
  
  - **5 Structure Learning**
    - **5.1 Bayesian Network Scoring**
      - Covers metrics for model evaluation and selection.
    - **5.2 Directed Graph Search**
      - Explains algorithms for searching graph structures.
    - **5.3 Markov Equivalence Classes**
      - Describes equivalence between graphical models representing the same distributions.
    - **5.4 Partially Directed Graph Search**
      - Introduces search methods for hybrid graph representations.

  - **6 Simple Decisions**
    - **6.1 Constraints on Rational Preferences**
      - States axioms defining rational consistency.
    - **6.2 Utility Functions**
      - Defines utility as a numerical representation of preferences.
    - **6.3 Utility Elicitation**
      - Describes techniques to quantify user preferences.
    - **6.4 Maximum Expected Utility Principle**
      - Introduces decision-making by maximizing expected utility.
    - **6.5 Decision Networks**
      - Explains graphical models combining decisions and uncertainties.
    - **6.6 Value of Information**
      - Discusses quantifying information's impact on decision quality.
    - **6.7 Irrationality**
      - Addresses deviations from rational decision behaviors.

- **II Sequential Problems**
  - **7 Exact Solution Methods**
    - **7.1 Markov Decision Processes**
      - Defines frameworks for sequential decision making under uncertainty.
    - **7.2 Policy Evaluation**
      - Details methods for computing expected returns of policies.
    - **7.3 Value Function Policies**
      - Explains representation of policies via value functions.
    - **7.4 Policy Iteration**
      - Describes iterative policy improvement algorithms.
    - **7.5 Value Iteration**
      - Covers dynamic programming for optimal value computation.
    - **7.6 Asynchronous Value Iteration**
      - Describes updates not requiring full state sweeps.
    - **7.7 Linear Program Formulation**
      - Presents LP approaches for solving MDPs.
    - **7.8 Linear Systems with Quadratic Reward**
      - Examines specialized formulations of MDPs with quadratic utilities.

  - **8 Approximate Value Functions**
    - **8.1 Parametric Representations**
      - Discusses using parameterized functions to represent value functions.
    - **8.2 Nearest Neighbor**
      - Describes interpolation via the closest known values.
    - **8.3 Kernel Smoothing**
      - Introduces smooth estimates using kernels.
    - **8.4 Linear Interpolation**
      - Covers linear interpolation techniques.
    - **8.5 Simplex Interpolation**
      - Explains higher-dimensional interpolation using simplices.
    - **8.6 Linear Regression**
      - Applies regression for value function approximation.
    - **8.7 Neural Network Regression**
      - Introduces neural networks as nonlinear function approximators.

  - **9 Online Planning**
    - **9.1 Receding Horizon Planning**
      - Defines planning over rolling finite horizons.
    - **9.2 Lookahead with Rollouts**
      - Covers simulating future outcomes to improve decisions.
    - **9.3 Forward Search**
      - Describes tree expansion from the current state.
    - **9.4 Branch and Bound**
      - Explains pruning of search spaces via bounds.
    - **9.5 Sparse Sampling**
      - Presents sampling-based approximations of planning.
    - **9.6 Monte Carlo Tree Search**
      - Details stochastic tree search balancing exploration and exploitation.
    - **9.7 Heuristic Search**
      - Introduces informed search strategies guided by heuristics.
    - **9.8 Labeled Heuristic Search**
      - Covers enhanced heuristic approaches using labels.
    - **9.9 Open-Loop Planning**
      - Discusses planning without state feedback.

  - **10 Policy Search**
    - **10.1 Approximate Policy Evaluation**
      - Describes estimating policy values without full models.
    - **10.2 Local Search**
      - Details iterative improvement in policy parameters.
    - **10.3 Genetic Algorithms**
      - Explains evolutionary strategies for optimizing policies.
    - **10.4 Cross Entropy Method**
      - Introduces probabilistic optimization technique.
    - **10.5 Evolution Strategies**
      - Covers adaptive search methods mimicking natural evolution.
    - **10.6 Isotropic Evolutionary Strategies**
      - Discusses a simplified evolutionary method using isotropic mutations.

  - **11 Policy Gradient Estimation**
    - **11.1 Finite Difference**
      - Describes gradient estimation by perturbing parameters.
    - **11.2 Regression Gradient**
      - Covers regression-based approaches to gradient estimation.
    - **11.3 Likelihood Ratio**
      - Explains an importance sampling method for gradients.
    - **11.4 Reward-to-Go**
      - Introduces variance reduction techniques using future rewards.
    - **11.5 Baseline Subtraction**
      - Details further variance reduction by removing baselines.

  - **12 Policy Gradient Optimization**
    - **12.1 Gradient Ascent Update**
      - Presents basic gradient-based policy improvement.
    - **12.2 Restricted Gradient Update**
      - Describes constrained optimization steps.
    - **12.3 Natural Gradient Update**
      - Covers geometry-aware gradient methods.
    - **12.4 Trust Region Update**
      - Explains stable policy update methods.
    - **12.5 Clamped Surrogate Objective**
      - Introduces objectives limiting policy changes to improve robustness.

  - **13 Actor-Critic Methods**
    - **13.1 Actor-Critic**
      - Defines architectures combining policy (actor) and value function (critic).
    - **13.2 Generalized Advantage Estimation**
      - Presents advanced advantage computations to reduce bias and variance.
    - **13.3 Deterministic Policy Gradient**
      - Covers gradients for deterministic policies.
    - **13.4 Actor-Critic with Monte Carlo Tree Search**
      - Integrates tree search with actor-critic learning.

  - **14 Policy Validation**
    - **14.1 Performance Metric Evaluation**
      - Discusses criteria for evaluating policies.
    - **14.2 Rare Event Simulation**
      - Covers techniques to evaluate policies on infrequent but important events.
    - **14.3 Robustness Analysis**
      - Addresses sensitivity of policies under model perturbations.
    - **14.4 Trade Analysis**
      - Explores compromises among competing objectives.
    - **14.5 Adversarial Analysis**
      - Examines testing policies against worst-case scenarios.

- **III Model Uncertainty**
  - **15 Exploration and Exploitation**
    - **15.1 Bandit Problems**
      - Defines the exploration-exploitation tradeoff in single-state settings.
    - **15.2 Bayesian Model Estimation**
      - Discusses Bayesian updating for uncertainty quantification.
    - **15.3 Undirected Exploration Strategies**
      - Details naive exploration tactics without model guidance.
    - **15.4 Directed Exploration Strategies**
      - Presents exploration guided by model uncertainty.
    - **15.5 Optimal Exploration Strategies**
      - Explains strategies optimizing long-term performance via exploration.
    - **15.6 Exploration with Multiple States**
      - Extends the exploration-exploitation trade to MDPs.

  - **16 Model-Based Methods**
    - **16.1 Maximum Likelihood Models**
      - Covers estimation of transition models from data.
    - **16.2 Update Schemes**
      - Describes procedures for updating model parameters online.
    - **16.3 Bayesian Methods**
      - Details full probabilistic modeling of uncertainty.
    - **16.4 Bayes-adaptive MDPs**
      - Introduces models incorporating learning into planning.
    - **16.5 Posterior Sampling**
      - Discusses sampling-based strategies for model uncertainty.

  - **17 Model-Free Methods**
    - **17.1 Incremental Estimation of the Mean**
      - Details simple online estimations of expected returns.
    - **17.2 Q-Learning**
      - Introduces off-policy temporal difference learning.
    - **17.3 Sarsa**
      - Describes on-policy temporal difference learning.
    - **17.4 Eligibility Traces**
      - Covers methods combining multi-step returns.
    - **17.5 Reward Shaping**
      - Presents techniques to modify reward signals for better learning.
    - **17.6 Action Value Function Approximation**
      - Explains function approximation for large state-action spaces.
    - **17.7 Experience Replay**
      - Describes data reuse for improved sample efficiency.

  - **18 Imitation Learning**
    - **18.1 Behavioral Cloning**
      - Defines supervised learning to mimic expert behavior.
    - **18.2 Dataset Aggregation**
      - Introduces iterative methods for correcting distributional drift.
    - **18.3 Stochastic Mixing Iterative Learning**
      - Covers probabilistic blending of learned and expert policies.
    - **18.4 Maximum Margin Inverse Reinforcement Learning**
      - Explains inferring reward functions via margin maximization.
    - **18.5 Maximum Entropy Inverse Reinforcement Learning**
      - Discusses probabilistic models of expert behavior.
    - **18.6 Generative Adversarial Imitation Learning**
      - Applies adversarial learning techniques to imitation.

- **IV State Uncertainty**
  - **19 Beliefs**
    - **19.1 Belief Initialization**
      - Describes setting initial probability distributions over states.
    - **19.2 Discrete State Filter**
      - Covers filtering for finite state spaces.
    - **19.3 Linear Gaussian Filter**
      - Introduces Kalman filtering for linear Gaussian systems.
    - **19.4 Extended Kalman Filter**
      - Details nonlinear extensions via local linearizations.
    - **19.5 Unscented Kalman Filter**
      - Presents alternative nonlinear filter using deterministic sampling.
    - **19.6 Particle Filter**
      - Explains sampling-based filtering approaches.
    - **19.7 Particle Injection**
      - Discusses techniques to maintain particle diversity.

  - **20 Exact Belief State Planning**
    - **20.1 Belief-State Markov Decision Processes**
      - Defines planning over continuous belief spaces.
    - **20.2 Conditional Plans**
      - Describes policy representations conditioned on observations.
    - **20.3 Alpha Vectors**
      - Introduces piecewise-linear representations of value functions.
    - **20.4 Pruning**
      - Covers removal of dominated alpha vectors.
    - **20.5 Value Iteration**
      - Details dynamic programming in belief space.
    - **20.6 Linear Policies**
      - Explains linear function approximations for policies.

  - **21 Offline Belief State Planning**
    - **21.1 Fully Observable Value Approximation**
      - Uses easier fully observable MDPs to bound belief state problems.
    - **21.2 Fast Informed Bound**
      - Discusses computationally efficient value function bounds.
    - **21.3 Fast Lower Bounds**
      - Presents methods for quickly estimating pessimistic values.
    - **21.4 Point-Based Value Iteration**
      - Introduces scalable approximate value iteration on selected belief points.
    - **21.5 Randomized Point-Based Value Iteration**
      - Extends point-based methods with random sampling.
    - **21.6 Sawtooth Upper Bound**
      - Explains upper bound approximations using sawtooth functions.
    - **21.7 Point Selection**
      - Discusses heuristics for choosing belief points.
    - **21.8 Sawtooth Heuristic Search**
      - Covers heuristic search guided by sawtooth bounds.
    - **21.9 Triangulated Value Functions**
      - Introduces interpolation schemes for value function approximation.

  - **22 Online Belief State Planning**
    - **22.1 Lookahead with Rollouts**
      - Describes simulating future trajectories for planning.
    - **22.2 Forward Search**
      - Explains expanding belief tree nodes forward.
    - **22.3 Branch and Bound**
      - Details pruning suboptimal branches.
    - **22.4 Sparse Sampling**
      - Covers sampling-based belief state exploration.
    - **22.5 Monte Carlo Tree Search**
      - Applies stochastic tree search in belief spaces.
    - **22.6 Determinized Sparse Tree Search**
      - Combines sparse sampling with determinization.
    - **22.7 Gap Heuristic Search**
      - Introduces heuristics based on approximation gaps.

  - **23 Controller Abstractions**
    - **23.1 Controllers**
      - Defines parametric controllers for decision making.
    - **23.2 Policy Iteration**
      - Covers iterative improvement of controller parameters.
    - **23.3 Nonlinear Programming**
      - Describes optimization techniques for controller synthesis.
    - **23.4 Gradient Ascent**
      - Explains gradient-based controller optimization.

- **V Multiagent Systems**
  - **24 Multiagent Reasoning**
    - **24.1 Simple Games**
      - Introduces foundational concepts in game theory.
    - **24.2 Response Models**
      - Covers models predicting agent reactions.
    - **24.3 Nash Equilibrium**
      - Defines stable solution concepts where no agent benefits from unilateral change.
    - **24.4 Correlated Equilibrium**
      - Extends equilibria allowing correlated strategies.
    - **24.5 Iterated Best Response**
      - Details iterative methods converging to equilibria.
    - **24.6 Hierarchical Softmax**
      - Presents computational methods for large action spaces.
    - **24.7 Fictitious Play**
      - Describes learning in games through repeated play.

  - **25 Sequential Problems**
    - **25.1 Markov Games**
      - Defines extension of MDPs to multiple interacting agents.
    - **25.2 Response Models**
      - Discusses predictive behavior models in multiagent settings.
    - **25.3 Nash Equilibrium**
      - Covers equilibrium concepts in Markov games.
    - **25.4 Opponent Modeling**
      - Explains techniques estimating opponents’ strategies.
    - **25.5 Nash Q-Learning**
      - Introduces Q-learning adapted for multiagent games.

  - **26 State Uncertainty**
    - **26.1 Partially Observable Markov Games**
      - Extends partially observable MDPs to multiagent contexts.
    - **26.2 Policy Evaluation**
      - Details evaluation of multiagent belief-dependent policies.
    - **26.3 Nash Equilibrium**
      - Discusses equilibria under state uncertainty.
    - **26.4 Dynamic Programming**
      - Covers dynamic programming methods for multiagent POMDPs.

  - **27 Collaborative Agents**
    - **27.1 Decentralized Partially Observable Markov Decision Processes**
      - Defines frameworks for decentralized decision making under uncertainty.
    - **27.2 Subclasses**
      - Describes restricted problem classes tractable for solution.
    - **27.3 Dynamic Programming**
      - Explains solution methods for decentralized control.
    - **27.4 Iterated Best Response**
      - Discusses iterative solution techniques in collaborative settings.
    - **27.5 Heuristic Search**
      - Presents heuristic approaches to decentralized problems.
    - **27.6 Nonlinear Programming**
      - Covers optimization formulations for decentralized controllers.

- **Appendices**
  - **A Mathematical Concepts**
    - Provides foundational mathematical tools including measure theory, convexity, and information theory.
  - **B Probability Distributions**
    - Details key probability distributions used throughout the book.
  - **C Computational Complexity**
    - Covers complexity classes and asymptotic notation relevant to algorithms.
  - **D Neural Representations**
    - Describes neural network architectures and training methods.
  - **E Search Algorithms**
    - Reviews classical search problem formulations and solution methods.
  - **F Problems**
    - Introduces benchmark problems used as examples or exercises in the book.
  - **G Julia**
    - Provides an overview of programming constructs in Julia for implementation examples.

- **References**
  - Comprehensive list of bibliographic sources cited throughout the book.

- **Index**
  - Alphabetical listing of key terms and topics with page references.
