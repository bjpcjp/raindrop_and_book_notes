- **26 State Uncertainty**
  - **26.1 Partially Observable Markov Games**
    - POMGs generalize Markov games to partial observability and POMDPs to multiple agents.  
    - Each agent selects actions based on local, noisy observations of a shared state.  
    - Inferring joint beliefs is complex due to nested reasoning about other agents' actions and observations.  
    - [Dynamic Programming for Partially Observable Stochastic Games](https://www.aaai.org/Papers/AAAI/2004/AAAI04-145.pdf) by Hansen et al. provides foundational algorithms.  
  - **26.2 Policy Evaluation**
    - Policies can be represented as tree-based conditional plans or graph-based stochastic controllers.  
    - Utilities for joint policies are computed recursively over states, actions, and observations.  
    - Stochastic controllers require solving systems of linear equations or iterative evaluation.  
    - Example algorithms show utility calculations for joint policies and controllers.  
  - **26.3 Nash Equilibrium**
    - Nash equilibria occur when no agent can improve reward by unilaterally changing policy.  
    - Computing d-step Nash equilibria involves enumerating joint conditional plans as actions in a simple game.  
    - Solving the simple game for equilibrium yields an equilibrium valid for the original POMG.  
    - Nash equilibrium computation is generally computationally challenging.  
  - **26.4 Dynamic Programming**
    - Dynamic programming iteratively constructs and prunes conditional plans to find equilibria efficiently.  
    - Pruning eliminates dominated policies via linear programming feasibility checks over joint beliefs.  
    - This approach significantly reduces computation compared to enumerating all policies outright.  
    - Example demonstrates pruning in the multi-caregiver crying baby problem.  
  - **26.5 Summary**
    - POMGs extend POMDPs and Markov games to multiple agents with partial observability.  
    - Policies use conditional plans or finite controllers instead of belief states.  
    - Nash equilibria correspond to equilibria in reduced simple games constructed from conditional plans.  
    - Dynamic programming improves computational tractability through iterative plan expansion and pruning.  
  - **26.6 Exercises**
    - Exercise solutions illustrate POMG generalization of POMDPs and MGs, communication modeling, and policy enumeration.  
    - Provides best response definitions and iterated best response algorithms for POMGs.  
    - Exercises deepen understanding of policy incentives, complexity, and equilibrium concepts.  
    - [Framework for Sequential Planning in Multi-Agent Settings](https://jair.org/index.php/jair/article/view/10302) by Gmytrasiewicz and Doshi offers further insight on interactive POMDPs.
