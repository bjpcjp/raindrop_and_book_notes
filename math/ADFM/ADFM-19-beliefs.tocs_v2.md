- 19 Beliefs
  - 19.1 Belief Initialization  
    The agent's initial belief over states can be represented parametrically (e.g., categorical or Gaussian) or non-parametrically (e.g., particles). Diffuse initial beliefs avoid overconfidence in incorrect states but pose challenges in sparse sampling, especially in non-parametric methods. Initializing beliefs after informative observations, such as landmark detections, helps focus belief sampling; see [Probabilistic Robotics](https://mitpress.mit.edu/books/probabilistic-robotics) for further details.
  - 19.2 Discrete State Filter  
    Beliefs in discrete POMDPs are categorical distributions over finite states and can be updated exactly using recursive Bayesian estimation based on transition and observation models. The belief vector lies in a probability simplex, ensuring non-negative elements summing to one. Accurate observation and transition models critically influence belief update success; foundational concepts appear in [Sutton and Barto's Reinforcement Learning](http://incompleteideas.net/book/the-book.html).
  - 19.3 Linear Gaussian Filter  
    For continuous states under linear–Gaussian assumptions on transitions, observations, and beliefs, the Kalman filter performs exact belief updates using mean and covariance predictions and Kalman gain adjustments. This method assumes Gaussian noise and linear dynamics, enabling efficient recursive estimation. The classical Kalman filter is detailed in [Kalman’s original paper](https://ieeexplore.ieee.org/document/4085255).
  - 19.4 Extended Kalman Filter  
    The EKF extends the Kalman filter to nonlinear transition and observation models by linearizing dynamics around the current belief mean using Jacobians. It produces Gaussian approximations but does not preserve exact posteriors or multimodality, trading accuracy for computational tractability. The EKF implementation and applicability are extensively covered in [Probabilistic Robotics](https://mitpress.mit.edu/books/probabilistic-robotics).
  - 19.5 Unscented Kalman Filter  
    The UKF avoids derivatives by deterministically sampling sigma points to approximate nonlinear transformations of Gaussian distributions. It generalizes belief updates by transforming sigma points through nonlinear functions, reconstructing new means and covariances. Parameters control sigma point distribution, and the method outperforms EKF in many nonlinear settings. For in-depth theory, see [Julier and Uhlmann's UKF paper](https://ieeexplore.ieee.org/document/1416923).
  - 19.6 Particle Filter  
    Particle filters represent beliefs as weighted samples (particles) and update by sampling successor states via transition dynamics, weighting by observation likelihoods, and resampling accordingly. They approximate arbitrary distributions and handle nonlinear, non-Gaussian dynamics but suffer from particle deprivation. Standard tutorials include [Arulampalam et al.’s particle filter overview](https://ieeexplore.ieee.org/document/978374).
  - 19.7 Particle Injection  
    Particle injection mitigates particle deprivation by adding random samples from broader distributions, balancing exploration and accuracy. Adaptive injection adjusts injection rates based on fast and slow moving averages of particle weights, preventing overconfidence during filter convergence or noisy observations. Core principles and adaptive strategies are discussed in [Probabilistic Robotics](https://mitpress.mit.edu/books/probabilistic-robotics).
