- Offline Belief State Planning
  - Fully Observable Value Approximation
    - Describes QMDP algorithm where the value function is approximated under the assumption of full observability after one step. The algorithm iterates alpha vectors for each action using value iteration on the fully observable MDP. QMDP produces an upper bound on the optimal POMDP value function but may poorly approximate information gathering actions. See [Hauskrecht 2000](https://doi.org/10.1613/jair.613) for QMDP theoretical properties.
  - Fast Informed Bound
    - Introduces a tighter upper bound than QMDP by accounting for the observation model in alpha vector updates. The fast informed bound requires O(|A||S||O|) operations and generally produces a tighter upper bound on the value function. Further details are available in [Hauskrecht 2000](https://doi.org/10.1613/jair.613).
  - Fast Lower Bounds
    - Presents two lower bound methods: the best-action worst-state (BAWS) lower bound and the blind lower bound. BAWS uses the worst-case state reward to produce a loose but quick lower bound. The blind lower bound assumes committing to a fixed action regardless of observations and is refined by iterative updates. These bounds seed more complex planning algorithms.
  - Point-Based Value Iteration
    - Describes a lower-bound method generating multiple alpha vectors, each tied to a belief point. The algorithm performs backups at these points to improve the value function approximation. Although it does not guarantee optimality due to limited belief coverage, it provides an effective lower bound for policy extraction. A survey on these solvers is [Shani et al. 2012](https://doi.org/10.1007/s10458-011-9188-0).
  - Randomized Point-Based Value Iteration
    - A variation that randomly updates alpha vectors associated with belief points, potentially increasing or decreasing their number. It improves computational efficiency while maintaining policy improvement guarantees at the sampled beliefs. It is introduced in [Spaan & Vlassis 2005](https://doi.org/10.1613/jair.1500).
  - Sawtooth Upper Bound
    - Defines a point-set representation of the upper bound with belief-utility pairs forming inverted pyramids ("teeth") which combine into the sawtooth shape. It interpolates utilities over belief points using L1-distance-based weighting and maintains standard basis beliefs for convexity guarantees. Iterative one-step lookahead updates improve the bound. Related concepts are detailed in [Hauskrecht 2000](https://doi.org/10.1613/jair.613).
  - Point Selection
    - Discusses strategies to select belief points for point-based methods, focusing on beliefs reachable under (approximate) policies. Methods include random belief expansion and exploratory expansion maximizing distance in belief space (L1-norm). Proper selection focuses computation on relevant parts of the belief space to improve policy quality. For detailed methods, see [Kurniawati et al. 2008](https://roboticsproceedings.org/rss04/p36.pdf).
  - Sawtooth Heuristic Search
    - An offline heuristic search method aiming to tighten upper and lower bounds on the value function, guided by the gap between these bounds at beliefs reachable from an initial belief. It selectively explores beliefs based on the gap threshold and updates both bounds iteratively until convergence. This approach is foundational to HSVI and SARSOP algorithms, as described in [Smith & Simmons 2004](https://www.cs.toronto.edu/~rgk/Research/HSVI-UAI04.pdf).
  - Triangulated Value Functions
    - Explains using Freudenthal triangulation for belief space discretization, enabling interpolation of the value function over a continuous belief space. Value function approximations rely on barycentric coordinates in the simplex enclosing the belief, allowing efficient upper bound approximations and dynamic programming. The method originates from [Lovejoy 1991](https://doi.org/10.1287/opre.39.1.162).
  - Summary
    - Summarizes the chapter's key offline belief state approximation methods: QMDP and fast informed bound as upper bounds, point-based algorithms as lower bounds, sawtooth upper bound improvements, belief point selection techniques, heuristic search tightening bounds, and triangulated interpolations for value approximation. It emphasizes the trade-offs and applicability in POMDP planning contexts.
