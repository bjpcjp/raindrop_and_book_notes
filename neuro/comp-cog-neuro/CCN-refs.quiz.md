1. **Q:** What computational and neurobiological mechanisms underpin hierarchical reinforcement learning in cortico-striatal circuits as described by Frank and Badre (2012), and how do these mechanisms interact to facilitate learning at multiple hierarchical levels?  
   **A:** Frank and Badre (2012) propose that hierarchical reinforcement learning in cortico-striatal circuits involves the organization of corticostriatal loops at multiple hierarchical levels, where higher levels guide learning and control at subordinate levels. These circuits utilize dopaminergic signals to update value representations, with the striatum encoding different levels of abstraction. This architecture supports the learning and transfer of complex rule structures by creating layered representations that enable cognitive control over both specific actions and overarching task goals.  
   **External example:** A study demonstrated hierarchical RL in humans using fMRI to show cortical and striatal areas representing different levels of task abstraction (Badre & Frank, 2012): https://www.ncbi.nlm.nih.gov/pubmed/21693491

2. **Q:** Explain the role of phasic dopamine signals within the basal ganglia system in reinforcement learning models, as detailed by Montague et al. (1996) and subsequent computational elaborations by O’Reilly and colleagues.  
   **A:** Phasic dopamine signals serve as prediction error signals in basal ganglia reinforcement learning models, updating the value of actions by signaling discrepancies between expected and received rewards. Montague et al. (1996) framed dopamine as a predictive Hebbian learning modulator; O’Reilly and colleagues extended this by modeling how dopamine dynamics shape corticostriatal plasticity and learning, influencing the acquisition of action policies in a biologically plausible manner. These signals enable flexible and adaptive learning within an opponent actor framework (OpAL), modulating both positive (D1 receptor-mediated) and negative (D2 receptor-mediated) reinforcement pathways.  
   **External example:** Schultz et al. (1997) showed dopaminergic neurons encode reward prediction error signals critical for learning: https://www.ncbi.nlm.nih.gov/pubmed/9054347

3. **Q:** How do computational models of the prefrontal cortex and basal ganglia, such as those developed by Hazy, Frank, and O’Reilly (2006, 2007), resolve the “homunculus problem” in executive control, and what are the key assumptions of these models?  
   **A:** These models resolve the homunculus problem by simulating executive control as an emergent property of interactions between the prefrontal cortex (PFC) and basal ganglia (BG) without requiring a central controller. The PFC maintains context representations while the BG gates the updating and maintenance of information via reinforcement-driven learning. The key assumptions include parallel distributed processing, reinforcement learning mechanisms that train gating functions, and dynamic interactions that produce task-set maintenance, selection, and switching purely through learned neural interactions, eliminating the need for an explicit “executive” agent.  
   **External example:** The PBWM model by Hazy et al. (2006) demonstrated executive function via basal ganglia gating in computational simulations: https://www.ncbi.nlm.nih.gov/pubmed/16343792

4. **Q:** Discuss the complementary learning systems theory as outlined by McClelland, McNaughton, and O’Reilly (1995), highlighting the distinct roles of the hippocampus and neocortex in learning and memory.  
   **A:** The Complementary Learning Systems theory posits that the hippocampus rapidly encodes episodic information with sparse, non-overlapping representations allowing quick storage and retrieval, whereas the neocortex gradually acquires structured knowledge via slow learning mechanisms that integrate across experiences. This division mitigates catastrophic interference, supports both rapid acquisition and generalization, and explains how memories are consolidated over time from hippocampus-dependent to cortex-dependent storage. The theory is grounded in connectionist model successes and failures illustrating the need for two interacting systems with distinct learning rates and representational schemes.  
   **External example:** Empirical studies show hippocampal involvement in rapid learning with cortical systems underpinning semantic memory consolidation (e.g., standard systems consolidation): https://www.ncbi.nlm.nih.gov/pubmed/7624455

5. **Q:** Describe the spike timing-dependent plasticity (STDP) mechanism and its theoretical grounding as discussed by Bi and Poo (1998) and Shouval et al. (2010), including the implications for synaptic modification rules.  
   **A:** STDP is a synaptic plasticity rule where the precise timing of presynaptic and postsynaptic spikes determines the direction and magnitude of synaptic change: presynaptic spikes preceding postsynaptic spikes within a critical window potentiate synapses, while the reverse timing leads to depression. Bi and Poo (1998) empirically demonstrated this dependence, while Shouval et al. (2010) argued that STDP emerges as a consequence of more fundamental, gradient-based learning rules shaped by underlying biophysical kinetics. This timing-dependent mechanism ensures input specificity and activity-dependent shaping of neural circuits essential for learning temporal sequences and associative encoding.  
   **External example:** Froemke's review details the biophysical basis and computational roles of STDP in neural plasticity: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3210027/

6. **Q:** What are the main insights from the neural network model frameworks proposed by Hopfield (1982, 1984) and Hinton and Sejnowski (1983) regarding associative memory and perceptual inference?  
   **A:** Hopfield introduced recurrent neural networks with symmetric weights capable of storing and retrieving patterns via emergent collective dynamics functioning as associative memory. Neurons with graded responses stabilize attractor states representing memories. Hinton and Sejnowski (1983) extended these ideas by framing perception as a statistical inference process, where neural networks optimize probabilistic representations to infer causes of sensory input. Together, these frameworks emphasize distributed representations, energy landscapes, and learning as optimizing internal models for memory recall and perceptual interpretation.  
   **External example:** Modern Hopfield networks underpin pattern completion capabilities in computational neuroscience: https://www.pnas.org/content/79/8/2554

7. **Q:** Summarize the key contributions of O’Reilly and Munakata (2000) in developing computational models that simulate cognitive functions based on biologically plausible neural mechanisms.  
   **A:** O’Reilly and Munakata (2000) provided computational explorations that bridge neural data and cognitive theory through biologically detailed models emphasizing local learning rules, recurrent connectivity, and realistic neural dynamics. They demonstrated how complex cognitive functions like working memory, learning, and cognitive control could emerge from networks constrained by neuroanatomy and physiology. Their approach integrates distributed representations with error-driven and Hebbian learning in a manner that respects known neural constraints, enabling simulations of development, memory, and executive functions with substantial explanatory power.  
   **External example:** Their book offers instructive models for prefrontal cortex function in cognition: https://mitpress.mit.edu/books/computational-explorations-cognitive-neuroscience

8. **Q:** How does the laminar structure of the ventral visual stream relate to differences in gamma and alpha coherence as reported by Buffalo et al. (2011), and what does this imply for visual processing?  
   **A:** Buffalo et al. (2011) found that different cortical layers within the ventral visual stream show distinct coherence patterns: superficial layers exhibited stronger gamma-band coherence, associated with feedforward processing, whereas deeper layers showed pronounced alpha-band coherence linked with feedback and inhibitory functions. This laminar differentiation supports hierarchical and directional communication within visual cortex, whereby high-frequency synchronization facilitates the encoding of sensory details, and lower-frequency rhythms modulate top-down influences and attentional gating.  
   **External example:** Laminar electrophysiological recordings in macaques reveal similar frequency-specific processing associated with visual information flow: https://www.pnas.org/content/108/27/11262

9. **Q:** What theoretical perspectives do Baars (1988) and Lamme (2006) provide on consciousness, particularly regarding the cognitive and neural correlates of awareness?  
   **A:** Baars (1988) developed the Global Workspace Theory, positing consciousness as a global broadcasting system integrating diverse specialized processors that enables flexible cognitive control and reportability. Lamme (2006) argues for a neural stance emphasizing recurrent cortical processing over mere feedforward activity, suggesting that consciousness arises from recurrent interactions and integrated processing rather than simple activation. Both perspectives highlight distributed neural dynamics and functional integration as core bases for conscious experience, moving beyond localization to considering system-wide coordination in the brain.  
   **External example:** Neuroscientific research supports global workspace dynamics mediating conscious access: https://doi.org/10.1016/j.tics.2006.09.001

10. **Q:** How do classical conditioning models such as the Rescorla-Wagner model (1972) relate to reinforcement learning frameworks discussed throughout the document, particularly in terms of prediction error and learning updates?  
    **A:** The Rescorla-Wagner model conceptualizes learning as driven by the discrepancy between expected and actual outcomes—the prediction error—which adjusts associative strengths incrementally. This core idea underpins modern reinforcement learning models that extend associative learning to action-value updating within neural circuits. Sutton and Barto (1981, 1998) formalized these principles computationally, providing algorithmic frameworks for adaptive networks incorporating reward prediction errors as teaching signals, closely paralleling biological dopamine-based learning mechanisms discussed in the document.  
    **External example:** Sutton and Barto’s Reinforcement Learning book elaborates the computational role of prediction error: https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf
