Learning in neural networks involves modifying synaptic weights based on the activity of sending and receiving neurons, primarily driven by calcium influx through NMDA channels which induces synaptic plasticity. The eXtended Contrastive Attractor Learning (XCAL) model integrates biologically grounded Hebbian self-organizing learning and error-driven learning through dynamic thresholds, enabling networks to extract statistical regularities and correct errors for complex cognitive tasks.
