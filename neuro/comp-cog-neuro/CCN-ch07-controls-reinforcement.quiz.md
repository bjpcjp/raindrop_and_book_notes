1. **Q:** How do the basal ganglia and cerebellum differ in their roles and learning mechanisms in motor control and action selection, and what neural pathways support these differences?
   **A:** The basal ganglia specialize in action selection via reinforcement learning based on reward/punishment signals, selecting the most rewarding actions and avoiding punishing ones through a disinhibitory gating mechanism that modulates thalamocortical circuits. Its direct (Go) and indirect (NoGo) pathways are modulated by dopamine signals encoding reward prediction errors. In contrast, the cerebellum is specialized for refining and coordinating executed actions through error-driven learning based on sensory outcome errors, employing a high-dimensional expansion of input representation via granule cells and climbing fiber-driven synaptic plasticity in Purkinje cells. The basal ganglia operate through loops with multiple frontal cortical areas and output to the thalamus (or superior colliculus for eye movements), while the cerebellum receives input exclusively from motor-related areas (parietal and motor cortex) and projects to motor brainstem outputs but does not connect directly with basal ganglia circuits.
   **External example:** The basal ganglia’s role in reinforcement learning is supported by studies of Parkinson’s disease motor deficits and dopamine’s modulation of Go/NoGo pathways: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2923450/

2. **Q:** Explain the computational basis and biological correlate of the reward prediction error signal in dopamine neurons and how it relates to reinforcement learning models like Rescorla-Wagner and Temporal Difference learning.
   **A:** Dopamine neurons in the VTA and SNc encode reward prediction errors—the difference between received and expected reward—firing bursts for unexpected rewards, shifting firing to conditioned stimulus (CS) onset after learning, and pausing when predicted rewards are omitted. This matches the Rescorla-Wagner rule mathematically expressed as δ = r − r̂, where learning updates weights to minimize this error. Temporal Difference (TD) learning extends this by incorporating future expected rewards (δ = r(t) + γV̂(t+1) − V̂(t)), explaining dopamine firing at CS onset via anticipation of future reward, thus enabling the learning signal to propagate backward in time. These computational models precisely characterize the biology of dopamine-dependent reinforcement learning.
   **External example:** Schultz et al.'s pioneering study that linked dopamine firing to reward prediction error: https://www.cell.com/neuron/fulltext/S0896-6273(97)00189-9

3. **Q:** Describe the actor-critic architecture as it applies to basal ganglia motor learning, including the role of dopamine and the Opponent Actor Learning (OpAL) model in weighing Go and NoGo pathways.
   **A:** In the actor-critic model, the basal ganglia serve as the actor, generating candidate motor or cognitive actions, while the dopaminergic system acts as the critic, computing reward prediction errors to train the system. Dopamine bursts reinforce Go pathway activity leading to action initiation, and dopamine dips reinforce NoGo pathway activity to avoid certain actions. The OpAL model refines this by maintaining separate Go (G) and NoGo (N) weights within the actor, with dopamine levels modulating the balance between them, weighting benefits versus costs of actions. Higher dopamine amplifies benefits boosting Go activity, promoting riskier choices, whereas lower dopamine amplifies costs enhancing NoGo activity, favoring avoidance. This dynamic enables nuanced decision-making integrating learned reward and cost information.
   **External example:** Collins & Frank’s OpAL model explains behavioral effects of dopamine on cost-benefit decision-making: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004104

4. **Q:** How does the PVLV model account for the biological generation of dopamine reward prediction error signals, and what neural substrates are involved in its Primary Value (PV) and Learned Value (LV) components?
   **A:** The PVLV model splits dopamine reward prediction error signaling into two interacting subsystems: Primary Value (PV), which generates dopamine responses to primary rewards via lateral hypothalamus excitation (PVe) and their inhibition by ventral striatum patch neurons (PVi); and Learned Value (LV), which drives dopamine firing to conditioned stimuli via excitatory input from the central nucleus of the amygdala (LVe) balanced by inhibitory ventral striatum neurons (LVi). PVLV learning occurs mainly at the time of actual rewards, with LV learning gated by primary reward signals, explaining why dopamine neurons respond to CS onset and not just rewards. This model fits with anatomical data showing distinct brain regions supporting different aspects of reward processing and dopamine modulation.
   **External example:** Mollick et al.’s research models PVLV functions with biological mapping to the VTA and SNc: https://pubmed.ncbi.nlm.nih.gov/34836567/

5. **Q:** Explain the cerebellar learning mechanism involving high-dimensional expansion and climbing fiber error signals, and how this leads to motor refinement rather than direct action selection.
   **A:** The cerebellum receives exclusively motor-related cortical inputs via mossy fibers, which synapse onto granule cells that massively expand input dimensionality, generating unique high-dimensional activity patterns for different sensory-motor states. Parallel fibers from granule cells excite Purkinje cells, which output inhibitory signals tonically to deep cerebellar nuclei. Climbing fibers from the inferior olivary nucleus provide powerful error signals that induce synaptic plasticity (e.g., LTD) at parallel fiber-Purkinje synapses for the active granule cells, reducing Purkinje cell activity on those patterns and disinhibiting motor programs, thereby correcting motor errors. This machinery supports a fine-tuned lookup table-like function for accurate, well-coordinated motor output without selecting which action to execute.
   **External example:** The role of climbing fibers in error-driven motor learning is supported by cerebellar lesion and stimulation studies: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3222888/

6. **Q:** Discuss how the basal ganglia engage in cognitive action selection beyond motor control and how their connectivity supports this expanded role.
   **A:** Beyond motor functions, basal ganglia circuits exist in parallel loops connected to multiple frontal cortical areas involved in higher cognition—prefrontal cortex (DLPFC), orbitofrontal cortex (OFC), and anterior cingulate cortex (ACC)—allowing the basal ganglia to select among abstract cognitive actions, strategies, working memory updates, and value-cost representations. Inputs to these loops provide the relevant sensory and contextual information (e.g., reward identity via inferotemporal cortex to OFC, effort costs via ACC), while the basal ganglia’s gating functions determine which cognitive plans are executed. This broad connectivity enables reinforcement learning mechanisms to shape not only physical motor actions but also complex cognitive functions.
   **External example:** Frank and O’Reilly’s work on basal ganglia involvement in working memory updating illustrates this cognitive role: https://www.nature.com/articles/nn1552

7. **Q:** What are the direct, indirect, and hyperdirect pathways within the basal ganglia circuitry, and how do they modulate action gating?
   **A:** The direct pathway involves striatal Go medium spiny neurons inhibiting GPi neurons, resulting in disinhibition of thalamic nuclei and allowing action initiation. The indirect pathway involves striatal NoGo neurons inhibiting the GPe, which normally inhibits GPi; thus, indirect pathway activation disinhibits GPi, increasing inhibition of the thalamus and suppressing action initiation. The hyperdirect pathway involves the subthalamic nucleus (STN) receiving direct cortical input and exciting the GPi broadly, providing global NoGo signaling that raises gating thresholds to prevent premature or conflicting actions, effectively buying time during high-conflict decision-making. These three pathways interact dynamically to balance initiation and suppression of both motor and cognitive actions.
   **External example:** The role of these pathways in motor control and inhibition is summarized in DeLong and Wichmann’s basal ganglia reviews: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2769744/

8. **Q:** How do dopamine levels modulate the balance between Go and NoGo pathway activity and influence decision-making under conditions of risk or uncertainty?
   **A:** High dopamine levels enhance activity in D1 receptor-expressing Go pathway neurons, facilitating the initiation of actions with higher expected benefits by amplifying Go signals and diminishing NoGo (D2 receptor) activity, promoting risk-taking behavior. Conversely, low dopamine reduces Go activation and disinhibits NoGo neurons, amplifying the perceived costs and leading to more conservative, risk-averse decisions. This dynamic modulation affects both learning (via synaptic plasticity) and moment-to-moment action selection, enabling flexible adaptation to changing reward contingencies and uncertainty.
   **External example:** Dopaminergic medication effects on risk-taking in Parkinson's patients illustrate these mechanisms: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3664770/
